[
  {
    "path": "posts/2023-12-30-code-repository-for-publicly-available-finance-data/",
    "title": "Code repository for publicly available finance data",
    "description": "Repository of R functions and code to obtain publicly available financial data from various sources",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2023-12-30",
    "categories": [
      "Data"
    ],
    "contents": "\r\n\r\nContents\r\nDamodaran’s finance data\r\nFunction\r\nCode to run function\r\n\r\nKenneth French’s stock market data\r\nFunctions\r\nCode to run functions\r\n\r\nECB FX rates\r\nFunctions\r\nCode to run functions\r\n\r\n\r\nThis post includes functions and code that obtain financial data such as FX rates, stock returns, and valuation multiples. Each section is named after the data source. I might add new code over time to this post. I will not explain the code or study the data in much detail, this is really only meant to show how to get the data into R quickly.\r\nDamodaran’s finance data\r\nThis code is simply copied from this post.\r\nFunction\r\n\r\n\r\nShow code\r\n\r\n#' Wrapper to import data from Excel\r\n#'\r\n#' This functions serves as a wrapper to easily import data from particular Excel files, for example\r\n#' as provided by Damodaran on his website. These Excel files have a format in which information is\r\n#' provided in the first couple of lines and then the actual data set starts.\r\n#'\r\n#' Functions such as \\code{\\link{read_excel}} allow the user to explicitly tell the function which\r\n#' rows to skip. While this is certainly a helpful feature, it becomes cumbersome for many similar\r\n#' files but with different number of rows that can be skipped as the user would have to open each\r\n#' file manually to come up with the parameters. This wrapper tries to do it automatically.\r\n#'\r\n#'\r\n#' @param .str_file string; file location.\r\n#' @param .file_ext string; extension of the Excel-file. Default: \".xls\", for which this function\r\n#'                  is tested. So might not work with other Excel file extensions.\r\n#' @param .perc_numeric numeric; the percentage above which you assume a column is of type numeric.\r\n#'                      Background: in the function, you convert a column to numeric with \\code{\r\n#'                      as.numeric}; this will return NA if this function does not know how to\r\n#'                      convert the value to numeric; hence, you can guess now that if there aren't\r\n#'                      too many NAs from this function, the column is actually a numeric one.\r\n#'                      The default is 0.15, which means that a column is considered numeric if\r\n#'                      at least 15 per cent of the values can be converted.\r\n#' @param .bol_option_1 boolean; if TRUE, the header position is determined by looking at the starting\r\n#'                      point of non-NA values after conversion to non-numeric that occurs most often.\r\n#'                      if FALSE, you use the first row that has all observations (which is most likely\r\n#'                      the header).\r\n#' @return data.table from reading the Excel file with the start row determined by this wrapper.\r\n#'\r\n#' @importFrom readxl read_excel\r\n#' @importFrom httr GET write_disk\r\n#'\r\n#' @export\r\n#'\r\n#' @references\r\n#' \\url{https://stackoverflow.com/questions/41368628/read-excel-file-from-a-url-using-the-readxl-package}\r\n#'\r\n#' @examples\r\n#' \\dontrun{\r\n#' str_file <- \"http://pages.stern.nyu.edu/~adamodar/pc/archives/vebitda18.xls\"\r\n#' import_from_excel(str_file)\r\n#' }\r\nimport_from_excel <- function(.str_file, .file_ext = \".xls\", .perc_numeric=0.15,\r\n                              .bol_option_1=TRUE, ...) {\r\n\r\n  #### Save URL file locally with GET\r\n  httr::GET(.str_file, write_disk(tf <- tempfile(fileext = \".xls\")))\r\n\r\n  ####Save the file into a data.table\r\n  intDT <- as.data.table(readxl::read_excel(tf))\r\n\r\n  ####Determine first row by two ways\r\n\r\n  ## 1. It's likely that most columns in the data set are numeric; hence, you will convert\r\n  #     each column to a numeric and save the first non-NA number; the row where this happens\r\n  #     most often (essentially the median) is your best guess for the first data row (i.e.\r\n  #     one below the header)\r\n  i <- 1\r\n  vec_first_number <- numeric(ncol(intDT))\r\n  col_types <- rep(\"guess\", times=ncol(intDT))\r\n  for (col in names(intDT)) {\r\n\r\n    vec <- suppressWarnings(as.numeric(unlist(intDT[,col, with=FALSE])))\r\n    vec_first_number[i] <- suppressWarnings(min(which(!is.na(vec))))\r\n    #In case there is no numeric data in there, your best  guess is that the data set starts\r\n    #at the second line (after headers)\r\n    if (!is.finite(vec_first_number[i])) {\r\n      vec_first_number[i] <- 2\r\n    }\r\n    # Also save if you believe that the column in a numeric one, as you can read the column as\r\n    # numeric later\r\n    if (sum(!is.na(vec))/length(vec)>.perc_numeric) {\r\n      col_types[i] <- \"numeric\"\r\n    }\r\n    i <- i + 1\r\n  }\r\n  pos_header1 <- median(vec_first_number, na.rm=TRUE) - 1\r\n\r\n  ## 2. If the first few rows do not belong yet to the actual data set, you would expect more NAs\r\n  #     there than in the data set thereafter. In particular in case of a data set with headers,\r\n  #     with you kinda require here, you must expect that the row with the headers doesn't have\r\n  #     any NAs; hence, simply use the first occurrence of a row with no NAs as the decision\r\n  #     criterium\r\n  #     Of course, this can fail if one of the information rows is filled up or if there is\r\n  #     no header row or even the header includes NAs.\r\n\r\n  # Get NAs per row\r\n  # https://stackoverflow.com/questions/35306500/r-data-table-count-na-per-row\r\n  intDT[, num_obs := Reduce(`+`, lapply(.SD,function(x) !is.na(x)))]\r\n  pos_header2 <- suppressWarnings(min(which(intDT$num_obs==max(intDT$num_obs))))\r\n  if (!is.finite(pos_header2)) {\r\n    pos_header2 <- 1\r\n  }\r\n  intDT[, num_obs:=NULL]\r\n\r\n  if (pos_header2==pos_header2) {\r\n    cat(\"Success: Both approaches in import_from_excel guessed the same start row of the data set.\\n\")\r\n    pos_header <- pos_header1\r\n  } else {\r\n    ifelse(.bol_option_1, pos_header <- pos_header1, pos_header <- pos_header2)\r\n  }\r\n\r\n  #Download data again, now with skip filled in and check then number of columns; trim col_types\r\n  #This has to be done as otherwise it can happen that the last column doesn't exist anymore\r\n  #because it was only read before due to a line that is now excluded.\r\n  #See http://pages.stern.nyu.edu/~adamodar/pc/archives/fundgrEB14.xls as an example\r\n  intDT <- as.data.table(readxl::read_excel(tf, skip=pos_header))\r\n  if (ncol(intDT)<length(col_types)) {\r\n    col_types <- col_types[1:ncol(intDT)]\r\n  }\r\n\r\n  intDT <- as.data.table(readxl::read_excel(tf,\r\n                                    skip=pos_header,\r\n                                    col_names = TRUE,\r\n                                    col_types = col_types,\r\n                                    ...))\r\n  unlink(tf) #Clean up the temporary file\r\n  return(intDT)\r\n\r\n}\r\n\r\n\r\nCode to run function\r\nAs str_file, you have to pass a link to a file that is saved on Damodaran’s website. You can find the overview here. For example, to download the EV/EBITDA multiples of 2018, run the following code.\r\n\r\n\r\nShow code\r\n\r\nlibrary(readxl)\r\nlibrary(httr)\r\nlibrary(data.table)\r\nstr_file <- \"http://pages.stern.nyu.edu/~adamodar/pc/archives/vebitda18.xls\"\r\ndtEBITDA2018 <- import_from_excel(str_file)\r\n\r\n\r\nIt is easy to wrap around a for loop and combine this data over time.\r\n\r\n\r\nShow code\r\n\r\n### Download Tax Rates\r\ntaxRatesDT <- data.table()\r\nstr_path <- \"http://pages.stern.nyu.edu/~adamodar/pc/archives/\" \r\nfor (year in 2014:2022) {\r\n  \r\n  str_file <- paste0(str_path, \"countrytaxrates\", substr(year,star=3,stop=4), \".xls\")\r\n  \r\n  intDT <- import_from_excel(.str_file=str_file,\r\n                             na = c(\"NA\", \"N/A\"))\r\n  intDT[, Year:=year]\r\n  taxRatesDT <- rbind(taxRatesDT, intDT, fill=TRUE)\r\n                          \r\n} \r\n\r\n\r\nGiven the sheer size of Damodaran’s archive, it is not surprising that some inconsistencies or changes happen over time that make it difficult to fully automate it to obtain the data. In this particular case, as of December 2023, you will notice that column country was labelled “Location” until 2014 and from 2015 “Country”. Hence, you have to combine the columns. It also turns out that Damodaran changed the format for this particular data set and includes historical tax rates in newer data sets. You therefore end up with several rows for one country. To fix this, I create a new data set with only one line per country, averaging over all non-NA tax rates per year (they should all be the same anyways as they are duplicates).\r\n\r\n\r\nShow code\r\n\r\n### Combine columns Location and Country\r\ntaxRatesDT[is.na(Country), Country:=Location]\r\ntaxRatesDT[, Location:=NULL]\r\n### Aggregate multiple rows per country\r\ntaxRatesDT[,Year:=NULL]\r\naggTaxRatesDT <- taxRatesDT[,  lapply(.SD, mean, na.rm=TRUE), by=Country]\r\naggTaxRatesDT[Country==\"Germany\"]\r\n\r\n   Country   2006   2007   2008   2009   2010   2011   2012   2013\r\n1: Germany 0.3834 0.3836 0.2951 0.2944 0.2941 0.2937 0.2948 0.2955\r\n     2014      2015   2016   2003   2004   2005   2017 2018 2019 2020\r\n1: 0.2958 0.2969667 0.2972 0.3958 0.3829 0.3831 0.2979  0.3  0.3  0.3\r\n   2021 2022 2023E\r\n1:  0.3  0.3   0.3\r\n\r\nKenneth French’s stock market data\r\nI used this function already for example in the post on sector performance to obtain data from Kenneth French’s website.\r\nFunctions\r\n\r\n\r\nShow code\r\n\r\n#' Wrapper to import data in CSV format within a zip-folder\r\n#'\r\n#' This functions serves as a wrapper to easily import data from particular ZIP files that hold\r\n#' CSV files, for example as provided by Kenneth French on his website. The user has to provide\r\n#' the location of the files (can be a URL) and how many lines should be skipped.\r\n#'\r\n#' The numbers of line skipped can be passed with the \\code{skip} argument in function\r\n#' \\code{fread} from the \\code{data.table} package. See \\code{?fread} for more information.\r\n#'\r\n#' @inheritParams import_from_excel\r\n#' @param .file_ext string; extension of the Excel-file. Default: \".xls\", for which this function\r\n#'                  is tested. So might not work with other Excel file extensions.\r\n#' @return data.table from reading the CSV file with the start row determined by this wrapper.\r\n#'\r\n#' @importFrom data.table fread\r\n#'\r\n#' @export\r\n#'\r\n#' @references\r\n#' \\url{https://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data}\r\n#'\r\n#' @examples\r\n#' \\dontrun{\r\n#' ### Download factors\r\n#' str_file <- \"http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip\"\r\n#' import_from_CSV_in_zip(str_file, skip = 3)\r\n#'\r\n#' ### Download industry performances\r\n#' str_file <- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/10_Industry_Portfolios_daily_CSV.zip\"\r\n#' import_from_CSV_in_zip(str_file, skip = 9, header=TRUE)\r\n#' }\r\nimport_from_CSV_in_zip <- function(.str_file, ...) {\r\n\r\n  temp <- tempfile()\r\n  download.file(.str_file,temp, mode=\"wb\")\r\n  csvDT <- data.table::fread(unzip(temp), ...)\r\n  unlink(temp)\r\n  return(csvDT)\r\n\r\n}\r\n\r\n\r\n#' Function to get data from Kenneth French's website and format it\r\n#'\r\n#' @inheritParams import_from_CSV_in_zip\r\n#'\r\n#' @return TODO\r\n#'\r\n#' @importFrom lubridate ymd\r\n#'\r\n#' @export\r\n#'\r\n#' @references\r\n#' \\url{https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html}\r\n#'\r\n#' @examples\r\n#' \\dontrun{\r\n#' str_file <- \"http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip\"\r\n#' get_FF_data(str_file, skip = 3)\r\n#'\r\n#' ### Download industry performances\r\n#' str_file <- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/10_Industry_Portfolios_daily_CSV.zip\"\r\n#' DT <- get_FF_data(str_file, skip = 9, header=TRUE)\r\n#' # Convert columns to numeric\r\n#' cols <- names(DT)[2:ncol(DT)]\r\n#' DT[ , (cols) := lapply(.SD, as.numeric), .SDcols = cols]\r\n#' # Calculate the mean daily return (in bps) over the last years\r\n#' require(lubridate)\r\n#' DT[year(Date)>2010, lapply(.SD, mean), by = year(Date),.SDcols = cols]\r\n#' DT[year(Date)>2010, lapply(.SD, mean), .SDcols = cols]\r\n#' }\r\nget_FF_data <- function(.str_file, ...) {\r\n\r\n  ###1. Use import_from_CSV_in_zip to get data\r\n  csvDT <- import_from_CSV_in_zip(.str_file, ...)\r\n\r\n\r\n  ###2. Change names of columns\r\n  #     FF doesn't label the date column, enter\r\n  data.table::setnames(csvDT,\r\n           old = \"V1\",\r\n           new = \"Date\")\r\n  #     Get rid of hyphens, don't ge well with column names\r\n  data.table::setnames(csvDT,\r\n           old = grep(\"-\",names(csvDT),value=TRUE),\r\n           new = gsub(\"-\", \"\", grep(\"-\",names(csvDT),value=TRUE)))\r\n\r\n  ###3. Convert Date column to date; through warning if format changed\r\n  if (sum(is.na(lubridate::ymd(csvDT$Date)))>0) {\r\n    stop(\"Error in parsing dates to date format: it seems the format in Kenneth French files has changed.\")\r\n  }\r\n  csvDT[, Date:=lubridate::ymd(Date)]\r\n\r\n  return(csvDT)\r\n\r\n}\r\n\r\n\r\nCode to run functions\r\n\r\n\r\nShow code\r\n\r\n### Download daily factors\r\nstr_file <- \"http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip\"\r\ndailyFactors_DT <- get_FF_data(str_file, skip = 3)\r\n### Download industry performances\r\nstr_file <- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/10_Industry_Portfolios_daily_CSV.zip\"\r\nDT <- get_FF_data(str_file, skip = 9, header=TRUE)\r\n# Convert columns to numeric\r\ncols <- names(DT)[2:ncol(DT)]\r\nDT[ , (cols) := lapply(.SD, as.numeric), .SDcols = cols]\r\n# Calculate the mean daily return (in bps) over the last years\r\nrequire(lubridate)\r\n#DT[year(Date)>2010, lapply(.SD, mean), by = year(Date),.SDcols = cols]\r\nDT[year(Date)>2010, lapply(.SD, mean), .SDcols = cols]\r\n\r\n        NoDur      Durbl      Manuf      Enrgy      HiTec      Telcm\r\n1: 0.03969231 0.06825231 0.04592923 0.03685538 0.07185846 0.03554769\r\n        Shops       Hlth      Utils      Other\r\n1: 0.05878154 0.05327692 0.03922154 0.05052308\r\n\r\nECB FX rates\r\nThe ECB posts daily FX rates between the EUR and other currencies.\r\nFunctions\r\n\r\n\r\nShow code\r\n\r\n#' Import FX rates from the ECB website\r\n#'\r\n#' This functions downloads the FX rates directly from the ECB website and returns them\r\n#' in long format. It also makes sure that there are no missing dates; in case a date is\r\n#' missing in the ECB data set, it is filled with the previous value (see reference\r\n#' to the Stackoverflow link on how this is done).\r\n#'\r\n#' All FX rates are in relation to the euro.\r\n#'\r\n#' It relies on \\code{\\link{import_from_CSV_in_zip}} to download the data, which is provided\r\n#' as a CSV file in a zip folder.\r\n#'\r\n#' @inheritParams import_from_CSV_in_zip\r\n#' @return data.table in long format including the FX rates as provided by the ECB bank on their\r\n#'         website. data.table has three columns: \"Date\", \"CUR\" (3-digit identifier of the currency),\r\n#'         \"FXrate\" (actual FX rate value).\r\n#'\r\n#' @export\r\n#'\r\n#' @references\r\n#' \\url{https://stackoverflow.com/questions/22956803/fastest-way-for-filling-in-missing-dates-for-data-table}\r\n#'\r\n#' @examples\r\n#' \\dontrun{\r\n#' require(data.table)\r\n#' str_file <- \"https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip?c39d518d60048add50a3cccd64e06146\"\r\n#' DT <- import_ECB_FX_rates(str_file)\r\n#'\r\n#' ### Which currencies (CUR) are covered\r\n#' unique(DT$CUR)\r\n#'\r\n#' ### Plot for example CHF/EUR over time\r\n#' require(ggplot2)\r\n#' ggplot(DT[CUR==\"CHF\"], aes(x=Date, y=FXrate)) + geom_line()\r\n#' }\r\nimport_ECB_FX_rates <- function(.str_file) {\r\n\r\n  ### Import CSV  File\r\n  DT <- import_from_CSV_in_zip(.str_file)\r\n\r\n  ### Convert dates to date-format\r\n  DT[, Date:=ymd(Date)]\r\n\r\n  ### Bring into long format\r\n  DT <- suppressWarnings(melt(DT, id.var=\"Date\",\r\n             variable.name = \"CUR\",\r\n             variable.factor = FALSE, #Keep as character\r\n             value.name    = \"FXrate\"))\r\n\r\n  DT[, FXrate:=as.numeric(FXrate)]\r\n\r\n  ###  Get rid of missing values\r\n  DT <- DT[!is.na(FXrate)]\r\n\r\n  ###Fill in missing dates with previous value\r\n  #https://stackoverflow.com/questions/22956803/fastest-way-for-filling-in-missing-dates-for-data-table\r\n  setkey(DT, Date)\r\n  indx <- DT[,.(Date=seq(min(Date),max(Date),\"days\")),CUR]\r\n\r\n  # key the tables and join them using a rolling join\r\n  setkey(DT,CUR,Date)\r\n  setkey(indx,CUR,Date)\r\n  DT <- DT[indx,roll=TRUE]\r\n\r\n  return(DT)\r\n\r\n}\r\n\r\n#' Get FX pair from ECB data\r\n#'\r\n#' Starting with the data.table returned by \\code{\\link{import_ECB_FX_rates}}, this function\r\n#' returns the FX rate between two currencies. \\code{.CUR_base} is the basis of the FX rate.\r\n#' For example, \\code{.CUR_base=\"EUR\"} and \\code{.CUR==\"USD\"} is the equivalent of \"USD/EUR\".\r\n#'\r\n#' @param .dt data.table as returned by \\code{\\link{import_ECB_FX_rates}}\r\n#' @param .start_date Start date for which the FX rate should be shown; Default: NA, i.e.\r\n#'         as far back as possible.\r\n#' @param .end_date End date for which the FX rate should be shown; Default: NA, i.e.\r\n#'        as current as possible.\r\n#' @param .CUR 3-digit currency identifier; think \".CUR_base/.CUR\"\r\n#' @param .CUR_base 3-digit currency identifier; think \".CUR_base/.CUR\"\r\n#' @return data.table with three columns: \"Date\", \"FX\", \"FXrate\"\r\n#'\r\n#' @export\r\n#'\r\n#' @examples\r\n#' \\dontrun{\r\n#' require(data.table)\r\n#' require(lubridate)\r\n#' str_file <- \"https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip?c39d518d60048add50a3cccd64e06146\"\r\n#' DT <- import_ECB_FX_rates(str_file)\r\n#'\r\n#' dt <- get_FX_pair_from_ECB_rates(.dt = DT,\r\n#'                                  .start_date= dmy(\"31122020\"),\r\n#'                                  .end_date  = dmy(\"31122021\"),\r\n#'                                  .CUR_base=\"EUR\", .CUR=\"CHF\")\r\n#'\r\n#' dt <- get_FX_pair_from_ECB_rates(.dt = DT,\r\n#'                                  .CUR_base=\"USD\", .CUR=\"CAD\")\r\n#'\r\n#' }\r\nget_FX_pair_from_ECB_rates <- function(.dt, .start_date=NA, .end_date=NA, .CUR_base, .CUR) {\r\n\r\n  ### Make a copy, you don't want to change the input .dt\r\n  #https://stackoverflow.com/questions/10225098/understanding-exactly-when-a-data-table-is-a-reference-to-vs-a-copy-of-another\r\n  intDT <- copy(.dt)\r\n\r\n  ### Check currencies are in the table\r\n  if (.CUR_base==\"EUR\" | .CUR==\"EUR\") {\r\n    if (sum(c(.CUR_base, .CUR) %in% unique(intDT$CUR)) == 0) {\r\n      stop(\"At least one currency is not in the ECB table.\")\r\n    }\r\n  } else {\r\n    if (sum(c(.CUR_base, .CUR) %in% unique(intDT$CUR)) < 2) {\r\n      stop(\"At least one currency is not in the ECB table.\")\r\n    }\r\n  }\r\n\r\n  ### Limit data.table from .start_date to .end_date\r\n  if (!is.na(.start_date)) {\r\n    intDT <- intDT[Date>=.start_date]\r\n  }\r\n  if (!is.na(.end_date)) {\r\n    intDT <- intDT[Date>=.end_date]\r\n  }\r\n\r\n  ### Change names\r\n  setnames(intDT, c(\"CUR\"), c(\"FX\"))\r\n\r\n  ### Check if one currency is EUR ==> just update FX value and return (potentially inversed)\r\n  if (.CUR_base==\"EUR\") {\r\n    intDT <- intDT[FX==.CUR]\r\n    intDT[, FX:=paste0(.CUR, \"/\", \"EUR\")]\r\n    intDT[, FXrate:=1/FXrate]\r\n    return(intDT)\r\n  }\r\n  if (.CUR==\"EUR\") {\r\n    intDT <- intDT[FX==.CUR_base]\r\n    intDT[, FX:=paste0(\"EUR/\", .CUR_base)]\r\n\r\n    return(intDT)\r\n  }\r\n\r\n  ### Limit data to relevant currencies\r\n  intDT <- intDT[FX %in% c(.CUR, .CUR_base)]\r\n  intDT <- dcast(intDT, Date ~ FX, value.var=\"FXrate\")\r\n\r\n  eval_text <- paste0(\"intDT[, FXrate:=\",.CUR, \"/\",.CUR_base,\"]\")\r\n  eval(parse(text=eval_text))\r\n  intDT[, FX:=paste0(.CUR_base, \"/\", .CUR)]\r\n\r\n  return(intDT[,list(Date,FX,FXrate)])\r\n\r\n}\r\n\r\n\r\nCode to run functions\r\n\r\n\r\nShow code\r\n\r\nrequire(data.table)\r\nrequire(lubridate)\r\nstr_file <- \"https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip?c39d518d60048add50a3cccd64e06146\"\r\nDT <- import_ECB_FX_rates(str_file)\r\n#'\r\ndt_USD_CAD <- get_FX_pair_from_ECB_rates(.dt = DT,\r\n                                 .CUR_base=\"USD\", .CUR=\"CAD\")\r\n#'\r\ndt_EUR_CHF <- get_FX_pair_from_ECB_rates(.dt = DT,\r\n                                 .start_date= dmy(\"31122020\"),\r\n                                 .end_date  = dmy(\"31122021\"),\r\n                                 .CUR_base=\"EUR\", .CUR=\"CHF\")\r\nlibrary(ggplot2)\r\nggplot(dt_EUR_CHF, aes(x=Date, y=FXrate)) + geom_line()\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-30-code-repository-for-publicly-available-finance-data/code-repository-for-publicly-available-finance-data_files/figure-html5/RunECBfunctions-1.png",
    "last_modified": "2023-12-30T13:56:02+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-02-05-the-takahashi-and-alexander-model/",
    "title": "The Takahashi and Alexander model",
    "description": "A brief introduction to the Takahashi and Alexander (TA) model for private equity fund cash flows and how to implement it in R.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2023-04-16",
    "categories": [
      "Private Equity",
      "Research",
      "R"
    ],
    "contents": "\r\nModelling cash flows of private equity funds is important for investors, especially for liquidity management. Unfortunately, it is also difficult. An important question an investor has to ask is what model to use. In the end, all models are simplifications of reality, but there is a huge variety on what assumptions are made, which parameters are important, and which are ignored.\r\nIn this post, I’m presenting one of the earliest models for fund cash flows, the Takahashi and Alexander (TA) model, and show how to implement it in R. Introduced in Takahashi and Alexander (2002) over two decades ago, it is still used in the industry today. This success stems in my opinion from its simplicity: the model can be described with a few simple equations and easily be implemented in tools such as Excel and R in a few minutes to draw nice-looking curves of capital calls and distributions.\r\nThis simplicity makes the model quite useful for quick analyses. Sometimes, all you need is a capital call and distribution curve of a fund to test out a few hypotheses such as what the gross/net spread of a fund should be with certain fee and carry terms. In the next section, I outline the relevant equations. I thereby follow the notation of Jeet (2020), who provides a nice introduction as well as a helpful modification to the model.\r\nThe Takahashi and Alexander model\r\nThere are three vectors an investor is interested about a PE fund: a vector of capital calls, distributions, and NAVs. The TA model starts by defining the capital call in period \\(t\\), \\(C_t\\), as the uncalled commitment at the end of the previous period, \\(UC_{t-1}\\), multiplied with a rate of contribution, \\(RC\\), that is a function of the age of the fund:\r\n\\[ C_t = UC_{t-1} \\times RC(Age_{t-1}). \\]\r\nTA give some guidance on how to specify RC: “Rather than specify a different rate of contribution every year, we simplify the model by separating the first two years of contributions from subsequent years. Typically, we would assume two similarly sized large contributions in years one and two, and geometrically declining contributions in subsequent years.” Of course, one can deviate from this. For example, I don’t see a good reason to separate between the first two years and subsequent years and would rather separate between the investment period (typically 3-5 years) and thereafter.\r\nDistributions in \\(D_t\\) are the product of the NAV at the end of the previous period, \\(NAV_{t-1}\\), multiplied with a growth rate, \\(G_t\\),1 and a rate of distribution function \\(RD\\):\r\n\\[ D_t=NAV_{t-1} \\times (1+G_t) \\times RD(Age_t-1, bow, L). \\]\r\nIn contrast to the RC function, TA specify the RD function, which is the defining characteristic of their model. Concretely,\r\n\\[ RD=\\left(\\frac{Age_{t-1}}{L} \\right)^{bow}. \\]\r\nBelow I plot the RD function for different \\(bow\\) values to get a better understanding of the mechanics. A few observations:\r\nThe rate of distribution increases over time and always ends up at 100%\r\nThe higher the \\(bow\\) factor, the lower the rate of distribution is in earlier years\r\nA \\(bow\\) factor of 1 is the special case for which the rate of distribution increases linearly with the age of the PE fund\r\n\r\n\r\nShow code\r\n\r\nlibrary(data.table)\r\nlibrary(ggplot2)\r\nRD_function <- function(age, L, bow) {\r\n  return((age/L)^bow)\r\n}\r\nvecBow <- seq(from=0.5,to=3,by=0.5)\r\nvecAge <- 1:15\r\nrdDT <- data.table(Bow = rep(vecBow, each=length(vecAge)),\r\n                   Age = rep(vecAge, times=length(vecBow)))\r\nrdDT[, RD:=RD_function(age=Age, bow=Bow,L=max(Age)), by=Bow]\r\n\r\nggplot(rdDT, aes(x=Age, y=RD)) + \r\n  geom_line() + \r\n  facet_wrap(vars(Bow), nrow=3) +\r\n  ylab(\"Rate of Distribution (RD)\")\r\n\r\n\r\n\r\nFigure 1: Rate of distribution (RD) in the TA model for different bow factors.\r\n\r\n\r\n\r\nImplementation in R\r\nBelow is the code for an R function to run the TA model in R, together with a function that produces outputs, including ggplot2 plots:\r\n\r\n\r\nShow code\r\n\r\n# This file includes functions to run the Takahashi / Alexander (TA) model\r\n\r\n#' This function produces vectors of contributions, distributions, and NAVs,\r\n#' based on the deterministic Takahashi / Alexander (TA) (2002) model and\r\n#' adjusted for a periodically changing growth rate, as proposed by\r\n#' Jeet (2020).\r\n#'\r\n#' The inputs of the model are explained in the parameter section. The\r\n#' inputs have to be adjusted based on the desired periodicity (annually,\r\n#' quarterly, etc.).. Three equations fully describe the model:\r\n#'\r\n#' Equation to model capital calls \\eqn{C_t} in period t, based on\r\n#' uncalled capital \\eqn{UC_t}:\r\n#'\r\n#' \\deqn{C[t] = UC[t] * .vec\\_RC[t]}\r\n#'\r\n#' \\code{.vec_RC} is the vector of rate of contributions. This one is pretty\r\n#' straightforward; the called capital in a period is the beginning-of-period\r\n#' uncalled capital multiplied with the percentage of capital assumed /\r\n#' expected to be called in the period.\r\n#'\r\n#' To model distributions \\eqn{D_t} in period t, the following equation is\r\n#' used:\r\n#'\r\n#' \\deqn{D[t] = NAV[t-1] * (1+.vec\\_growth[t]) * RD(Age[t-1],.bow,.life)}\r\n#'\r\n#' The distributions have to come from the NAV of the fund, which is grown\r\n#' by a growth factor over time. The level of distributions is then defined\r\n#' by a function RD as follows:\r\n#'\r\n#' \\deqn{RD = (\\frac{Age[t-1]}{.life})^{.bow}}\r\n#'\r\n#' The term in brackets linearly increases from 0 to 1 over the fund's\r\n#' life time \\code{.life}. With the \\code{.bow} factor, the modeler can\r\n#' control how front or backloaded those distributions are. The higher\r\n#' the bow factor, the smaller RD becomes as the factor in brackets is\r\n#' typically below 1.\r\n#'\r\n#' The NAV is modeled simply by growing it with the growth factor and\r\n#' adjusting for distributions and capital calls:\r\n#'\r\n#' \\deqn{NAV[t] = NAV[t-1]*(1+vec_growth[t]) + C[t] - D[t]}\r\n#'\r\n#'\r\n#' @param .bow numeric; defines the distribution rate over the life of the\r\n#'        investment. A higher bow factor projects more distributions\r\n#'        occurring later in the investment’s life.\r\n#' @param .life numeric; the term / lifetime of the fund.\r\n#' @param .vec_growth numeric vector; growth vector of the NAV; user can\r\n#'        also only supply a scaler / one number, in which case a constant\r\n#'        growth rate is assumed.\r\n#' @param .vec_RC numeric vector; vector of rate of contributions. Valid inputs\r\n#'         are between 0 and 1.\r\n#' @param .nav numeric; NAV at the beginning; default value is 0, which implies\r\n#'         that the full term of the fund is modeled; however, the TA model\r\n#'         can also be applied to funds that are already older than age 0.\r\n#' @param .uc numeric; uncalled capital; for a new fund (e.g.\\code{.nav=0}),\r\n#'        this value is equal to the commitment amount.\r\n#' @return A data.table holding all the relevant information of the fund:\r\n#'         Inputs of TA model: Age, bow factor, rate of contributions, growth rate\r\n#'         Outputs of TA model: calls and distributions (also cumulated); NAV\r\n#'         and uncalled capital both at the beginning of the period (BOP) and\r\n#'         end of period (EOP).\r\n#'\r\n#' @export\r\n#'\r\n#' @references\r\n#' Takahashi / Alexander (2002): Illiquid Alternative Asset Fund Modeling,\r\n#' The Journal of Portfolio Management\r\n#' Jeet (2020): Modeling Private Investment Cash Flows with\r\n#' Market-Sensitive Periodic Growth, PGIM\r\n#'\r\n#' @examples\r\n#' # Taken from Jeet (2020): Modeling Private Investment Cash Flows\r\n#' # with Market_Sensitive Periodic Growth\r\n#' run_TA_model(.bow=2,\r\n#'              .life=13,\r\n#'              .nav=0,\r\n#'              .uc=1,\r\n#'              .vec_growth = c(0.05,0.04,0.06,0.01,-0.01,0.02,\r\n#'                              0.07,0.08,0.02,0.05,0.07,0.02,0.03),\r\n#'              .vec_RC = c(0.25,0.33,rep(0.5,11)))\r\n#' # Run it quarterly (not quite the same as above, as cash flows are not\r\n#' # compouned for a full year anymore)\r\n#' run_TA_model(.bow=2,\r\n#'              .life=13*4,\r\n#'              .nav=0,\r\n#'              .uc=1,\r\n#'              .vec_growth = rep(c(0.05,0.04,0.06,0.01,-0.01,0.02,\r\n#'                              0.07,0.08,0.02,0.05,0.07,0.02,0.03),\r\n#'                              each=4)/4,\r\n#'              .vec_RC = rep(c(0.25,0.33,rep(0.5,11)),\r\n#'                        each=4)/5)\r\n#' # Model fund that is already running for a few years\r\n#' run_TA_model(.bow=2,\r\n#'              .life=7,\r\n#'              .nav=0.8,\r\n#'              .uc=0.15,\r\n#'              .vec_growth = rep(0.07, times=7),\r\n#'              .vec_RC =     rep(0.75,times=7))\r\nrun_TA_model <- function(.bow, .life, .vec_growth, .vec_RC,\r\n                         .nav = 0, .uc) {\r\n\r\n\r\n  ####### Checks\r\n  #'#################################################################################\r\n  # Check that .vec_growth and .vec_RC are the same length\r\n  len <- length(.vec_growth)\r\n  #If user used scalar, create vector with same length as .vec_RC\r\n  if (len==1) {\r\n    .vec_growth <- rep(.vec_growth, times=length(.vec_RC))\r\n  } else if (len!=length(.vec_RC)) {\r\n    stop(\"run_TA_model: .vec_growth and .vec_RC not of same length.\")\r\n  } else if (length(.vec_RC)!=.life) {\r\n    stop(\"run_TA_model: the length of .vec_RC has to be the same as .life\")\r\n  }\r\n\r\n  #'#################################################################################\r\n  # Check that assumptions are reasonable\r\n  if (!is.numeric(.bow) | .bow<0 | length(.bow)!=1) {\r\n    stop(\"Bow factor .bow should be numeric, not a vector, and non-negative.\")\r\n  }\r\n  if (!is.numeric(.life) | .life<0 | length(.life)!=1 | .life<2) {\r\n    stop(\"Life .life should be numeric, not a vector, larger than 1, and non-negative.\")\r\n  }\r\n  if (!is.numeric(.nav) | .nav<0 | length(.nav)!=1) {\r\n    stop(\"NAV .nav should be numeric, not a vector, and non-negative.\")\r\n  }\r\n  if (!is.numeric(.uc) | .uc<0 | length(.uc)!=1) {\r\n    stop(\"Uncalled capital .uc should be numeric, not a vector, and non-negative.\")\r\n  }\r\n\r\n  ### Calculate the vector of rate of distributions\r\n  vec_RD <- ((1:.life)/.life)^.bow\r\n\r\n  ### Initialize vectors\r\n  vec_CC  <- numeric(.life)\r\n  vec_D   <- numeric(.life)\r\n  vec_NAV <- numeric(.life+1)\r\n  vec_UC  <- numeric(.life+1)\r\n  vec_NAV[1] <- .nav\r\n  vec_UC[1]  <- .uc\r\n\r\n  ### Loop through each period to calculate values\r\n  for (t in 1:.life) {\r\n    vec_CC[t] <- vec_UC[t] * .vec_RC[t]\r\n    vec_D[t]  <- vec_NAV[t] * (1+.vec_growth[t])*vec_RD[t]\r\n    vec_NAV[t+1] <- vec_NAV[t] * (1+.vec_growth[t]) + vec_CC[t] - vec_D[t]\r\n    vec_UC[t+1]  <- vec_UC[t] - vec_CC[t]\r\n  }\r\n\r\n  return(data.table::data.table(Age      = 1:.life,\r\n                                Bow      = .bow,\r\n                                RC       = .vec_RC,\r\n                                Growth   = .vec_growth,\r\n                                Calls    = vec_CC,\r\n                                Dist     = vec_D,\r\n                                CumCalls = cumsum(vec_CC),\r\n                                CumDist  = cumsum(vec_D),\r\n                                NAV_BOP  = vec_NAV[1:.life],\r\n                                NAV_EOP  = vec_NAV[2:(.life+1)],\r\n                                UC_BOP   = vec_UC[1:.life],\r\n                                UC_EOP   = vec_UC[2:(.life+1)]))\r\n\r\n}\r\n\r\n#' Produces plots and output statistics for function \\code{\\link{run_TA_model}}\r\n#'\r\n#' This function takes as an input the data.table produced by \\code{\\link{\r\n#' run_TA_model}} and produces output plots and statistics.\r\n#'\r\n#' @param .dt_TA data.table, as returned by function \\code{\\link{run_TA_model}}\r\n#' @return A list containing three elements: 1) plotTA: a ggplot with the\r\n#'         the distributions, calls, uncalled capital and NAV; 2) plotCumTA:\r\n#'         the same as plotTA, but with Dist and Calls cumulated; 3) statsTA:\r\n#'         a data.table including the multiple and IRR of the fund.\r\n#'\r\n#' @importFrom data.table melt\r\n#' @importFrom data.table data.table\r\n#' @importFrom utilitiesCJ IRR_fixed_intervals\r\n#' @importFrom ggplot2 ggplot\r\n#'\r\n#' @export\r\n#' @examples\r\n#' # Taken from Jeet (2020): Modeling Private Investment Cash Flows\r\n#' # with Market_Sensitive Periodic Growth\r\n#' require(data.table)\r\n#' require(ggplot2)\r\n#' require(utilitiesCJ)\r\n#' dt_TA <- run_TA_model(.bow=2,\r\n#'              .life=13,\r\n#'              .nav=0,\r\n#'              .uc=1,\r\n#'              .vec_growth = 0.12,\r\n#'              .vec_RC = c(0.25,0.33,rep(0.5,11)))\r\n#' output_dt_TA <- output_TA_model(.dt_TA = dt_TA)\r\noutput_TA_model <- function(.dt_TA) {\r\n\r\n  melt_dt_TA <- data.table::melt(.dt_TA,id.vars = \"Age\")\r\n\r\n  plotTA <- ggplot2::ggplot(melt_dt_TA[variable==\"Age\"  | variable==\"Calls\" |\r\n                             variable==\"Dist\" | variable==\"NAV_BOP\" |\r\n                             variable==\"UC_BOP\"],\r\n                  aes(x=Age, y=value, color= variable)) +\r\n    geom_line() + scale_y_continuous(labels = scales::percent) +\r\n    labs(caption = paste0(\"Bow factor: \", .dt_TA[1,Bow]))\r\n\r\n  plotCumTA <- ggplot2::ggplot(melt_dt_TA[variable==\"Age\"  | variable==\"CumCalls\" |\r\n                                         variable==\"CumDist\" | variable==\"NAV_BOP\" |\r\n                                         variable==\"UC_BOP\"],\r\n                            aes(x=Age, y=value, color= variable)) +\r\n    geom_line() + scale_y_continuous(labels = scales::percent) +\r\n    labs(caption = paste0(\"Bow factor: \", .dt_TA[1,Bow]))\r\n\r\n  statsTA <- data.table(TVPI = .dt_TA$CumDist[nrow(.dt_TA)]/.dt_TA$CumCalls[nrow(.dt_TA)],\r\n                        IRR  = utilitiesCJ::IRR_fixed_intervals(.vec_cf = .dt_TA$Dist - .dt_TA$Calls))\r\n\r\n\r\n  return(list(plotTA    = plotTA,\r\n              plotCumTA = plotCumTA,\r\n              statsTA   = statsTA))\r\n}\r\n\r\n\r\nLet’s take those functions for a test drive. Concretely, let’s model a fund with 12-year life, a bow factor of 2, and an annual growth of 12%, i.e., a 12% IRR. The rate of contribution is 40%.\r\n\r\n\r\nShow code\r\n\r\nt <- 12\r\nres_TA <- run_TA_model(.bow=2,\r\n                       .life=t,\r\n                       .nav=0,\r\n                       .uc=1,\r\n                       .vec_growth = rep(0.12, times=t),\r\n                       .vec_RC =     rep(0.4, times=t))\r\noutput_TA <- output_TA_model(res_TA)\r\n\r\n\r\nIt’s now straightforward to get the relevant information of the output object, output_TA in the above example. To get the plot of cumulative calls and distributions over time as well the NAV and unfunded commitment, one can call output_TA:\r\n\r\n\r\nShow code\r\n\r\noutput_TA$plotCumTA\r\n\r\n\r\n\r\nFigure 2: Cumulative calls, distributions, NAV, and unfunded commitment in the TA model. Inputs: bow factor=2, L=12, RC=0.4,G=0.12.\r\n\r\n\r\n\r\nThe TVPI and IRR are obtained by calling output_TA$statsTA$TVPIand output_TA$statsTA$IRR, respectively.\r\nThanks to Rs capabilities, it’s also pretty easy to compare the results for different inputs. For example, let’s look at how the results change for different bow factors, while leaving all the other inputs the same.\r\n\r\n\r\nShow code\r\n\r\nl_results <- lapply(seq(from=0,to=4,by=0.5),\r\n                    run_TA_model,\r\n                       .life=t,\r\n                       .nav=0,\r\n                       .uc=1,\r\n                       .vec_growth = rep(0.12, times=t),\r\n                       .vec_RC =     rep(0.4, times=t))\r\nDT <- rbindlist(l_results)\r\nmeltDT <- melt(DT[, list(Age, Bow, CumCalls,CumDist, NAV_BOP)], id.vars = c(\"Age\", \"Bow\"))\r\nggplot(meltDT, aes(x=Age, y=value, color=variable)) + \r\n  geom_line() + \r\n  facet_wrap(vars(Bow), nrow=3) + ylab(\"\")\r\n\r\n\r\n\r\nFigure 3: Cumulative calls, distributions, and NAV in the TA model for different bow factors. Inputs: L=12, RC=0.4,G=0.12.\r\n\r\n\r\n\r\nShow code\r\n\r\nresults_TVPI_IRR <- rbindlist(lapply(l_results, function(x) output_TA_model(x)$statsTA))\r\nresults_TVPI_IRR[, Bow:=seq(from=0,to=4,by=0.5)]\r\nlibrary(kableExtra)\r\nkbl(results_TVPI_IRR[,list(Bow,TVPI,IRR)],\r\n    digits=3,\r\n    caption=paste0(\"TVPI and IRR for different bow factors.\")) %>%\r\n  kable_classic(full_width = FALSE) \r\n\r\n\r\nTable 1: TVPI and IRR for different bow factors.\r\n\r\n\r\nBow\r\n\r\n\r\nTVPI\r\n\r\n\r\nIRR\r\n\r\n\r\n0.0\r\n\r\n\r\n1.118\r\n\r\n\r\n0.119\r\n\r\n\r\n0.5\r\n\r\n\r\n1.233\r\n\r\n\r\n0.120\r\n\r\n\r\n1.0\r\n\r\n\r\n1.383\r\n\r\n\r\n0.120\r\n\r\n\r\n1.5\r\n\r\n\r\n1.547\r\n\r\n\r\n0.120\r\n\r\n\r\n2.0\r\n\r\n\r\n1.705\r\n\r\n\r\n0.120\r\n\r\n\r\n2.5\r\n\r\n\r\n1.850\r\n\r\n\r\n0.120\r\n\r\n\r\n3.0\r\n\r\n\r\n1.978\r\n\r\n\r\n0.120\r\n\r\n\r\n3.5\r\n\r\n\r\n2.090\r\n\r\n\r\n0.120\r\n\r\n\r\n4.0\r\n\r\n\r\n2.188\r\n\r\n\r\n0.120\r\n\r\n\r\nThe higher the bow factor, the more backloaded the distributions are. This implies a higher multiple as there is more time for value creation, while the IRR, or growth rate of value, stays the same, at 12% in this case. The example illustrates nicely that a benchmark based on the TVPI alone is meaningless: a multiple of 2x can either be good or bad, depending on how long it took to generate it.\r\nFinal remarks\r\nIn this post, I have introduced the TA model and have shown how to implement it in R. Of course, this is the easy part. The much harder part would be the calibration of the model. What parameters should one use to derive reasonable outcomes? To do so, one would have to get real cash flow data of funds and run regressions to estimate the parameters. As the function is non-linear, some adjustments have to be made. My statistic knowledge is diminishing, but this post seems like a good starting point to do so. Fundamentally, the issue I see with calibration is that most funds actually have highly non-linear functions of cash flows, in particular distributions, which do not resemble the curves of the TA model at all. In particular (lower) mid-market buyout funds with only 5-6 investments might not distribute at all in a year or two, just to have large distributions in the next year. Hence, the TA model might therefore be better suited to model portfolio of funds, as the actual curves get the smoother, the more diversified the portfolio is.\r\n\r\n\r\n\r\nJeet, Vishv. 2020. “Modeling Private Investment Cash Flows with Market-Sensitive Periodic Growth.” PGIM IAS-October.\r\n\r\n\r\nTakahashi, Dean, and Seth Alexander. 2002. “Illiquid Alternative Asset Fund Modeling.” The Journal of Portfolio Management 28 (2): 90–100.\r\n\r\n\r\nThis is the one modification of Jeet (2020) that I mentioned above. In his model, the growth rate is time-varying and not fixed as in Takahashi and Alexander (2002).↩︎\r\n",
    "preview": "posts/2023-02-05-the-takahashi-and-alexander-model/the-takahashi-and-alexander-model_files/figure-html5/PlotRDfunction-1.png",
    "last_modified": "2023-04-16T19:40:43+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-13-the-lbo-value-bridge-revisited/",
    "title": "The LBO value bridge - revisited",
    "description": "Some additional thoughts about the LBO value bridge.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2022-11-13",
    "categories": [
      "Valuation",
      "R",
      "Private Equity"
    ],
    "contents": "\r\nAfter studying my previous post about the LBO value bridge, a reader recently reached out to me with an interesting question: how to separate organic and acquired EBITDA growth in the LBO value bridge? As buy-and-build platforms have become more and more prominent in private equity, I gave it some thought, and also tried to figure out more generally how to expand the value creation bridge analysis.1\r\nRecap\r\nTo start with, remember from the previous post that the goal is to explain the total net proceeds, \\(TNP\\), over a period \\(t\\) to \\(T\\). The total net proceeds are essentially the difference between the equity proceeds to equity holders at exit, \\(E_T\\), and the invested cost of equity at entry, \\(E_t\\), as well as any interim dividends and equity injections. Back then, I replaced the equity values with enterprise values minus net debt and continued to calculate the enterprise value with the product of the EBITDA and its multiple. Some steps further, I ended up with a formula that explained the total net proceeds as a combination of the following factors:\r\nEBITDA growth: \\(m_t \\Delta C\\)\r\nChange in the EV/EBITDA valuation multiple: \\(\\Delta m C_t\\)\r\nChanges in net debt/cash: \\(\\Delta ND\\)\r\nA combination of 1. and 2.: \\(\\Delta m \\Delta C\\)\r\nAs a reminder, the formula looked as follows:\r\n\\[\r\nTNP = m_t \\Delta C + \\Delta m C_t + \\Delta m \\Delta C - \\Delta ND + \\sum_{n=t+1}^{T-1} E_n\r\n\\]\r\nHowever, it’s important to keep in mind that the value bridge can be applied independently of any specific metric. Instead of looking at EBITDA, one can also use other metrics such as Free Cash Flow to the Firm.2 One can also add further factors, such as dilution by management, transaction costs, etc. The only important condition is that all factors on the right end up explaining the total net proceeds on the left.\r\nExample: Split organically grown and acquired EBITDA\r\nAs an example, let’s separate between organically grown EBITDA and acquired EBITDA. To do so, I change the definition of \\(\\Delta C\\) as the change of the EBITDA of the original platform as well as all add-ons, excluding the acquired EBITDA base of the add-ons. To consider for the acquired EBITDA, I include two new terms:\r\nValue of the acquired EBITDA: \\(m_T * C_{T,acq}\\)\r\nCosts of the acquired EBITDA: \\(-X_{acq}\\)\r\nThe equation looks then as follows:\r\n\\[\r\nTNP = m_t \\Delta C + \\Delta m C_t + \\Delta m \\Delta C - \\Delta ND + m_T * C_{T,acq} - X_{acq} + \\sum_{n=t+1}^{T-1} E_n\r\n\\]\r\nOf course, I could have also subsumed the acquisition costs in either the net debt change or further capital injections and saved one bar in the chart. Then, however, the value bridge wouldn’t clearly show how much value was actually created by acquiring additional EBITDA. Another decision I made is that any growth of the acquired EBITDA is considered in the original EBITDA term. These decisions are all arbitrary and one could make different assumptions. It really depends on what the user wants to get out of the value bridge and what data he has to start with. I discuss this point further below.\r\n\r\nFor example, add-ons are often acquired with the purpose of realizing synergies. To the extent the EBITDA of both the platform and/or the add-ons grows due to such synergies and can be measured, one could include yet another term that separates them.\r\nBelow, I update the code and run the original example I took from Puche but assume further that the company acquired an additional EBITDA of 10 for total costs of 50 - hence, with an implicit entry multiple of 5x. As the platform is ultimately sold for 11x, there is an additional gain of 60 for equity owners.\r\n\r\n\r\nShow code\r\n\r\n#' Calculate the value creation bridge for a PE investment, splitting it up in leverage, multiple and EBITDA effect.\r\n#' The EBITDA effect is further split in an organic and inorganic part.\r\n#'\r\n#' The function creates the value creation for LBOs; detailed explanation about methodology can be found in Puche (2016) and Pindur (2007).\r\n#' The split-up of the total proceeds (which are net of invested cost, i.e. the net capital gain) in multiple effect, EBITDA effect,\r\n#' combination effect and FCF effect is straightforward and best explained\r\n#' in Pindur (2007). Pindur then scales everything simply by dividing by the total proceeds so that everything adds to 1.\r\n#'\r\n#' In addition, Puche (2016, see footnote 23) focuses on the multiple, which is partly a result of leverage; hence, he\r\n#' unlevers first the multiple and the difference between the levered and unlevered mutiple is the multiple effect.\r\n#'\r\n#'@param .startEquity numeric; equity valuation at start date of the observation period (typically at entry), i.e. EV minus the debt at start point.\r\n#'                    Careful: if fund does not acquire 100 per cent of equity, you have to scale the fund's investment up.\r\n#'@param .endEquity numeric; equity valuation at end date of observation period.\r\n#'@param .startDebt numeric; debt at start date.\r\n#'@param .endDebt numeric; debt at end date.\r\n#'@param .startRev numeric; revenue at start date.\r\n#'@param .endRev numeric; revenue at end date.\r\n#'@param .startEBITDA numeric; EBITDA at start date.\r\n#'@param .endEBITDA numeric; EBITDA at end date. As you model the acquired EBITDA separately, this is now only the organic EBITDA.\r\n#'@param .acqEBITDA numeric; EBITDA acquired during ownership.\r\n#'@param .acqCosts numeric; total costs for acquiring EBITDA during ownership. Note: these could also be modeled in \\code{.interimCC} or \r\n#'        change in net debt, but goal is to make it visible.\r\n#'@param .interimCC numeric; interim additional equity investments. Careful with scaling, should be 100 per cent of equity.\r\n#'@param .interimDist numeric; interim distributions to equity from investments. Careful with scaling, should be 100  per cent of equity.\r\n#'@param .interimIntRate numeric; the interest rate during the holding period for that specific deal. \r\n#'@param .holdingPeriod numeric; holding period (in years) from start to end date of the observation.\r\n#'@return List including the following: 1) A vector with the levered and unlevered multiple (TM_lev and TM_unlev), \r\n#'        the overall gain / (loss) on the deal, and \r\n#'        the leverage effect, which is the difference between TM_lev and TM_unlev; it then includes the different effects (FCF, EBITDA, etc.) \r\n#'        that explain the gain. Important: those effects are shown in absolute numbers. 2) A data.table with the relevant input parameters. \r\n#'        3) A data.table with the relevant results.\r\n#'@export\r\n#'\r\n#'@references\r\n#' Puche (2016): Essays on Value Creation and its Determinants in Private Equity, Dissertation, p. 29ff\r\n#' Pindur (2007): Value Creation in Successful LBOs, Dissertation, p. 69ff\r\n#'\r\n#'@examples\r\n#' ##### Use numerical example from Puche (2016, Table 2-1); adjusted for acquired EBITDA\r\n#' list_results_update <- value_creation_LBO(.startEquity         = 50,\r\n#'                    .endEquity           = 135,\r\n#'                    .startDebt           = 50,\r\n#'                    .endDebt             = 30,\r\n#'                    .startRev            = 100,\r\n#'                    .endRev              = 120,\r\n#'                    .startEBITDA         = 10,\r\n#'                    .endEBITDA           = 15,\r\n#'                    .acqEBITDA           = 5,\r\n#'                    .acqCosts,           = 25,\r\n#'                    .interimCC           = 15,\r\n#'                    .interimDist         = 20,\r\n#'                    .interimIntRate      = 0.09,\r\n#'                    .holdingPeriod       = 4)\r\nvalue_creation_LBO_update <- function(.startEquity, .endEquity, .startDebt, .endDebt, .startRev, .endRev, .startEBITDA, .endEBITDA,\r\n                               .acqEBITDA=0, .acqCosts=0,\r\n                               .interimCC, .interimDist, .interimIntRate, .holdingPeriod) {\r\n  \r\n  ### Calculations of additional inputs\r\n  startEV <- .startEquity + .startDebt\r\n  endEV   <- .endEquity   + .endDebt + .acqCosts\r\n  startEV_EBITDA_mult <- startEV/.startEBITDA\r\n  endEV_EBITDA_mult   <- endEV/(.endEBITDA + .acqEBITDA)\r\n  costDebt <- (1+.interimIntRate)^.holdingPeriod - 1\r\n  invCapital <- .startEquity + .interimCC\r\n  deltaEquity <- .endEquity - .startEquity\r\n  gain <- deltaEquity + .interimDist - .interimCC\r\n  avgDebtEquity <- (.startDebt/.startEquity + (.endDebt + .acqCosts)/.endEquity)/2 #TODO: Understand why you need the average here\r\n  startMargin <- .startEBITDA/.startRev\r\n  endMargin   <- .endEBITDA/.endRev\r\n  \r\n  ### Calculations of output\r\n  TM_lev                  <- gain/invCapital\r\n  TM_unlev                <- (TM_lev + costDebt * avgDebtEquity)/(1 + avgDebtEquity)\r\n  lev_effect              <- TM_lev - TM_unlev\r\n  mult_effect             <- .startEBITDA * (endEV_EBITDA_mult - startEV_EBITDA_mult)\r\n  mult_EBITDA_comb_effect <- (.endEBITDA - .startEBITDA) * (endEV_EBITDA_mult - startEV_EBITDA_mult)\r\n  FCF_effect              <- -(.endDebt - .startDebt) + .interimDist - .interimCC\r\n  EBITDA_effect           <- (.endEBITDA - .startEBITDA) * startEV_EBITDA_mult\r\n  Acq_EBITDA_effect       <- .acqEBITDA * endEV_EBITDA_mult\r\n  Rev_effect              <- (.endRev - .startRev) * startMargin * startEV_EBITDA_mult\r\n  margin_effect           <- (endMargin - startMargin) * .startRev  * startEV_EBITDA_mult\r\n  SM_comb_effect          <- (.endRev - .startRev) * (endMargin - startMargin) * startEV_EBITDA_mult\r\n  \r\n  vec_results <-c(TM_lev                  = TM_lev,\r\n                  TM_unlev                = TM_unlev,\r\n                  gain                    = gain,\r\n                  lev_effect              = lev_effect,\r\n                  mult_effect             = mult_effect,\r\n                  mult_EBITDA_comb_effect = mult_EBITDA_comb_effect,\r\n                  EBITDA_effect           = EBITDA_effect,\r\n                  FCF_effect              = FCF_effect,\r\n                  Rev_effect              = Rev_effect,\r\n                  margin_effect           = margin_effect,\r\n                  SM_comb_effect          = SM_comb_effect,\r\n                  startEV_EBITDA_mult     = startEV_EBITDA_mult,\r\n                  endEV_EBITDA_mult       = endEV_EBITDA_mult,\r\n                  startEV                 = startEV,\r\n                  endEV                   = endEV,\r\n                  totalInv                = .startEquity + .interimCC,\r\n                  TM_mult                 = mult_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_mult_EBITDA_comb     = mult_EBITDA_comb_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_EBITDA               = EBITDA_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_FCF                  = FCF_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_Rev                  = Rev_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_margin               = margin_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_SM_comb              = SM_comb_effect/abs(gain) * abs(TM_unlev),\r\n                  Acq_EBITDA_effect       = Acq_EBITDA_effect,\r\n                  Acq_Costs_effect        = -.acqCosts,\r\n                  TM_Acq_EBITDA           = Acq_EBITDA_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_Acq_Costs            = -.acqCosts/abs(gain) * abs(TM_unlev))\r\n  \r\n  print_table_inputs <- data.table(\r\n    Item  = c(\"Revenue\", \"EBITDA\", \"Equity\", \"Net Debt\", \"EV\", \"EV/EBITDA\", \"CC\", \"Dist\",\r\n              \"Holding period\", \"Interest rate p.a.\",  \"Avg debt/equity ratio\", \"Cost of debt\"),\r\n    Entry = c(.startRev, .startEBITDA, .startEquity, .startDebt, startEV, startEV_EBITDA_mult,\r\n              rep(NA, 6)),\r\n    Interim = c(NA, .acqEBITDA, NA, .acqCosts, NA, NA, .interimCC, .interimDist, \r\n                .holdingPeriod, .interimIntRate, avgDebtEquity, costDebt),\r\n    Exit    = c(.endRev, .endEBITDA, .endEquity, .endDebt, endEV, endEV_EBITDA_mult, rep(NA, 6))\r\n  )\r\n  \r\n  print_table_outputs <- data.table(\r\n    Item  = c(\"TM (levered)\", \r\n              \"Gain / TM (unlevered)\",\r\n              \"FCF\",\r\n              \"Comb. multiple / EBITDA\",\r\n              \"Multiple\",\r\n              \"Acquired EBITDA\",\r\n              \"Costs acquired EBITDA\",\r\n              \"EBITDA\",\r\n              \"Comb. revenue/margin\",\r\n              \"Revenue\",\r\n              \"Margin\"),\r\n    Absolute = c(NA, vec_results[c(3,8,6,5,24,25,7,11,9,10)]),\r\n    TM       = c(vec_results[c(1,2,20,18,17,26,27,19,23,21,22)])\r\n  )\r\n  \r\n  \r\n  ### Output\r\n  return(list(vec_results         = vec_results,\r\n              print_table_inputs  = print_table_inputs,\r\n              print_table_outputs = print_table_outputs))\r\n  \r\n}\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#' Wrapper function for ggplot to plot value bridge for function \\code{\\link{value_creation_LBO_update}},\r\n#' adjusted for accquired EBITDA.\r\n#'\r\n#' @param .list_value_creation_LBO list; output from function \\code{\\link{value_creation_LBO_update}}.\r\n#' @return ggplot2 bar plot that shows the value creation bridge.\r\n#'\r\n#' @import ggplot2\r\n#' @export\r\n#'\r\n#' @references\r\n#' \\url{https://learnr.wordpress.com/2010/05/10/ggplot2-waterfall-charts/}\r\nplot_value_bridge_update <- function(.vec_value_creation_LBO, .fillColors = c(\"#FFC300\", \"#581845\")) {\r\n  \r\n  #Calculate inbetween multiples\r\n  mult_OpImpr <- .vec_value_creation_LBO[\"TM_unlev\"] - .vec_value_creation_LBO[\"TM_mult\"] - .vec_value_creation_LBO[\"TM_mult_EBITDA_comb\"]\r\n  \r\n  strDesc <- c(\"Combination S/M effect\", \"Margin effect\", \"Sales effect\", \"EBITDA effect\",\"Acquired EBITDA effect\",\r\n               \"Acquired EBITDA costs\",  \"FCF effect\", \"Operating improvements\",\r\n               \"Combination effect\", \"Multiple effect\", \"TM (unlev)\", \"Leverage effect\", \"TM (lev)\")\r\n  \r\n  plotDF <- data.frame(Desc  = factor(strDesc, levels = strDesc),\r\n                       Type  = c(\"Bridge\", \"Bridge\", \"Bridge\", \"Full\",  \"Bridge\", \"Bridge\", \"Bridge\", \"Full\", \r\n                                 \"Bridge\", \"Bridge\", \"Full\", \"Bridge\", \"Full\"),\r\n                       Start = c(0,\r\n                                 .vec_value_creation_LBO[\"TM_SM_comb\"], \r\n                                 .vec_value_creation_LBO[\"TM_SM_comb\"] + .vec_value_creation_LBO[\"TM_margin\"], \r\n                                 0,\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"], \r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"] + .vec_value_creation_LBO[\"TM_Acq_EBITDA\"],\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"] + .vec_value_creation_LBO[\"TM_Acq_EBITDA\"] + \r\n                                                                        .vec_value_creation_LBO[\"TM_Acq_Costs\"],\r\n                                 0,\r\n                                 mult_OpImpr,\r\n                                 mult_OpImpr + .vec_value_creation_LBO[\"TM_mult_EBITDA_comb\"], \r\n                                 0,\r\n                                 .vec_value_creation_LBO[[\"TM_unlev\"]],\r\n                                 0),\r\n                       End   = c(.vec_value_creation_LBO[\"TM_SM_comb\"],\r\n                                 .vec_value_creation_LBO[\"TM_SM_comb\"] + .vec_value_creation_LBO[\"TM_margin\"],\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"],\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"],\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"] + .vec_value_creation_LBO[\"TM_Acq_EBITDA\"],\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"] + .vec_value_creation_LBO[\"TM_Acq_EBITDA\"] + \r\n                                                                        .vec_value_creation_LBO[\"TM_Acq_Costs\"],\r\n                                 mult_OpImpr,\r\n                                 mult_OpImpr,\r\n                                 mult_OpImpr + .vec_value_creation_LBO[\"TM_mult_EBITDA_comb\"],\r\n                                 .vec_value_creation_LBO[[\"TM_unlev\"]],\r\n                                 .vec_value_creation_LBO[[\"TM_unlev\"]],\r\n                                 .vec_value_creation_LBO[[\"TM_lev\"]],\r\n                                 .vec_value_creation_LBO[[\"TM_lev\"]]))\r\n  plotDF$Amount <- plotDF$End - plotDF$Start\r\n  plotDF$ID <- 1:nrow(plotDF)\r\n  \r\n  #https://stackoverflow.com/questions/50688764/r-ggplot2-ignoring-unknown-aesthetics-with-geom-rect\r\n  suppressWarnings(ggplot(plotDF, aes(x=Desc, fill = Type)) +\r\n                     scale_fill_manual(values = .fillColors) +\r\n                     geom_rect(aes(x=Desc, xmin = ID - 0.45, xmax = ID + 0.45, ymin = Start, ymax = End)) +\r\n                     xlab(\"\") +\r\n                     theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1)))\r\n  \r\n}\r\n\r\n\r\nLet’s first plot the inputs again. The acquired EBITDA is shown in the interim column and so are the acquisition costs (under net debt).\r\n\r\n\r\nShow code\r\n\r\n##### Use numerical example from Puche (2016, Table 2-1), updated for acquired EBITDA\r\nacqEBITDA <- 10\r\nacqCosts  <- 50\r\nlist_results <- value_creation_LBO_update(.startEquity = 50,\r\n                   .endEquity           = 135 + acqEBITDA*11 - acqCosts, #Add additional value created by add-ons to exit equity\r\n                   .startDebt           = 50,\r\n                   .endDebt             = 30,\r\n                   .startRev            = 100,\r\n                   .endRev              = 120,\r\n                   .startEBITDA         = 10,\r\n                   .endEBITDA           = 15,\r\n                   .acqEBITDA           = acqEBITDA,\r\n                   .acqCosts            = acqCosts,\r\n                   .interimCC           = 15,\r\n                   .interimDist         = 20,\r\n                   .interimIntRate      = 0.09,\r\n                   .holdingPeriod       = 4)\r\nkbl(list_results$print_table_inputs,\r\n    caption=\"Inputs of value creation analysis of Puche's example; adjusted for acquired EBITDA with multiple expansion\") %>%\r\n  kable_classic(full_width = FALSE)\r\n\r\n\r\nTable 1: Inputs of value creation analysis of Puche’s example; adjusted for acquired EBITDA with multiple expansion\r\n\r\n\r\nItem\r\n\r\n\r\nEntry\r\n\r\n\r\nInterim\r\n\r\n\r\nExit\r\n\r\n\r\nRevenue\r\n\r\n\r\n100\r\n\r\n\r\nNA\r\n\r\n\r\n120\r\n\r\n\r\nEBITDA\r\n\r\n\r\n10\r\n\r\n\r\n10.0000000\r\n\r\n\r\n15\r\n\r\n\r\nEquity\r\n\r\n\r\n50\r\n\r\n\r\nNA\r\n\r\n\r\n195\r\n\r\n\r\nNet Debt\r\n\r\n\r\n50\r\n\r\n\r\n50.0000000\r\n\r\n\r\n30\r\n\r\n\r\nEV\r\n\r\n\r\n100\r\n\r\n\r\nNA\r\n\r\n\r\n275\r\n\r\n\r\nEV/EBITDA\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n11\r\n\r\n\r\nCC\r\n\r\n\r\nNA\r\n\r\n\r\n15.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nDist\r\n\r\n\r\nNA\r\n\r\n\r\n20.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nHolding period\r\n\r\n\r\nNA\r\n\r\n\r\n4.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nInterest rate p.a.\r\n\r\n\r\nNA\r\n\r\n\r\n0.0900000\r\n\r\n\r\nNA\r\n\r\n\r\nAvg debt/equity ratio\r\n\r\n\r\nNA\r\n\r\n\r\n0.7051282\r\n\r\n\r\nNA\r\n\r\n\r\nCost of debt\r\n\r\n\r\nNA\r\n\r\n\r\n0.4115816\r\n\r\n\r\nNA\r\n\r\n\r\nRunning the value bridge analysis with the updated numbers produces the following results:\r\n\r\n\r\nShow code\r\n\r\nkbl(list_results$print_table_outputs,\r\n    caption=\"Results of value creation analysis of Puche's example; adjusted for acquired EBITDA with multiple expansion\") %>%\r\n  kable_classic(full_width = FALSE)\r\n\r\n\r\nTable 2: Results of value creation analysis of Puche’s example; adjusted for acquired EBITDA with multiple expansion\r\n\r\n\r\nItem\r\n\r\n\r\nAbsolute\r\n\r\n\r\nTM\r\n\r\n\r\nTM (levered)\r\n\r\n\r\nNA\r\n\r\n\r\n2.3076923\r\n\r\n\r\nGain / TM (unlevered)\r\n\r\n\r\n150\r\n\r\n\r\n1.5235864\r\n\r\n\r\nFCF\r\n\r\n\r\n25\r\n\r\n\r\n0.2539311\r\n\r\n\r\nComb. multiple / EBITDA\r\n\r\n\r\n5\r\n\r\n\r\n0.0507862\r\n\r\n\r\nMultiple\r\n\r\n\r\n10\r\n\r\n\r\n0.1015724\r\n\r\n\r\nAcquired EBITDA\r\n\r\n\r\n110\r\n\r\n\r\n1.1172967\r\n\r\n\r\nCosts acquired EBITDA\r\n\r\n\r\n-50\r\n\r\n\r\n-0.5078621\r\n\r\n\r\nEBITDA\r\n\r\n\r\n50\r\n\r\n\r\n0.5078621\r\n\r\n\r\nComb. revenue/margin\r\n\r\n\r\n5\r\n\r\n\r\n0.0507862\r\n\r\n\r\nRevenue\r\n\r\n\r\n20\r\n\r\n\r\n0.2031449\r\n\r\n\r\nMargin\r\n\r\n\r\n25\r\n\r\n\r\n0.2539311\r\n\r\n\r\nShow code\r\n\r\nplot_value_bridge_update(list_results$vec_results)\r\n\r\n\r\n\r\nThe value bridge plot shows that the acquired EBITDA has a large positive impact on the operational performance of the company. This impact is partially offset by the costs of acquisition though. The example shows how PE firms would like the add-on strategy to work: buy add-ons cheap, sell them for a much larger multiple as part of a platform. Of course, reality is often more complicated and, in my opinion, often not enough thought is given as to why someone would pay a much higher multiple just because the add-ons are now under one umbrella. This becomes apparent when you talk with PE firms of different size: the platform for a small PE player is the add-on for a large one. Should it be valued high or low?\r\nWhich value creation levers to use\r\nThe example above showed how to incorporate one additional value creation lever, inorganic EBITDA growth. As pointed out though, one could use plenty more and the studies linked above do so: revenue growth is split up between market growth, market share growth and alpha, margins are separated between industry changes, acquired margins, and alpha, etc. Ultimately, they end up with close to 20 bars in their chart. So which levers should you use? In my opinion, the key criteria should be data availability and comparability.\r\nAt the one end of the spectrum, if you have detailed data available about a specific investment and you are interested about the levers of value creation for this specific investment, you might add as many as you deem reasonable / interesting. For example, if the investment did only three add-ons, but all three of them were important return drivers and had different characteristics, you might want to add them individually. You could also add levers that reduce the returns, such as transaction costs, or dilution by management.\r\nAt the other end of the spectrum, if you want to compare different investment cases with limited insights, you want to focus on levers that make sense for almost all of the investment cases. Almost all buyout investments report their total EBITDA; however, you often will not know how much of the EBITDA growth was due to organic growth initiatives or add-ons. You are often also in the dark about the transaction costs and other issues.\r\nTo see that this analysis is still helpful, let’s re-run the above example, but let’s map the EBITDA growth and costs from the add-ons into total EBITDA and net debt. Essentially, let’s run the analysis from the original post.\r\n\r\n\r\nShow code\r\n\r\nlist_results <- value_creation_LBO(.startEquity = 50,\r\n                   .endEquity           = 135 + acqEBITDA*11 - acqCosts, #Add additional value created by add-ons to exit equity\r\n                   .startDebt           = 50,\r\n                   .endDebt             = 30 + acqCosts,\r\n                   .startRev            = 100,\r\n                   .endRev              = 120 + acqEBITDA/(15/120), #I assume the acquired EBITDA has the same margin as the platform\r\n                   .startEBITDA         = 10,\r\n                   .endEBITDA           = 15 + acqEBITDA,\r\n                   .interimCC           = 15,\r\n                   .interimDist         = 20,\r\n                   .interimIntRate      = 0.09,\r\n                   .holdingPeriod       = 4)\r\nplot_value_bridge(list_results$vec_results)\r\n\r\n\r\n\r\nNot surprisingly, some information is lost in comparison to the previous chart. There is a large EBITDA effect and one can not figure out where it is coming from: did the company grow organically that strongly? Did it acquire add-ons? And if it acquired add-ons, did it do so cheaply or did it acquire them close to the original platform’s multiple, but then lifted meaningful synergies. However, the fact that the FCF effect is negative, despite very strong operational performance, is an indication that a fair share of the value creation must be through add-ons. If not, where did all the money go? Ultimately, both value bridges tell almost the same story.\r\n\r\nThere are many studies that apply the value bridge with an ever increasing number of factors, for example Kroll’s Created Value Attribution (2014) and Insead’s Value Creation 2.0 (2016), but while they show plenty of charts, they are remarkably reserved to show numeric examples or the formulas to derive the results.↩︎\r\nDamodaran has a very insightful recent post that shows nicely though why this might not be such a good idea.↩︎\r\n",
    "preview": "posts/2022-11-13-the-lbo-value-bridge-revisited/the-lbo-value-bridge-revisited_files/figure-html5/PucheExampleOutput-1.png",
    "last_modified": "2022-11-13T17:37:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-09-17-insights-from-burgiss-holdings-data/",
    "title": "Insights from Burgiss holdings data",
    "description": "A short review of Brown et al. (2020) with a few intriguing statistics.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2022-09-18",
    "categories": [
      "Private Equity",
      "Research",
      "Data"
    ],
    "contents": "\r\nI came across a neat working paper by Brown et al. (2020). One key issue in private equity (“PE”) research is the lack of high-quality data, in particular on underlying portfolio investments. The authors of the paper, some of the most prominent names in the field, are fortunate to get their hands on a large data set of over 45,000 investments in VC and buyout funds. Burgiss can be considered as the gold standard of PE data as they source the data directly from LPs and GPs that use it for their reporting and book-keeping - hence, it comes with very high quality. As I have outlined in another post, such high-quality data is hard to come by, so it’s exciting to see when researchers get their hands on it.\r\nIt seems the authors of the paper were just as excited about this: instead of coming up with novel hypotheses and running fancy econometric models, they just provide some summary statistics. I appreciate that. Sometimes a simple table or graph is more insightful than a regression with 10 explanatory factors.\r\n\r\nGiven the authors’ background though, I’m sure they are working hard to squeeze some novel findings out of the data and I look forward to those studies, too.\r\nDistribution of multiples\r\nThis will be a short post. The paper is short to start with, so I recommend just reading it, but I will pick those findings that I found most interesting. Let’s get started by looking at the distribution of multiples of realized investments in the sample, shown below. The graph illustrates nicely why VC funds are much riskier than buyout funds, despite more underlying investments per fund: roughly every second VC fund is a full write-off and many more do only marginally better. How do strong VC funds create their performance? By investments that don’t even show up on this chart (as it is cut at 10x). Have a read of this article about investments that made 40x (Xiaomi), 80x (Zynga), and even 300x (Google) for their investors.\r\nScreenshot of Figure 1 of the paper by Brown et al. (2020). Distribution of multiples / TVPIs for buyout and VC. Only realized investments considered.Such returns are simply not possible in a traditional buyout deal with more established companies and larger investment amounts to start with.1 However, investors get their money back much more consistently. “Only” every fifth investment is a full write-off and roughly 75% of all investments return more than the invested capital. The mean / median multiple is 2.28x / 2.07x for buyout investments, so on average the invested capital is more than doubled.\r\nNot surprisingly, returns are very cyclical, as shown below. Buyout investments done in the 2010s have seen multiples on average above 3x. In contrast, pre-GFC investments in 2007 only returned 2x, while investments in the 90s did even worse.\r\nScreenshot of Figure A.3 of the paper by Brown et al. (2020). Equal- and value-weighted mean and median multiples /TVPIs for realized investments over time, split by type of investment.Does size matter?\r\nThe paper goes on to show that there is a negative relation between the size of an investment and its multiple. The 25% largest buyout investments within a year have a value-weighted multiple that is c. 0.7x lower than the 25% smallest buyout investments (see their Figure 3, Panel A). I find this quite interesting as I have found in my blog post about PE fund performance that the weighted mean multiple of buyout funds is comparable, if not higher, than the mean / median, which implies that large buyout funds perform just as well as the smaller ones (see my Tables 4 and 6).\r\n\r\nIn their Figure A.2, they compare fund vs. holdings performance, so the data set includes both views. Hence, they could look into this topic in more detail. It might simply be the result of different data sources.\r\nFor VC investments, the authors find that the link between size and performance is positive. Their explanation is that investments in one holding are the sum across all of a fund’s exposure to a portfolio company. Obviously, VC funds are more likely to double down on investments that go well. Note that the effect is much smaller though compared to buyout.\r\nHow long is the holding period?\r\nJust like me, the authors are surprised to find that the median duration for both buyout and VC investments is five years. The surprise stems from the fact that the duration of VC funds is much longer than for buyout funds. A reconciliation of this finding can be found when we look at the relationship between performance and duration further below.\r\nScreenshot of Figure 4 of the paper by Brown et al. (2020). Distribution of duration of realized investments for buyout and VC.Is performance related to duration?\r\nSimilar to investment size, the results are opposite for buyout and VC investments:\r\nBuyout: The most successful buyout investments are exited relatively early, as can be seen in the graph below. The mean TVPI is the highest for investments exited after 3 years, while it continuously declines thereafter.\r\nVC: The opposite is true for VC investments: the longer an investment is held, the higher the multiple.\r\nScreenshot of Figure 5 of the paper by Brown et al. (2020). Multiples / TVPIs for buyout and VC investments vs. duration of investment. Only realized investments considered.To summarize: the average duration of VC and buyout investments is essentially the same. However, buyout funds exit their best investments relatively early and the multiples go down after a three year holding period. VC funds, on the other hand, generate the highest multiples with investments they were holding for a longer time and in which they invested in several financing rounds. This explains why the DPI on the fund level increases so much faster for buyout than for VC funds (see Figure 7 in my performance post).\r\nThis concludes my post about this working paper. As mentioned, do yourself a favour and download the paper, it is quite short. You will be rewarded with a few more insights that I did not cover here. For example, they have a detailed descriptive statistics table (Table A.2) that neatly summarizes their data set.\r\n\r\n\r\n\r\nBrown, Gregory W, Robert S Harris, Wendy Hu, Tim Jenkinson, Steven N Kaplan, and David T Robinson. 2020. “Private Equity Portfolio Companies: A First Look at Burgiss Holdings Data.” Available at SSRN 3532444.\r\n\r\n\r\nIn Table A.1 of the paper, you can see that the mean/median size for buyout investments in the sample is 10.8x/7.5x larger than for VC investments.↩︎\r\n",
    "preview": {},
    "last_modified": "2022-09-18T12:50:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-27-sector-performance/",
    "title": "Sector performance",
    "description": "How to use data from Kenneth French and Aswath Damodaran to understand the impact of macroeconomic events on certain sectors.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2022-04-03",
    "categories": [
      "Data",
      "Research"
    ],
    "contents": "\r\nWhenever a macroeconomic shock happens, such as the COVID pandemic or Russia’s invasion of Ukraine, investors wonder how companies from certain sectors are impacted. One way is a detailed bottom-up analysis by looking into a particular business in detail and trying to understand how demand, supply chains, labor force, etc. are impacted by the event. However, this is an unrealistic tack for investors with larger portfolios. Fortunately, millions of investors do this exercise as well and trade shares in businesses as a response to their findings. One can piggyback on this work and look at sector indices to get a rough guess on how bad a particular sector and its underlying companies are impacted. In this post, let’s look at two publicly available data sources to do this.\r\nData from Kenneth French’s website\r\nKenneth French kindly provides daily updated performance data for different sectors on his website. I wrote a short wrapper function that allows me to download the data from the website and get it into a format I can work with in R. You can find the function in the code chunk below.\r\n\r\nThe R package frenchdata is a pre-built package to efficiently download data from Kenneth French’s website into R with additional functions.\r\n\r\n\r\nShow code\r\n\r\n### Load packages that I need later\r\nlibrary(data.table)\r\nlibrary(lubridate)\r\nlibrary(ggplot2)\r\nlibrary(kableExtra)\r\nlibrary(utilitiesCJ)\r\n### Define function\r\nget_FF_data <- function(.str_file, .bol_quarterly=FALSE, ...) {\r\n\r\n  ###1. Download data\r\n  temp <- tempfile()\r\n  download.file(.str_file,temp, mode=\"wb\")\r\n  csvDT <- data.table::fread(unzip(temp), ...)\r\n  unlink(temp)\r\n\r\n\r\n  ###2. Change names of columns\r\n  #     FF doesn't label the date column, enter\r\n  data.table::setnames(csvDT,\r\n           old = \"V1\",\r\n           new = \"Date\")\r\n  #     Get rid of hyphens, don't go well with column names\r\n  data.table::setnames(csvDT,\r\n           old = grep(\"-\",names(csvDT),value=TRUE),\r\n           new = gsub(\"-\", \"\", grep(\"-\",names(csvDT),value=TRUE)))\r\n\r\n  ###3. Convert Date column to date; through warning if format changed\r\n  if (sum(is.na(lubridate::ymd(csvDT$Date)))>0) {\r\n    stop(\"Error in parsing dates to date format: it seems the format in Kenneth French files has changed.\")\r\n  }\r\n  csvDT[, Date:=lubridate::ymd(Date)]\r\n\r\n  return(csvDT)\r\n\r\n}\r\n\r\n###Change to the folder you want the data to be downlaoded\r\nsetwd(\"C:/Users/Christoph Jaeckel/Downloads/\")\r\n###Go to Kenneth French's website (https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) and get the link to the data file you want to download\r\n### Change link here to play around with other sector classifications\r\nstr_file <- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/12_Industry_Portfolios_daily_CSV.zip\"\r\n#str_file <- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/17_Industry_Portfolios_daily_CSV.zip\"\r\n#str_file <- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/38_Industry_Portfolios_daily_CSV.zip\"\r\n#str_file <- \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/49_Industry_Portfolios_daily_CSV.zip\"\r\nDT <- get_FF_data(str_file, skip = 9, header=TRUE)\r\n\r\n### Convert columns to numeric\r\ncols <- names(DT)[2:ncol(DT)]\r\nDT[ , (cols) := lapply(.SD, as.numeric), .SDcols = cols]\r\n# # Calculate the mean daily return (in bps) over the last years\r\n# DT[year(Date)>2010, lapply(.SD, mean), by = year(Date),.SDcols = cols]\r\n# \r\n\r\n\r\n\r\nIn this post, I use a classification of 12 sector or industry portfolios. French also has data with lower (only 5 sectors) or higher (12, 17, 30, 38, 48, 49 sectors) granularity on his website. The 12 portfolios are the following:\r\nNoDur: Consumer Nondurables – Food, Tobacco, Textiles, Apparel, Leather, Toys\r\nDurbl: Consumer Durables – Cars, TVs, Furniture, Household Appliances\r\nManuf: Manufacturing – Machinery, Trucks, Planes, Off Furn, Paper, Com Printing\r\nEnrgy: Oil, Gas, and Coal Extraction and Products\r\nChems: Chemicals and Allied Products\r\nBusEq: Business Equipment – Computers, Software, and Electronic Equipment\r\nTelcm: Telephone and Television Transmission\r\nUtils: Utilities\r\nShops: Wholesale, Retail, and Some Services (Laundries, Repair Shops)\r\nHlth: Healthcare, Medical Equipment, and Drugs\r\nMoney: Finance\r\nOther: Other – Mines, Constr, BldMt, Trans, Hotels, Bus Serv, Entertainment\r\nLet’s first look at the average daily returns of those sectors over the last decade in the table below.\r\n\r\n\r\nShow code\r\n\r\nstartYear <- 2011\r\nintDT <- DT[year(Date)>=startYear, lapply(.SD, mean), .SDcols = cols]\r\nkbl(intDT,\r\n    digits=3,\r\n    caption=paste0(\"Average daily returns since \",\r\n                   startYear, \r\n                   \" for 12 sectors. Data from Kenneth French's website, obtained on \",\r\n                   today(), \". Numbers in percent.\")) %>%\r\n  kable_classic(full_width = FALSE)  \r\n\r\n\r\n\r\nTable 1: Average daily returns since 2011 for 12 sectors. Data from Kenneth French’s website, obtained on 2022-04-03. Numbers in percent.\r\n\r\n\r\nNoDur\r\n\r\n\r\nDurbl\r\n\r\n\r\nManuf\r\n\r\n\r\nEnrgy\r\n\r\n\r\nChems\r\n\r\n\r\nBusEq\r\n\r\n\r\nTelcm\r\n\r\n\r\nUtils\r\n\r\n\r\nShops\r\n\r\n\r\nHlth\r\n\r\n\r\nMoney\r\n\r\n\r\nOther\r\n\r\n\r\n0.046\r\n\r\n\r\n0.079\r\n\r\n\r\n0.055\r\n\r\n\r\n0.031\r\n\r\n\r\n0.046\r\n\r\n\r\n0.076\r\n\r\n\r\n0.047\r\n\r\n\r\n0.044\r\n\r\n\r\n0.065\r\n\r\n\r\n0.06\r\n\r\n\r\n0.061\r\n\r\n\r\n0.05\r\n\r\n\r\nThese numbers are already shown as percentage. As an example, the BusEq sector returned 0.076% on average per day. Doesn’t sound like a lot, but compounded over 252 days this results in an annualized return of 21.2%.\r\nLet’s now look how the COVID pandemic impacted these sectors.\r\nInitial COVID impact\r\nThe S&P 500 declined from close to 3,400 on 21 February 2020 to just over 2,300 on 20 March 2020. How did the different sectors contribute to those massive losses?\r\n\r\n\r\nShow code\r\n\r\nintDT <- melt(DT[Date>=dmy(\"21022020\") & Date<=dmy(\"20032020\")], id.var=\"Date\")[,list(Ret=prod(1+value/100)-1),by=variable]\r\nplotBar(.intDT = intDT,\r\n        .x_name = \"variable\",\r\n        .y_name = \"Ret\",\r\n        .x_lab = \"Sector\",\r\n        .y_lab = \"Return\",\r\n        .fillColor = \"#581845\",\r\n        .bolSort = TRUE) + \r\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\r\n  theme(axis.text.x = element_text(angle = 45, hjust=1))\r\n\r\n\r\n\r\n\r\nFigure 1: Sector returns from 21 February 2020 to 20 March 2020. Data from Kenneth French’s website.\r\n\r\n\r\n\r\nHealthcare was the most resilient sector in the first phase of the pandemic and only lost slightly over 20%. Shops is an interesting one as many retailers had to close.1 Later on, I show with different data that this sector most likely also includes retailers for daily goods that profited from the panic buying, which explains the overall low impact of the pandemic on this sector. This is a good example of how classifications can lead to ambiguous interpretations.\r\nThe most impacted sectors were Durables and Energy. Both these sectors are cyclical sectors and initially there was fear among investors that the pandemic would lead to a recession, so it is not surprising that they were impacted heavily. In case of Energy, it would be interesting if part of the underperformance was also attributed to the expectation of substantially lower travel activities due to the lock down and, as a result, lower fuel consumption.\r\nCOVID recovery\r\nThe S&P 500 then rose close to 3,800 until the end of the year. Which sectors drove this exceptional recovery? The chart below gives the answer.\r\n\r\n\r\nShow code\r\n\r\nintDT <- melt(DT[Date>dmy(\"20032020\") & Date<=dmy(\"31122020\")], id.var=\"Date\")[,list(Ret=prod(1+value/100)-1),by=variable]\r\nplotBar(.intDT = intDT,\r\n        .x_name = \"variable\",\r\n        .y_name = \"Ret\",\r\n        .x_lab = \"Sector\",\r\n        .y_lab = \"Return\",\r\n        .fillColor = \"#581845\",\r\n        .bolSort = TRUE) + \r\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\r\n  theme(axis.text.x = element_text(angle = 45, hjust=1))\r\n\r\n\r\n\r\n\r\nFigure 2: Sector returns from 21 March 2020 to 31 December 2020. Data from Kenneth French’s website.\r\n\r\n\r\n\r\nDurables was by far the best performing sector. It is such an outlier that it would indeed by intersting to know the constituents as my first guess would not have been that companies dealing with “Cars, TVs, Furniture, Household Appliances” were the clear COVID winners. Obviously, Cars include Tesla and Tesla rose over 700% in that period, being one very important driver of this sector, given how large this company’s market cap was at the end of 2020.\r\n\r\nRe-run the code with a more granular classification, for example the 38 industry portfolios, and cars shows up as a strongly performing sector.\r\nUnfortunately, the classifications by French do not really put tech / software companies in one bucket. Obviously, these firms performed exceptionally throughout the pandemic, but I don’t quite know where they ended up. Could be Durables, could be Business Equipment, or something else. Fortunately, the data set I use next shows the sector performance in more granularity.\r\nData from Aswath Damodaran’s website\r\nIn my very first post, I already introduced a function that downloads the data from Damodaran’s website. I use this function here to download his Excel file that documents the COVID impact on U.S. firms.\r\n\r\n\r\nShow code\r\n\r\nlibrary(readxl)\r\nlibrary(httr)\r\nintDT <- import_from_excel(.str_file =\"https://pages.stern.nyu.edu/~adamodar/pc/datasets/COVIDeffects.xls\",\r\n                           na = c(\"NA\", \"N/A\"))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nsetnames(intDT,\r\n         old = c(\"Industry Name\", \"Number of firms\", \"12/31/19\", \"2/14/20\", \"3/20/20\", \"9/1/20\", \"12/31/21\", \r\n                 \"1/1/20 -  2/14\", \"2/14/20 - 3/20/20\", \"3/20/20 - 9/1/20\", \"9/1/20 - 12/31/21\", \"1/1/20 - 12/31/21\", \r\n                 \"LTM 2020\", \"LTM 2021\", \"% Change\", \"LTM 20202\", \"LTM 20213\", \"% Change4\"),\r\n         new = c(\"Sector\", \"N\", \"MktCap_31Dec19\", \"MktCap_14Feb20\", \"MktCap_20Mar20\", \"MktCap_1Sep20\", \"MktCap_31Dec21\", \r\n                 \"Chg_preCOVID\", \"Chg_initial\", \"Chg_Recov\", \"Chg_end20\", \"Chg_21\", \r\n                 \"Rev_LTM_2020\", \"Rev_LTM_2021\", \"Chg_Rev\", \"OpInc_LTM_2020\", \"OpInc_LTM_2021\", \"Chg_OpInc\"))\r\n## Report\r\nintDT <- intDT[grep(\"Total Market\", Sector,invert =TRUE)]\r\nintDT[, Comb:=(1+Chg_initial)*(1+Chg_Recov)*(1+Chg_end20)-1]\r\nintDT2 <- intDT[,list(Sector, N, Chg_initial, Chg_Recov, Chg_end20, Comb)][order(Chg_initial)]\r\nintDT2[, Rank:=round(rank(-Comb),digits=0)]\r\ncols <- c(\"Chg_initial\", \"Chg_Recov\", \"Chg_end20\", \"Comb\")\r\nintDT2[, (cols) := lapply(.SD, function(x) {paste0(format(round(x*100,digits=1),nsmall=1), \"%\")}), .SDcols = cols]  \r\nkbl(intDT2,\r\n    align = \"c\",\r\n    col.names = c(\"Sector\", \"Nr. of firms\", \"2/14/20 - 3/20/20\", \"3/20/20 - 9/1/20\", \"9/1/20 - 12/31/21\", \"Combined\", \"Rank\"),\r\n    caption=\"Performance of different sectors during different phases of the COVID pandemic, sorted by performance in the initial phase. Rank is based on the combined performance. Data from Damodaran's website.\") %>%\r\n  kable_classic(full_width = FALSE)\r\n\r\n\r\n\r\nTable 2: Performance of different sectors during different phases of the COVID pandemic, sorted by performance in the initial phase. Rank is based on the combined performance. Data from Damodaran’s website.\r\n\r\n\r\nSector\r\n\r\n\r\nNr. of firms\r\n\r\n\r\n2/14/20 - 3/20/20\r\n\r\n\r\n3/20/20 - 9/1/20\r\n\r\n\r\n9/1/20 - 12/31/21\r\n\r\n\r\nCombined\r\n\r\n\r\nRank\r\n\r\n\r\nAir Transport\r\n\r\n\r\n21\r\n\r\n\r\n-60.9%\r\n\r\n\r\n49.8%\r\n\r\n\r\n46.3%\r\n\r\n\r\n-14.4%\r\n\r\n\r\n92\r\n\r\n\r\nReal Estate (Development)\r\n\r\n\r\n19\r\n\r\n\r\n-60.5%\r\n\r\n\r\n72.3%\r\n\r\n\r\n59.5%\r\n\r\n\r\n8.5%\r\n\r\n\r\n78\r\n\r\n\r\nOilfield Svcs/Equip.\r\n\r\n\r\n100\r\n\r\n\r\n-58.0%\r\n\r\n\r\n50.6%\r\n\r\n\r\n58.4%\r\n\r\n\r\n0.4%\r\n\r\n\r\n88\r\n\r\n\r\nOil/Gas (Production and Exploration)\r\n\r\n\r\n183\r\n\r\n\r\n-57.0%\r\n\r\n\r\n56.0%\r\n\r\n\r\n161.2%\r\n\r\n\r\n75.4%\r\n\r\n\r\n27\r\n\r\n\r\nFood Wholesalers\r\n\r\n\r\n15\r\n\r\n\r\n-56.9%\r\n\r\n\r\n74.3%\r\n\r\n\r\n40.5%\r\n\r\n\r\n5.6%\r\n\r\n\r\n83\r\n\r\n\r\nInsurance (Life)\r\n\r\n\r\n24\r\n\r\n\r\n-53.8%\r\n\r\n\r\n56.3%\r\n\r\n\r\n50.6%\r\n\r\n\r\n8.7%\r\n\r\n\r\n76\r\n\r\n\r\nHomebuilding\r\n\r\n\r\n29\r\n\r\n\r\n-53.8%\r\n\r\n\r\n132.5%\r\n\r\n\r\n45.8%\r\n\r\n\r\n56.6%\r\n\r\n\r\n36\r\n\r\n\r\nHotel/Gaming\r\n\r\n\r\n66\r\n\r\n\r\n-53.8%\r\n\r\n\r\n72.4%\r\n\r\n\r\n79.3%\r\n\r\n\r\n42.9%\r\n\r\n\r\n51\r\n\r\n\r\nOil/Gas Distribution\r\n\r\n\r\n21\r\n\r\n\r\n-52.9%\r\n\r\n\r\n47.2%\r\n\r\n\r\n56.3%\r\n\r\n\r\n8.4%\r\n\r\n\r\n79\r\n\r\n\r\nFurn/Home Furnishings\r\n\r\n\r\n32\r\n\r\n\r\n-51.4%\r\n\r\n\r\n102.6%\r\n\r\n\r\n54.0%\r\n\r\n\r\n51.5%\r\n\r\n\r\n39\r\n\r\n\r\nReinsurance\r\n\r\n\r\n2\r\n\r\n\r\n-51.2%\r\n\r\n\r\n35.4%\r\n\r\n\r\n16.3%\r\n\r\n\r\n-23.2%\r\n\r\n\r\n94\r\n\r\n\r\nBroadcasting\r\n\r\n\r\n28\r\n\r\n\r\n-49.6%\r\n\r\n\r\n40.9%\r\n\r\n\r\n25.7%\r\n\r\n\r\n-10.7%\r\n\r\n\r\n90\r\n\r\n\r\nRubber& Tires\r\n\r\n\r\n2\r\n\r\n\r\n-48.7%\r\n\r\n\r\n70.3%\r\n\r\n\r\n166.1%\r\n\r\n\r\n132.3%\r\n\r\n\r\n7\r\n\r\n\r\nOil/Gas (Integrated)\r\n\r\n\r\n4\r\n\r\n\r\n-48.4%\r\n\r\n\r\n28.6%\r\n\r\n\r\n53.7%\r\n\r\n\r\n2.1%\r\n\r\n\r\n87\r\n\r\n\r\nReal Estate (Operations & Services)\r\n\r\n\r\n51\r\n\r\n\r\n-48.1%\r\n\r\n\r\n97.8%\r\n\r\n\r\n95.6%\r\n\r\n\r\n100.7%\r\n\r\n\r\n13\r\n\r\n\r\nAerospace/Defense\r\n\r\n\r\n73\r\n\r\n\r\n-48.1%\r\n\r\n\r\n37.6%\r\n\r\n\r\n22.9%\r\n\r\n\r\n-12.2%\r\n\r\n\r\n91\r\n\r\n\r\nHospitals/Healthcare Facilities\r\n\r\n\r\n31\r\n\r\n\r\n-48.0%\r\n\r\n\r\n66.2%\r\n\r\n\r\n69.2%\r\n\r\n\r\n46.3%\r\n\r\n\r\n45\r\n\r\n\r\nPaper/Forest Products\r\n\r\n\r\n11\r\n\r\n\r\n-48.0%\r\n\r\n\r\n73.8%\r\n\r\n\r\n67.3%\r\n\r\n\r\n51.3%\r\n\r\n\r\n40\r\n\r\n\r\nRecreation\r\n\r\n\r\n60\r\n\r\n\r\n-47.6%\r\n\r\n\r\n122.6%\r\n\r\n\r\n43.3%\r\n\r\n\r\n67.0%\r\n\r\n\r\n33\r\n\r\n\r\nAuto & Truck\r\n\r\n\r\n26\r\n\r\n\r\n-47.3%\r\n\r\n\r\n314.3%\r\n\r\n\r\n168.4%\r\n\r\n\r\n485.6%\r\n\r\n\r\n1\r\n\r\n\r\nEngineering/Construction\r\n\r\n\r\n48\r\n\r\n\r\n-45.9%\r\n\r\n\r\n82.6%\r\n\r\n\r\n92.6%\r\n\r\n\r\n90.1%\r\n\r\n\r\n18\r\n\r\n\r\nChemical (Basic)\r\n\r\n\r\n35\r\n\r\n\r\n-45.7%\r\n\r\n\r\n66.8%\r\n\r\n\r\n48.3%\r\n\r\n\r\n34.4%\r\n\r\n\r\n57\r\n\r\n\r\nAdvertising\r\n\r\n\r\n49\r\n\r\n\r\n-45.1%\r\n\r\n\r\n35.1%\r\n\r\n\r\n109.4%\r\n\r\n\r\n55.2%\r\n\r\n\r\n38\r\n\r\n\r\nChemical (Diversified)\r\n\r\n\r\n4\r\n\r\n\r\n-44.4%\r\n\r\n\r\n92.7%\r\n\r\n\r\n63.9%\r\n\r\n\r\n75.6%\r\n\r\n\r\n26\r\n\r\n\r\nApparel\r\n\r\n\r\n39\r\n\r\n\r\n-44.1%\r\n\r\n\r\n28.0%\r\n\r\n\r\n52.0%\r\n\r\n\r\n8.6%\r\n\r\n\r\n77\r\n\r\n\r\nBank (Money Center)\r\n\r\n\r\n7\r\n\r\n\r\n-44.0%\r\n\r\n\r\n18.4%\r\n\r\n\r\n55.5%\r\n\r\n\r\n3.1%\r\n\r\n\r\n85\r\n\r\n\r\nMetals & Mining\r\n\r\n\r\n74\r\n\r\n\r\n-43.4%\r\n\r\n\r\n122.6%\r\n\r\n\r\n94.6%\r\n\r\n\r\n145.3%\r\n\r\n\r\n6\r\n\r\n\r\nOffice Equipment & Services\r\n\r\n\r\n18\r\n\r\n\r\n-43.2%\r\n\r\n\r\n35.3%\r\n\r\n\r\n38.3%\r\n\r\n\r\n6.4%\r\n\r\n\r\n81\r\n\r\n\r\nAuto Parts\r\n\r\n\r\n38\r\n\r\n\r\n-42.2%\r\n\r\n\r\n73.7%\r\n\r\n\r\n81.4%\r\n\r\n\r\n82.2%\r\n\r\n\r\n22\r\n\r\n\r\nBrokerage & Investment Banking\r\n\r\n\r\n31\r\n\r\n\r\n-42.1%\r\n\r\n\r\n49.0%\r\n\r\n\r\n127.2%\r\n\r\n\r\n96.0%\r\n\r\n\r\n16\r\n\r\n\r\nBanks (Regional)\r\n\r\n\r\n563\r\n\r\n\r\n-42.1%\r\n\r\n\r\n23.6%\r\n\r\n\r\n84.8%\r\n\r\n\r\n32.2%\r\n\r\n\r\n61\r\n\r\n\r\nSemiconductor Equip\r\n\r\n\r\n34\r\n\r\n\r\n-41.5%\r\n\r\n\r\n79.4%\r\n\r\n\r\n113.5%\r\n\r\n\r\n123.9%\r\n\r\n\r\n8\r\n\r\n\r\nInsurance (General)\r\n\r\n\r\n23\r\n\r\n\r\n-41.5%\r\n\r\n\r\n45.5%\r\n\r\n\r\n62.0%\r\n\r\n\r\n37.8%\r\n\r\n\r\n56\r\n\r\n\r\nShipbuilding & Marine\r\n\r\n\r\n8\r\n\r\n\r\n-41.5%\r\n\r\n\r\n22.8%\r\n\r\n\r\n82.9%\r\n\r\n\r\n31.5%\r\n\r\n\r\n62\r\n\r\n\r\nRetail (Building Supply)\r\n\r\n\r\n16\r\n\r\n\r\n-41.1%\r\n\r\n\r\n104.2%\r\n\r\n\r\n41.7%\r\n\r\n\r\n70.6%\r\n\r\n\r\n30\r\n\r\n\r\nEducation\r\n\r\n\r\n35\r\n\r\n\r\n-41.1%\r\n\r\n\r\n77.1%\r\n\r\n\r\n12.7%\r\n\r\n\r\n17.6%\r\n\r\n\r\n70\r\n\r\n\r\nCoal & Related Energy\r\n\r\n\r\n18\r\n\r\n\r\n-40.8%\r\n\r\n\r\n23.7%\r\n\r\n\r\n266.6%\r\n\r\n\r\n168.6%\r\n\r\n\r\n5\r\n\r\n\r\nSteel\r\n\r\n\r\n28\r\n\r\n\r\n-40.4%\r\n\r\n\r\n56.2%\r\n\r\n\r\n116.2%\r\n\r\n\r\n101.3%\r\n\r\n\r\n12\r\n\r\n\r\nRetail (Distributors)\r\n\r\n\r\n68\r\n\r\n\r\n-40.2%\r\n\r\n\r\n84.9%\r\n\r\n\r\n62.8%\r\n\r\n\r\n80.1%\r\n\r\n\r\n23\r\n\r\n\r\nFinancial Svcs. (Non-bank & Insurance)\r\n\r\n\r\n223\r\n\r\n\r\n-40.0%\r\n\r\n\r\n49.2%\r\n\r\n\r\n57.5%\r\n\r\n\r\n41.1%\r\n\r\n\r\n53\r\n\r\n\r\nRetail (Special Lines)\r\n\r\n\r\n76\r\n\r\n\r\n-39.7%\r\n\r\n\r\n47.2%\r\n\r\n\r\n55.9%\r\n\r\n\r\n38.3%\r\n\r\n\r\n55\r\n\r\n\r\nInvestments & Asset Management\r\n\r\n\r\n687\r\n\r\n\r\n-39.7%\r\n\r\n\r\n52.4%\r\n\r\n\r\n126.0%\r\n\r\n\r\n107.6%\r\n\r\n\r\n10\r\n\r\n\r\nR.E.I.T.\r\n\r\n\r\n238\r\n\r\n\r\n-39.5%\r\n\r\n\r\n36.8%\r\n\r\n\r\n53.0%\r\n\r\n\r\n26.6%\r\n\r\n\r\n69\r\n\r\n\r\nRetail (Automotive)\r\n\r\n\r\n32\r\n\r\n\r\n-38.9%\r\n\r\n\r\n118.7%\r\n\r\n\r\n28.1%\r\n\r\n\r\n71.2%\r\n\r\n\r\n29\r\n\r\n\r\nTrucking\r\n\r\n\r\n34\r\n\r\n\r\n-38.9%\r\n\r\n\r\n66.1%\r\n\r\n\r\n64.4%\r\n\r\n\r\n66.9%\r\n\r\n\r\n34\r\n\r\n\r\nElectrical Equipment\r\n\r\n\r\n104\r\n\r\n\r\n-38.7%\r\n\r\n\r\n79.9%\r\n\r\n\r\n71.9%\r\n\r\n\r\n89.5%\r\n\r\n\r\n19\r\n\r\n\r\nElectronics (Consumer & Office)\r\n\r\n\r\n16\r\n\r\n\r\n-38.4%\r\n\r\n\r\n76.0%\r\n\r\n\r\n286.0%\r\n\r\n\r\n318.8%\r\n\r\n\r\n3\r\n\r\n\r\nRestaurant/Dining\r\n\r\n\r\n70\r\n\r\n\r\n-38.0%\r\n\r\n\r\n60.4%\r\n\r\n\r\n34.0%\r\n\r\n\r\n33.2%\r\n\r\n\r\n58\r\n\r\n\r\nComputer Services\r\n\r\n\r\n83\r\n\r\n\r\n-38.0%\r\n\r\n\r\n42.6%\r\n\r\n\r\n44.9%\r\n\r\n\r\n28.2%\r\n\r\n\r\n68\r\n\r\n\r\nReal Estate (General/Diversified)\r\n\r\n\r\n10\r\n\r\n\r\n-37.5%\r\n\r\n\r\n30.6%\r\n\r\n\r\n90.6%\r\n\r\n\r\n55.6%\r\n\r\n\r\n37\r\n\r\n\r\nTransportation (Railroads)\r\n\r\n\r\n4\r\n\r\n\r\n-37.4%\r\n\r\n\r\n63.1%\r\n\r\n\r\n28.4%\r\n\r\n\r\n31.0%\r\n\r\n\r\n64\r\n\r\n\r\nShoe\r\n\r\n\r\n12\r\n\r\n\r\n-36.6%\r\n\r\n\r\n70.9%\r\n\r\n\r\n51.0%\r\n\r\n\r\n63.8%\r\n\r\n\r\n35\r\n\r\n\r\nMachinery\r\n\r\n\r\n111\r\n\r\n\r\n-36.5%\r\n\r\n\r\n68.7%\r\n\r\n\r\n40.6%\r\n\r\n\r\n50.4%\r\n\r\n\r\n41\r\n\r\n\r\nBusiness & Consumer Services\r\n\r\n\r\n160\r\n\r\n\r\n-36.0%\r\n\r\n\r\n59.0%\r\n\r\n\r\n41.2%\r\n\r\n\r\n43.8%\r\n\r\n\r\n48\r\n\r\n\r\nBeverage (Alcoholic)\r\n\r\n\r\n21\r\n\r\n\r\n-35.3%\r\n\r\n\r\n52.2%\r\n\r\n\r\n16.3%\r\n\r\n\r\n14.6%\r\n\r\n\r\n72\r\n\r\n\r\nConstruction Supplies\r\n\r\n\r\n48\r\n\r\n\r\n-35.2%\r\n\r\n\r\n67.0%\r\n\r\n\r\n32.1%\r\n\r\n\r\n42.9%\r\n\r\n\r\n50\r\n\r\n\r\nInformation Services\r\n\r\n\r\n79\r\n\r\n\r\n-35.1%\r\n\r\n\r\n63.8%\r\n\r\n\r\n6.4%\r\n\r\n\r\n13.1%\r\n\r\n\r\n74\r\n\r\n\r\nChemical (Specialty)\r\n\r\n\r\n81\r\n\r\n\r\n-34.2%\r\n\r\n\r\n56.3%\r\n\r\n\r\n41.4%\r\n\r\n\r\n45.4%\r\n\r\n\r\n46\r\n\r\n\r\nPublishing & Newspapers\r\n\r\n\r\n21\r\n\r\n\r\n-34.1%\r\n\r\n\r\n41.2%\r\n\r\n\r\n37.8%\r\n\r\n\r\n28.2%\r\n\r\n\r\n67\r\n\r\n\r\nFarming/Agriculture\r\n\r\n\r\n36\r\n\r\n\r\n-34.1%\r\n\r\n\r\n69.5%\r\n\r\n\r\n58.8%\r\n\r\n\r\n77.3%\r\n\r\n\r\n25\r\n\r\n\r\nInsurance (Prop/Cas.)\r\n\r\n\r\n52\r\n\r\n\r\n-33.8%\r\n\r\n\r\n30.8%\r\n\r\n\r\n21.8%\r\n\r\n\r\n5.4%\r\n\r\n\r\n84\r\n\r\n\r\nPower\r\n\r\n\r\n50\r\n\r\n\r\n-33.5%\r\n\r\n\r\n25.6%\r\n\r\n\r\n28.9%\r\n\r\n\r\n7.6%\r\n\r\n\r\n80\r\n\r\n\r\nElectronics (General)\r\n\r\n\r\n137\r\n\r\n\r\n-33.1%\r\n\r\n\r\n55.2%\r\n\r\n\r\n68.2%\r\n\r\n\r\n74.8%\r\n\r\n\r\n28\r\n\r\n\r\nSemiconductor\r\n\r\n\r\n67\r\n\r\n\r\n-32.0%\r\n\r\n\r\n77.2%\r\n\r\n\r\n71.7%\r\n\r\n\r\n106.9%\r\n\r\n\r\n11\r\n\r\n\r\nBeverage (Soft)\r\n\r\n\r\n32\r\n\r\n\r\n-31.4%\r\n\r\n\r\n33.6%\r\n\r\n\r\n23.9%\r\n\r\n\r\n13.6%\r\n\r\n\r\n73\r\n\r\n\r\nUtility (General)\r\n\r\n\r\n16\r\n\r\n\r\n-31.2%\r\n\r\n\r\n22.4%\r\n\r\n\r\n16.5%\r\n\r\n\r\n-1.9%\r\n\r\n\r\n89\r\n\r\n\r\nHealthcare Support Services\r\n\r\n\r\n131\r\n\r\n\r\n-30.8%\r\n\r\n\r\n43.1%\r\n\r\n\r\n50.4%\r\n\r\n\r\n48.9%\r\n\r\n\r\n44\r\n\r\n\r\nHealthcare Products\r\n\r\n\r\n244\r\n\r\n\r\n-30.7%\r\n\r\n\r\n58.0%\r\n\r\n\r\n31.0%\r\n\r\n\r\n43.5%\r\n\r\n\r\n49\r\n\r\n\r\nComputers/Peripherals\r\n\r\n\r\n46\r\n\r\n\r\n-30.6%\r\n\r\n\r\n122.9%\r\n\r\n\r\n28.2%\r\n\r\n\r\n98.3%\r\n\r\n\r\n15\r\n\r\n\r\nSoftware (Entertainment)\r\n\r\n\r\n88\r\n\r\n\r\n-30.4%\r\n\r\n\r\n75.0%\r\n\r\n\r\n46.3%\r\n\r\n\r\n78.3%\r\n\r\n\r\n24\r\n\r\n\r\nPackaging & Container\r\n\r\n\r\n26\r\n\r\n\r\n-30.3%\r\n\r\n\r\n40.3%\r\n\r\n\r\n35.3%\r\n\r\n\r\n32.4%\r\n\r\n\r\n59\r\n\r\n\r\nDiversified\r\n\r\n\r\n22\r\n\r\n\r\n-29.8%\r\n\r\n\r\n27.7%\r\n\r\n\r\n28.2%\r\n\r\n\r\n15.0%\r\n\r\n\r\n71\r\n\r\n\r\nCable TV\r\n\r\n\r\n11\r\n\r\n\r\n-29.5%\r\n\r\n\r\n40.9%\r\n\r\n\r\n3.0%\r\n\r\n\r\n2.3%\r\n\r\n\r\n86\r\n\r\n\r\nBuilding Materials\r\n\r\n\r\n44\r\n\r\n\r\n-29.5%\r\n\r\n\r\n97.5%\r\n\r\n\r\n56.4%\r\n\r\n\r\n117.9%\r\n\r\n\r\n9\r\n\r\n\r\nEntertainment\r\n\r\n\r\n108\r\n\r\n\r\n-28.2%\r\n\r\n\r\n67.0%\r\n\r\n\r\n24.7%\r\n\r\n\r\n49.4%\r\n\r\n\r\n43\r\n\r\n\r\nTobacco\r\n\r\n\r\n16\r\n\r\n\r\n-28.2%\r\n\r\n\r\n27.6%\r\n\r\n\r\n16.0%\r\n\r\n\r\n6.2%\r\n\r\n\r\n82\r\n\r\n\r\nSoftware (System & Application)\r\n\r\n\r\n375\r\n\r\n\r\n-26.7%\r\n\r\n\r\n76.1%\r\n\r\n\r\n43.3%\r\n\r\n\r\n84.9%\r\n\r\n\r\n20\r\n\r\n\r\nEnvironmental & Waste Services\r\n\r\n\r\n58\r\n\r\n\r\n-26.5%\r\n\r\n\r\n30.6%\r\n\r\n\r\n37.7%\r\n\r\n\r\n32.2%\r\n\r\n\r\n60\r\n\r\n\r\nGreen & Renewable Energy\r\n\r\n\r\n20\r\n\r\n\r\n-26.2%\r\n\r\n\r\n302.3%\r\n\r\n\r\n61.2%\r\n\r\n\r\n378.7%\r\n\r\n\r\n2\r\n\r\n\r\nTelecom. Equipment\r\n\r\n\r\n82\r\n\r\n\r\n-25.3%\r\n\r\n\r\n24.0%\r\n\r\n\r\n61.9%\r\n\r\n\r\n49.9%\r\n\r\n\r\n42\r\n\r\n\r\nUtility (Water)\r\n\r\n\r\n14\r\n\r\n\r\n-23.9%\r\n\r\n\r\n24.5%\r\n\r\n\r\n35.6%\r\n\r\n\r\n28.4%\r\n\r\n\r\n66\r\n\r\n\r\nHeathcare Information and Technology\r\n\r\n\r\n142\r\n\r\n\r\n-23.5%\r\n\r\n\r\n65.3%\r\n\r\n\r\n58.2%\r\n\r\n\r\n100.1%\r\n\r\n\r\n14\r\n\r\n\r\nSoftware (Internet)\r\n\r\n\r\n36\r\n\r\n\r\n-23.5%\r\n\r\n\r\n112.1%\r\n\r\n\r\n92.1%\r\n\r\n\r\n211.9%\r\n\r\n\r\n4\r\n\r\n\r\nTelecom (Wireless)\r\n\r\n\r\n17\r\n\r\n\r\n-22.7%\r\n\r\n\r\n117.5%\r\n\r\n\r\n-0.1%\r\n\r\n\r\n68.0%\r\n\r\n\r\n31\r\n\r\n\r\nDrugs (Pharmaceutical)\r\n\r\n\r\n298\r\n\r\n\r\n-21.4%\r\n\r\n\r\n29.1%\r\n\r\n\r\n29.5%\r\n\r\n\r\n31.4%\r\n\r\n\r\n63\r\n\r\n\r\nFood Processing\r\n\r\n\r\n92\r\n\r\n\r\n-21.0%\r\n\r\n\r\n30.5%\r\n\r\n\r\n8.5%\r\n\r\n\r\n11.9%\r\n\r\n\r\n75\r\n\r\n\r\nHousehold Products\r\n\r\n\r\n118\r\n\r\n\r\n-20.8%\r\n\r\n\r\n35.9%\r\n\r\n\r\n21.7%\r\n\r\n\r\n31.0%\r\n\r\n\r\n65\r\n\r\n\r\nTelecom. Services\r\n\r\n\r\n42\r\n\r\n\r\n-20.1%\r\n\r\n\r\n9.6%\r\n\r\n\r\n-9.5%\r\n\r\n\r\n-20.8%\r\n\r\n\r\n93\r\n\r\n\r\nTransportation\r\n\r\n\r\n17\r\n\r\n\r\n-17.9%\r\n\r\n\r\n78.3%\r\n\r\n\r\n33.5%\r\n\r\n\r\n95.5%\r\n\r\n\r\n17\r\n\r\n\r\nDrugs (Biotechnology)\r\n\r\n\r\n581\r\n\r\n\r\n-17.7%\r\n\r\n\r\n42.0%\r\n\r\n\r\n24.1%\r\n\r\n\r\n45.0%\r\n\r\n\r\n47\r\n\r\n\r\nPrecious Metals\r\n\r\n\r\n76\r\n\r\n\r\n-14.7%\r\n\r\n\r\n77.6%\r\n\r\n\r\n-7.9%\r\n\r\n\r\n39.5%\r\n\r\n\r\n54\r\n\r\n\r\nRetail (Online)\r\n\r\n\r\n60\r\n\r\n\r\n-14.6%\r\n\r\n\r\n94.6%\r\n\r\n\r\n0.6%\r\n\r\n\r\n67.1%\r\n\r\n\r\n32\r\n\r\n\r\nRetail (General)\r\n\r\n\r\n16\r\n\r\n\r\n-9.0%\r\n\r\n\r\n31.1%\r\n\r\n\r\n19.5%\r\n\r\n\r\n42.6%\r\n\r\n\r\n52\r\n\r\n\r\nRetail (Grocery and Food)\r\n\r\n\r\n15\r\n\r\n\r\n4.3%\r\n\r\n\r\n34.8%\r\n\r\n\r\n31.2%\r\n\r\n\r\n84.4%\r\n\r\n\r\n21\r\n\r\n\r\nDamodaran’s sector classification is much more granular, which confirms some of the hypotheses I made above when looking at French’s data. For example, Shops did reasonably well as some of the clear initial COVID winners were grocery and food retailers. Shoes and apparel shops, on the other hand, suffered substantially.\r\nThe table also shows nicely for which sectors the sentiment stayed relatively constant and for which it changed. Air Transport is a sector that was initially hit the worst and has not recovered grounds since then in comparison to other sectors. Other sectors, such as Oil/Gas (Production and Exploration), did terrible at the start, but ended 2020 as one of the stronger sectors.\r\nAgain, Tesla shows up as the clear anomaly in this data set, pushing Auto & Truck to the best performing sector. For me, this is difficult to explain: in an unexpected pandemic, one would have expected certain sector rotations. Air Transport becomes less relevant as people don’t travel anymore; Software and Entertainment profits as people stay at home. Why do cars though become out of a sudden so much more relevant during a pandemic? People always had them, it’s hard for me to come up with a story as to why they became so much more relevant during COVID. Of course, one could argue that the rise of Tesla co-incided with the pandemic, but in this case one would expect the other car manufacturers to lose just as much. Essentially an intra-sector rotation away from the old manufacturers towards Tesla. This, however, didn’t seem to happen either as Tesla gained so much to move the whole sector towards new heights. One explanation could be that Tesla took those gains from non-U.S. competitors such as Daimler, BMW, or Toyota. However, Toyota’s share price increased and most of the others are flat compared to the pandemic.\r\nDamodaran also shows the change in revenues and operating income from 2020 to 2021 and the scatterplots below compare them to the stock market performance from mid-February 2020 to the end of 2020. A strong correlation would indicate that the stock price movements were anticipating subsequent moves in cash flows. However, there is no such correlation visible in the data.\r\n\r\n\r\nShow code\r\n\r\nggplot(intDT[!is.na(Chg_Rev)], \r\n       aes(x=Comb, y=Chg_Rev)) + #, color=sector_group)) +\r\n  scale_x_continuous(labels = scales::percent)  + \r\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))  +\r\n  geom_point() + xlab(\"Stock return: 2/14/20-12/31/20\") + ylab(\"Change Rev 20-21\") +\r\n  geom_smooth(method=lm, se=FALSE) +\r\n  theme(legend.title = element_blank())\r\nggplot(intDT[!is.na(Chg_OpInc)], \r\n       aes(x=Comb, y=Chg_OpInc)) + #, color=sector_group)) +\r\n  scale_x_continuous(labels = scales::percent)  + \r\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))  +\r\n  geom_point() + xlab(\"Stock return: 2/14/20-12/31/20\") + ylab(\"Change OpInc 20-21\") +\r\n  geom_smooth(method=lm, se=FALSE) +\r\n  theme(legend.title = element_blank())\r\n\r\n\r\n\r\n\r\nFigure 3: Sector performance from 14 Feb 2020 to 31 Dec 2020 vs. change in revenues and operating income from 2020 to 2021. Data obtained from Damodaran’s website.\r\n\r\n\r\n\r\nSummary\r\nThis post has shown how to leverage publicly available data to understand the impact of a macroeconomic shock on different sectors. It used the COVID pandemic as an example, but the code is easily adjustable to look at other events as well.\r\n\r\nThis Wikipedia article mentions that Apple and many other players closed all their shops on March 14. Hence, investors were already well aware of the dramatic impact of the pandemic on retail.↩︎\r\n",
    "preview": "posts/2022-03-27-sector-performance/sector-performance_files/figure-html5/PlotChartCOVID-1.png",
    "last_modified": "2022-04-03T15:53:24+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-24-the-performance-of-private-equity/",
    "title": "The ultimate guide to private equity performance",
    "description": "A thorough analysis of the performance of the asset class private equity.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2022-02-18",
    "categories": [
      "Private Equity",
      "Data",
      "Research"
    ],
    "contents": "\r\n\r\nContents\r\nLiterature review\r\nKaplan and Schoar (2005)\r\nPhalippou and Gottschalg (2009)\r\nStucke (2011)\r\nR. S. Harris, Jenkinson, and Kaplan (2014)\r\nPhalippou (2014)\r\nHonorable mentions\r\nSummary\r\n\r\nData source and characteristics\r\nNumber of funds over time and by strategy\r\nNumber of funds by region\r\nComparison with other data sets\r\nCoverage in relation to PE universe\r\nSize of funds\r\n\r\nPerformance of PE\r\nTotal-Value to Paid-In (“TVPI”)\r\nBy vintage year\r\nBy region\r\n\r\nInternal Rate of Return (“IRR”)\r\nDistributed to Paid-In (“DPI”)\r\nPublic Market Equivalent (“PME”)\r\n\r\nSummary\r\n\r\n\r\n\r\n\r\nWhat’s the performance of private equity (“PE”)? A seemingly simple question, but believe me that there is no simple answer. There are too many parameters to look at, such as the performance metric, the time period and the source of the data. In this blog post, I provide an overview of research papers that have investigated this topic and compare their results. I then do my own analysis with Preqin data. It’s gonna be a long post, so let’s get right into it.\r\n\r\nI’m focusing on fund level performance. There is a separate literature that looks at the performance on deal level. I discuss two such studies in more detail in this blog post.\r\nLiterature review\r\nIn the following, I briefly introduce several studies about the performance of PE. This is not a comprehensive summary, but rather a selection of studies I consider important or helpful in shedding light on the question of what returns PE funds deliver to investors. For a more detailed literature review with a very helpful overview table, have a look at Korteweg (2019).\r\nKaplan and Schoar (2005)\r\nOne of the earliest and most influential studies of PE performance, it was some sort of a kickstarter for research on PE. By finding persistence in fund returns - GPs that outperform the industry in one fund are more likely to do so with the next one as well - they also justified, or even established, investors key focus on a GP’s track record. PE fund returns are predictable, in contrast to mutual funds, so betting on the same horses again and again pays off for investors. A GP with a top tercile fund has close to a 50% chance of having a top tercile fund with their next one (see Table IX), both when looking at the PME and IRR.\r\nKaplan and Schoar (2005) do not find an outperformance of PE over the S&P 500 overall. For their analysis, they use Thomson Venture Economics (TVE) data, which was the prevalent data source back then and also used by the two studies that I introduce next. I don’t want to spill the beans, but let’s just say that data quality really matters…\r\nPhalippou and Gottschalg (2009)\r\nThe authors summarize their research question and answer concisely:\r\n\r\nThe objective of this study is to estimate the performance of private equity funds both net-of-fees and gross-of-fees. We find that the average private equity fund underperforms the S&P 500 Index net-of-fees by about 3% per year and overperforms that index gross-of-fees by 3% per year.\r\n\r\nWhile the language might be prosaic, the content is everything but. How to justify a 3% p.a. underperformance compared to the overall stock market for an asset class that should be deemed at least as risky (highly levered investments in much smaller companies than the S&P 500 constituents) and much more illiquid? And how to reconcile this underperformance with PE’s unprecedented growth in committed capital from USD 5 to USD 300 billion from 1980 to 2004? Did investors keep pouring money into PE despite the poor performance or were they not able to calculate their returns correctly?\r\nTo derive at their results, Phalippou and Gottschalg (2009) start with the observation that many older funds, 10 years or more, show no sign of activity: no capital calls or distributions and no changes to the NAV or residual value.1 They interpret this lack of activity as confirmation that the funds are liquidated and write-off their final NAV to zero, in contrast to other studies that treat the residual value as a cash inflow of the same amount.\r\nPhalippou and Gottschalg (2009) propose further corrections, such as different weightings of the funds and risk adjustments. However, the write-off of the NAVs has the most profound impact on their results.\r\nWriting off the NAVs of the funds fully has a dramatic impact on the performance metrics. However, Stucke (2011) questioned that the inactivity of those funds was really a sign of living dead investments and attributed it to poor data quality instead, which would make the results of Phalippou and Gottschalg (2009) obsolete. Without further ado, let’s discuss Stucke’s findings next.\r\nStucke (2011)\r\nThis study illustrates nicely the complications that arise in PE research from uncertainty about the data quality. Concretely, Stucke looks closer at the Thomson Venture Economics (“TVE”) data, which was until then the predominant data source for researchers. He finds that aggregate performance numbers of TVE are significantly smaller than those from other providers. More importantly, he is able to show that this is the result of about 40 percent of the funds in the TVE data not being updated.\r\nThis paper had a profound impact in academia as newer studies pretty much abandoned the TVE data set and focused on other sources instead (see, e.g., R. S. Harris, Jenkinson, and Kaplan (2014) further below). It also brought into question many of the findings of studies that used the TVE data. Finally, it helped explain why far too many GPs could claim to be a top-quartile fund.2\r\nThe starting point of Stucke’s investigation are the findings of Phalippou and Gottschalg (2009). In the previous section, I described how those authors took the evidence that the majority of funds had no cash flow activity and no changes in residual values as proof that these funds are “zombie” funds, i.e., funds that do not hold any meaningful residual value, despite reporting it. To deal with this issue, they set the residual value to 0 to calculate the fund’s performance. Stucke finds this curious as\r\n\r\nconstant NAVs rarely exist (particularly not over several years). Even in the later years of a fund’s lifetime the values of remaining investments get updated regularly. Furthermore, private equity funds without a single cash flow activity for more than three years should equally not exist (there will still be annual management fees or dividends from mature investments). In addition to this, the authors find that average NAVs of these funds equate to over 50% of the amount they invested. Keeping in mind that all of these funds are between 10 and 24 years old and most of them should be liquidated, remaining investments with a constant value as high as 50% of a fund’s size – on average – are surprising.\r\n\r\nStucke subsequently shows that the constant NAVs and missing cash flows are indeed due to the fact that TVE stopped to receive updates from the GPs about the fund performance. To do so, he matches 140 funds from the TVE data with a data set obtained from LPs and finds that these funds showed substantial value creation after TVE stopped to receive updates. Not surprisingly then, results based on the TVE data lead to a downward bias of PE. When Stucke only focuses on fully liquidated funds in the data, which are unaffected by this issue, he finds a meaningful outperformance of U.S. buyout funds compared to the S&P 500.\r\nR. S. Harris, Jenkinson, and Kaplan (2014)\r\nThis study, published in one of the most prestigious finance journals, is the first to use data sourced by Burgiss. Why is that such a big deal? Because it is notoriously difficult to obtain high quality PE data. There is typically no obligation by GPs and LPs to report their performance to any external parties. So other data providers, such as Preqin, must on data that is provided to them, either because some LPs are forced to do so - essentially U.S. public pension funds because of the Freedom of Information Act - or because GPs want to. This, of course, creates a few issues: as the pension funds are forced to publish the data, but there is no benefits for them to do so, the data provided is not really well documented. They also do not provide the data in a standardized and granular way. This makes it very difficult to figure out exactly what happens, and it is not uncommon to find different performance results from different pension funds for the very same fund. With data provided by GPs, there is a high risk of selection bias: obviously, GPs only have an interest to report those funds that perform well. In addition, GPs that had to close down because of their bad performance cannot report anymore (hindsight bias). Finally, you might have GPs with such a strong track record and such a high demand of LPs that they do not see any reason to publish their performance. And finally a GP might decide to stop reporting, as seemed to have happened in the case of TVE, with dire consequences as we have seen in the previous sections.\r\nBurgiss does not have these issues as they source the data directly from LPs that use their platform for their internal bookkeeping. As a result, the quality checks when entering the data are rigorous and there is good reason to believe that there is no selection bias: LPs don’t stop reporting on a fund just because it performs poorly. One could still argue that the group of LPs that use Burgiss did not assemble a portfolio that is representative of the whole PE universe. For example, it is likely that the LPs that use Burgiss are larger, more institutionalized LPs that tend to invest in larger funds. However, one would have to argue that such biases in fund selection by the LPs are correlated with the performance of the funds selected. And there is no strong reason to do so.\r\nR. S. Harris, Jenkinson, and Kaplan (2014) continue to show that their sample of U.S. buyout funds outperformed the S&P 500 by roughly 3% p.a., which is a significant premium. Furthermore, this outperformance happened fairly consistently over their time period from 1984 to 2008 (see Table IV in their study). They also use other indices such as the Russell 3000, which is focused more on smaller listed companies that might be a better comparison for PE. This slightly reduces the outperformance but does not get rid of it.\r\nFor VC, the picture is more mixed. While VC outperformed markets considerably in the 90s, this trend reversed in the 2000s until 2008. Overall, they find an average outperformance, but a median underperformance over the whole sample. One would assume that the strong performance over the last few years, so after the end of their analysis, for growth companies might have changed the picture a bit - more on this below. Overall, VC’s performance is more cyclical than the buyout one.\r\nIn their Table VIII, R. S. Harris, Jenkinson, and Kaplan (2014) compare the PMEs from the Burgiss data set with those from Cambridge Associates (CA), Preqin and Venture Economics. While the latter has a downward bias issue as uncovered by Stucke (2011), they show that the results with CA and Preqin data are rather similar. This result counters fears that because of the weaker data collection process applied by providers such as CA and Preqin systematic biases impact the conclusions meaningfully. Instead, the results are comparable. My co-author and I found the same when we compared the Preqin data set with two data sets that were sourced directly from the LPs or the GPs.\r\nPhalippou (2014)\r\nThis study does pretty much the same as R. S. Harris, Jenkinson, and Kaplan (2014), with two important deviations.\r\nFirst, it uses data from Preqin. However, to counter any criticism of data quality issues, it shows that the results are comparable to studies such as R. S. Harris, Jenkinson, and Kaplan (2014). This is of course not surprising, as they have made the same argument in their study.\r\nSecond, it argues that buyout funds invest in companies that are much smaller than most listed companies, that are value, instead of growth companies, and that use more leverage. As all these factors are known to have led to higher stock returns historically, Phalippou argues that one should use indices that adjust for these factors. Using a small value index essentially gets rid of any outperformance in the data, leveraging the index up based on some high-level assumptions, the mean and median PME falls below 1, indicating an underperformance of PE funds to these indices.\r\nSimilar results have been found by studies that looked at deal-level performance. Of course, it is important to bear in mind that the public indices Phalippou had to use to get the PME down had some of the strongest performances in U.S. public equity, which by itself is one of the strongest asset classes. It’s a bit like saying that compared to Michael Jordan, LeBron James is a worse basketball player…that might be true, but it certainly doesn’t mean he is bad at playing basketball.\r\nHonorable mentions\r\nThe literature on PE performance has become too vast to fully cover, so this overview cannot cover all studies published on the topic of PE performance. Above, I introduced a few studies in more detail, let’s mention briefly a few more:\r\nHooke and Yook (2016) compare 18 larger, well regarded buyout managers with the overall buyout market and find that their returns are slightly above average.\r\nSorensen, Wang, and Yang (2014) ask the question if the net performance of PE investments is enough to compensate the investors for the leverage and illiquidity they take on and the management and incentive fees they pay to the GPs. To answer it, they develop a model that incorporate these risk/costs and find that funds must generate substantial alpha to compensate investors for them.\r\nKorteweg and Sorensen (2017) build upon the work of Kaplan and Schoar (2005) and decompose PE performance into three components: long-term persistence of the GP; spurious persistence that exists due to exposure to the same market conditions of two consecutive funds; persistence due to luck or noise. They find that long-term persistence still exists, although it has declined in the 2000s, relative to the 1990s. Interestingly, the decline in persistence is strongest in the case of VC, while buyout GPs show much stronger persistence.\r\nCavagnaro et al. (2019) show that institutional investors’ skill in selecting PE funds is important in determining the returns. As Korteweg and Sorensen (2017) point out, this is a key piece to solve the puzzle as to why GP persistence continues to exist in PE: “Skilled PE firms are scarce, but LPs with the ability to identify these skilled firms can also be scarce.”\r\nBrown, Gredil, and Kaplan (2019) is one of my favorite studies as it helps me frequently in my job when I’m assessing a GP’s track record. They examine if one can trust the GP’s reported valuations. They should be mark-to-market, but we all know that there is enough wiggle room that one could be too optimistic or cautious, depending on the agenda of the GP. They find that GPs that struggle to raise a new fund inflate their valuations and in consequence their track record. Strong GPs, on the other hand, do not require such tricks and have no incentive to overpromise to their investors, which is why they typically value their assets conservatively. Finally, their results indicate that investors look through the valuation manipulation of poor GPs as it does not help them in raising new funds. With regards to the question about PE performance, this is a very important study as it shows that residual values cannot be taken at face value and one has to be careful to draw conclusions from funds that are not fully realized. After all, this was the premise of Phalippou and Gottschalg (2009), they just took an extreme approach (writing the residual value off).3\r\nSummary\r\nTo summarize: Earlier studies found no outperformance, or even an underperformance, of PE compared to the broader stock market. These results, however, were due to a bias in the data used. Using better data, often reported directly from the investors, newer studies found an outperformanceinstead. A plethora of research linked this outperformance with risks - for example due to illiquidity, leverage, and factor exposure - and costs (fees and carry), arguing that the true alpha, after adjusting for them, is closer to zero or even negative again.\r\nFrom a practitioner’s perspective, I find that there is often not much interest in explanations as to which factors drive the overall good returns of PE and arguments if they simply compensate investors for risk factors or can be considered true alpha. Alas, one can even argue that Buffett’s returns are simply due to exposure to some risk factors and leverage. How much do Buffett’s investors care?\r\nWhat is of great interest to practitioners is often a thorough understanding of PE returns, with a focus on metrics that are used in the industry (IRR, multiple / MOIC). Yes, they have shortcomings, but despite these shortcomings they are still widely used. This is the focus of the empirical part of this blog post, where I present the results for different strategies (VC, growth, buyout) and geographies with an up-to-date data set as of February 2022.\r\nData source and characteristics\r\nLet’s revisit some of topics discussed above with an up-to-date data set from Preqin. This is not the gold standard of data sets available - right now this can be considered to be Burgiss - but it has been shown repeatedly that results from Preqin lead to comparable conclusions as other, higher quality data sets (see, e.g., R. S. Harris, Jenkinson, and Kaplan (2014), Phalippou (2014) or Diller and Jäckel (2015)). I will only focus on funds for which Preqin provides cash flow data, which is only a subset of the funds that Preqin tracks and of the overall PE fund universe. I obtained the data in February 2022.\r\nIn this section, I will introduce the data set in more detail. As we have seen above, the quality of the data has a large impact on the results, and hence, it is important to get comfortable with the data set and understand its characteristics. However, readers interested in PE performance can skip this section.\r\nNumber of funds over time and by strategy\r\nBefore we look into the performance, let’s first have a look at the number and size of funds for which Preqin has cash flows. This gives us a better understanding of what share of the PE market the data set covers.\r\n\r\n\r\n\r\nLooking at the numbers of funds shows that Preqin only has a few funds per vintage year until the mid-90s, while they increase substantially over time thereafter. While there is a persistent long-term growth for buyout and growth funds, there is quite a bit of volatility across vintage years, in line with overall market cycles. For example, there are 72 buyout funds with a vintage year of 2007 in the sample, while there are only 32 for 2009. Interestingly, it took two decades to overtake 2000 as the all-time record year for VC funds, showing how keen investors were to invest in young companies around the dot-com bubble.\r\n\r\n\r\n\r\nFigure 1: Number of funds per vintage year with full cash flow data from Preqin, split by the strategy of fund.\r\n\r\n\r\n\r\nNumber of funds by region\r\nAnother interesting question is how many funds there are per geography. The table below gives the answer. Not surprisingly, North America is by far the largest contributor, which is also in line with current fundraising data that shows North America’s leading position. Europe is the second-largest market, followed by Asia. The other markets are very small in comparison. Going forward, I will therefore only report statistics for North America and Europe and pool the rest under “Rest of the World.”\r\n\r\n\r\nTable 1: Number of funds in the Preqin data set with cash flow data for different geographies. Vintage years from 1980 to 2019 considered.\r\n\r\n\r\nGeography\r\n\r\n\r\nBuyout\r\n\r\n\r\nGrowth\r\n\r\n\r\nVC\r\n\r\n\r\nTotal\r\n\r\n\r\nAfrica\r\n\r\n\r\n4\r\n\r\n\r\n7\r\n\r\n\r\n0\r\n\r\n\r\n11\r\n\r\n\r\nAmericas\r\n\r\n\r\n24\r\n\r\n\r\n7\r\n\r\n\r\n2\r\n\r\n\r\n33\r\n\r\n\r\nAsia\r\n\r\n\r\n56\r\n\r\n\r\n76\r\n\r\n\r\n45\r\n\r\n\r\n177\r\n\r\n\r\nAustralasia\r\n\r\n\r\n22\r\n\r\n\r\n2\r\n\r\n\r\n7\r\n\r\n\r\n31\r\n\r\n\r\nDiversified Multi-Regional\r\n\r\n\r\n2\r\n\r\n\r\n8\r\n\r\n\r\n2\r\n\r\n\r\n12\r\n\r\n\r\nEurope\r\n\r\n\r\n287\r\n\r\n\r\n47\r\n\r\n\r\n82\r\n\r\n\r\n416\r\n\r\n\r\nMiddle East & Israel\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n20\r\n\r\n\r\n24\r\n\r\n\r\nNorth America\r\n\r\n\r\n832\r\n\r\n\r\n179\r\n\r\n\r\n769\r\n\r\n\r\n1780\r\n\r\n\r\nGlobal\r\n\r\n\r\n1231\r\n\r\n\r\n326\r\n\r\n\r\n927\r\n\r\n\r\n2484\r\n\r\n\r\nComparison with other data sets\r\nAt the risk of repeating myself: Preqin, as any PE data set, does not cover the whole universe of PE funds. As such, one should be careful from drawing conclusions about PE in general when looking at one particular data set. One important question to ask is therefore if the data set at hand is comparable to other data sets. I argued above that Preqin leads to comparable results when looking at the performance of PE. Let’s look now if the Preqin data set also has a coverage that is similar to other sources. To do so, the plot below compares the number of funds in the Preqin data set with various other sources, as reported by R. S. Harris, Jenkinson, and Kaplan (2014) in their Table I. As they look at vintages from 1984 to 2008 and only North American funds, I limit the sample accordingly. Also, they only report numbers for buyout and VC funds. As I do not know how they classify growth funds, I ignore them.\r\n\r\n\r\n\r\nFigure 2: Comparison of number of funds from North American funds in the Preqin data with other data sets, as provided by Harris et al. (2014) (see Table I).\r\n\r\n\r\n\r\nWith regards to buyout funds, Preqin (dark blue) is comparable with Burgiss (yellow) and other data providers, although being on the lower end of funds covered. For VC, the coverage of Preqin is substantially lower than others, in particular until around 2000. Most notably Cambridge Associates and Venture Economics cover many more funds. However, Preqin’s coverage seemed to have increased in the 2000s. With regards to the coverage over time, the patterns are comparable across the main data providers: strong increase until 2000, sharp decrease during the dot-com bubble crash, steady increase thereafter. This gives me comfort that these data sets cover the overall PE market cycle, rather than some idiosyncratic data collection trends.4\r\nCoverage in relation to PE universe\r\nWhile Preqin is comparable to other data providers in terms of coverage, especially after 2000, the question remains how good the coverage is. Do those data providers cover most of the PE universe or only a fraction of it? There will not be a perfect answer to this question as no one knows for sure how large the PE universe really is. However, we can look at data that estimates the size of the overall PE market and compare it with the size by the funds covered by Preqin.\r\nIn the table below, I compare fundraising numbers for the overall private markets, as reported by McKinsey (2021) (see Exhibit 2), with the aggregate size of the funds in Preqin that have cash flow data. As the McKinsey study only reports fundraising numbers over time for private markets, not PE, I estimate the ratio by multiplying the historic numbers with the 2020 ratio shown in Exhibit 1 of their report (59%, USD 857.8bn for private markets vs. USD 502.9bn for PE). This is obviously a rather crude measure, but it should give use a ballpark idea of how good the coverage of Preqin is.\r\n\r\n\r\nTable 2: For vintage years from 2010 to 2019, the table compares the overall capital raised in private markets and PE (estimated) with the total size of the funds for which Preqin has cash flow data. Data for the market comes from the McKinsey Global Private Markets Review 2021. Numbers in USD billion.\r\n\r\n\r\nVintage year\r\n\r\n\r\nPrivate Markets Fundraising\r\n\r\n\r\nPE Fundraising (estimated)\r\n\r\n\r\nTotal size of Preqin funds with CF data\r\n\r\n\r\nCoverage\r\n\r\n\r\n2010\r\n\r\n\r\n311\r\n\r\n\r\n182\r\n\r\n\r\n45\r\n\r\n\r\n25%\r\n\r\n\r\n2011\r\n\r\n\r\n378\r\n\r\n\r\n222\r\n\r\n\r\n103\r\n\r\n\r\n46%\r\n\r\n\r\n2012\r\n\r\n\r\n434\r\n\r\n\r\n254\r\n\r\n\r\n115\r\n\r\n\r\n45%\r\n\r\n\r\n2013\r\n\r\n\r\n593\r\n\r\n\r\n348\r\n\r\n\r\n94\r\n\r\n\r\n27%\r\n\r\n\r\n2014\r\n\r\n\r\n670\r\n\r\n\r\n393\r\n\r\n\r\n171\r\n\r\n\r\n44%\r\n\r\n\r\n2015\r\n\r\n\r\n733\r\n\r\n\r\n430\r\n\r\n\r\n137\r\n\r\n\r\n32%\r\n\r\n\r\n2016\r\n\r\n\r\n846\r\n\r\n\r\n496\r\n\r\n\r\n211\r\n\r\n\r\n43%\r\n\r\n\r\n2017\r\n\r\n\r\n979\r\n\r\n\r\n574\r\n\r\n\r\n171\r\n\r\n\r\n30%\r\n\r\n\r\n2018\r\n\r\n\r\n1040\r\n\r\n\r\n610\r\n\r\n\r\n288\r\n\r\n\r\n47%\r\n\r\n\r\n2019\r\n\r\n\r\n1091\r\n\r\n\r\n640\r\n\r\n\r\n304\r\n\r\n\r\n48%\r\n\r\n\r\nThe results are rather comforting as Preqin has on average a coverage of 39% over the last decade. They can’t cover the full universe, but quite a big chunk of it! In the following, I will make the implicit assumptions that all results I find from the Preqin data set apply to the overall PE universe. Of course, there is always the risk that I report patterns that are due to the specific biases of the Preqin data set and do not apply to the PE universe. However, it’s a risk I’m willing to take. It’s a blog after all, not a PhD thesis…\r\nSize of funds\r\nI apologize for the long table below, I know it’s dense, but I think also insightful. The table shows, for each strategy and vintage year, three statistics: the number of funds, the average fund size, and the aggregate fund size, which is the product of the first two.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBuyout\r\n\r\n\r\n\r\n\r\nGrowth\r\n\r\n\r\n\r\n\r\nVC\r\n\r\n\r\n\r\n\r\nVintage\r\n\r\n\r\n\r\n\r\nNr. funds\r\n\r\n\r\n\r\n\r\nAvg. size (in USDm)\r\n\r\n\r\n\r\n\r\nAgg. size (in USDbn)\r\n\r\n\r\n\r\n\r\nNr. funds\r\n\r\n\r\n\r\n\r\nAvg. size (in USDm)\r\n\r\n\r\n\r\n\r\nAgg. size (in USDbn)\r\n\r\n\r\n\r\n\r\nNr. funds\r\n\r\n\r\n\r\n\r\nAvg. size (in USDm)\r\n\r\n\r\n\r\n\r\nAgg. size (in USDbn)\r\n\r\n\r\n\r\n1990\r\n\r\n\r\n7\r\n\r\n\r\n425.5\r\n\r\n\r\n2.6\r\n\r\n\r\n2\r\n\r\n\r\n953.0\r\n\r\n\r\n1.9\r\n\r\n\r\n7\r\n\r\n\r\n111.4\r\n\r\n\r\n0.8\r\n\r\n\r\n1991\r\n\r\n\r\n2\r\n\r\n\r\n121.0\r\n\r\n\r\n0.2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n3\r\n\r\n\r\n87.4\r\n\r\n\r\n0.2\r\n\r\n\r\n1992\r\n\r\n\r\n8\r\n\r\n\r\n280.8\r\n\r\n\r\n1.7\r\n\r\n\r\n1\r\n\r\n\r\n184.0\r\n\r\n\r\n0.2\r\n\r\n\r\n10\r\n\r\n\r\n132.6\r\n\r\n\r\n1.2\r\n\r\n\r\n1993\r\n\r\n\r\n10\r\n\r\n\r\n322.6\r\n\r\n\r\n2.9\r\n\r\n\r\n2\r\n\r\n\r\n164.5\r\n\r\n\r\n0.3\r\n\r\n\r\n9\r\n\r\n\r\n105.6\r\n\r\n\r\n1.0\r\n\r\n\r\n1994\r\n\r\n\r\n16\r\n\r\n\r\n493.1\r\n\r\n\r\n7.9\r\n\r\n\r\n2\r\n\r\n\r\n263.1\r\n\r\n\r\n0.5\r\n\r\n\r\n7\r\n\r\n\r\n113.7\r\n\r\n\r\n0.7\r\n\r\n\r\n1995\r\n\r\n\r\n17\r\n\r\n\r\n661.6\r\n\r\n\r\n11.2\r\n\r\n\r\n4\r\n\r\n\r\n191.8\r\n\r\n\r\n0.8\r\n\r\n\r\n13\r\n\r\n\r\n102.6\r\n\r\n\r\n1.3\r\n\r\n\r\n1996\r\n\r\n\r\n18\r\n\r\n\r\n399.9\r\n\r\n\r\n7.2\r\n\r\n\r\n4\r\n\r\n\r\n279.5\r\n\r\n\r\n1.1\r\n\r\n\r\n20\r\n\r\n\r\n198.7\r\n\r\n\r\n4.0\r\n\r\n\r\n1997\r\n\r\n\r\n23\r\n\r\n\r\n786.6\r\n\r\n\r\n18.1\r\n\r\n\r\n5\r\n\r\n\r\n440.4\r\n\r\n\r\n2.2\r\n\r\n\r\n20\r\n\r\n\r\n136.9\r\n\r\n\r\n2.6\r\n\r\n\r\n1998\r\n\r\n\r\n40\r\n\r\n\r\n978.7\r\n\r\n\r\n39.1\r\n\r\n\r\n8\r\n\r\n\r\n1,063.9\r\n\r\n\r\n8.5\r\n\r\n\r\n28\r\n\r\n\r\n246.4\r\n\r\n\r\n6.9\r\n\r\n\r\n1999\r\n\r\n\r\n28\r\n\r\n\r\n1,013.7\r\n\r\n\r\n28.4\r\n\r\n\r\n5\r\n\r\n\r\n574.8\r\n\r\n\r\n2.9\r\n\r\n\r\n37\r\n\r\n\r\n373.2\r\n\r\n\r\n13.4\r\n\r\n\r\n2000\r\n\r\n\r\n34\r\n\r\n\r\n1,579.3\r\n\r\n\r\n53.7\r\n\r\n\r\n14\r\n\r\n\r\n999.4\r\n\r\n\r\n14.0\r\n\r\n\r\n73\r\n\r\n\r\n426.5\r\n\r\n\r\n31.1\r\n\r\n\r\n2001\r\n\r\n\r\n22\r\n\r\n\r\n1,413.3\r\n\r\n\r\n31.1\r\n\r\n\r\n7\r\n\r\n\r\n1,185.8\r\n\r\n\r\n8.3\r\n\r\n\r\n46\r\n\r\n\r\n452.6\r\n\r\n\r\n20.4\r\n\r\n\r\n2002\r\n\r\n\r\n25\r\n\r\n\r\n1,053.5\r\n\r\n\r\n26.3\r\n\r\n\r\n6\r\n\r\n\r\n156.8\r\n\r\n\r\n0.9\r\n\r\n\r\n24\r\n\r\n\r\n260.5\r\n\r\n\r\n6.3\r\n\r\n\r\n2003\r\n\r\n\r\n17\r\n\r\n\r\n1,848.3\r\n\r\n\r\n31.4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n17\r\n\r\n\r\n253.4\r\n\r\n\r\n4.3\r\n\r\n\r\n2004\r\n\r\n\r\n27\r\n\r\n\r\n1,266.0\r\n\r\n\r\n34.2\r\n\r\n\r\n4\r\n\r\n\r\n931.8\r\n\r\n\r\n3.7\r\n\r\n\r\n28\r\n\r\n\r\n225.7\r\n\r\n\r\n6.3\r\n\r\n\r\n2005\r\n\r\n\r\n58\r\n\r\n\r\n1,751.7\r\n\r\n\r\n101.6\r\n\r\n\r\n8\r\n\r\n\r\n1,385.6\r\n\r\n\r\n11.1\r\n\r\n\r\n26\r\n\r\n\r\n278.1\r\n\r\n\r\n7.2\r\n\r\n\r\n2006\r\n\r\n\r\n60\r\n\r\n\r\n3,015.5\r\n\r\n\r\n180.9\r\n\r\n\r\n14\r\n\r\n\r\n1,178.0\r\n\r\n\r\n16.5\r\n\r\n\r\n40\r\n\r\n\r\n489.5\r\n\r\n\r\n19.6\r\n\r\n\r\n2007\r\n\r\n\r\n72\r\n\r\n\r\n2,706.9\r\n\r\n\r\n194.9\r\n\r\n\r\n14\r\n\r\n\r\n1,814.0\r\n\r\n\r\n25.4\r\n\r\n\r\n46\r\n\r\n\r\n276.5\r\n\r\n\r\n12.7\r\n\r\n\r\n2008\r\n\r\n\r\n63\r\n\r\n\r\n2,608.1\r\n\r\n\r\n164.3\r\n\r\n\r\n13\r\n\r\n\r\n1,436.4\r\n\r\n\r\n18.7\r\n\r\n\r\n34\r\n\r\n\r\n403.2\r\n\r\n\r\n13.3\r\n\r\n\r\n2009\r\n\r\n\r\n32\r\n\r\n\r\n1,510.8\r\n\r\n\r\n48.3\r\n\r\n\r\n8\r\n\r\n\r\n833.5\r\n\r\n\r\n6.7\r\n\r\n\r\n17\r\n\r\n\r\n304.0\r\n\r\n\r\n4.3\r\n\r\n\r\n2010\r\n\r\n\r\n35\r\n\r\n\r\n871.3\r\n\r\n\r\n30.5\r\n\r\n\r\n12\r\n\r\n\r\n763.5\r\n\r\n\r\n9.2\r\n\r\n\r\n15\r\n\r\n\r\n357.3\r\n\r\n\r\n5.0\r\n\r\n\r\n2011\r\n\r\n\r\n41\r\n\r\n\r\n2,115.7\r\n\r\n\r\n86.7\r\n\r\n\r\n15\r\n\r\n\r\n635.2\r\n\r\n\r\n9.5\r\n\r\n\r\n24\r\n\r\n\r\n299.4\r\n\r\n\r\n6.9\r\n\r\n\r\n2012\r\n\r\n\r\n59\r\n\r\n\r\n1,483.1\r\n\r\n\r\n87.5\r\n\r\n\r\n14\r\n\r\n\r\n1,328.0\r\n\r\n\r\n18.6\r\n\r\n\r\n23\r\n\r\n\r\n387.0\r\n\r\n\r\n8.5\r\n\r\n\r\n2013\r\n\r\n\r\n53\r\n\r\n\r\n1,423.7\r\n\r\n\r\n75.5\r\n\r\n\r\n10\r\n\r\n\r\n1,068.5\r\n\r\n\r\n10.7\r\n\r\n\r\n22\r\n\r\n\r\n358.2\r\n\r\n\r\n7.5\r\n\r\n\r\n2014\r\n\r\n\r\n58\r\n\r\n\r\n2,484.6\r\n\r\n\r\n141.6\r\n\r\n\r\n23\r\n\r\n\r\n909.5\r\n\r\n\r\n20.9\r\n\r\n\r\n33\r\n\r\n\r\n274.3\r\n\r\n\r\n8.5\r\n\r\n\r\n2015\r\n\r\n\r\n60\r\n\r\n\r\n1,586.6\r\n\r\n\r\n95.2\r\n\r\n\r\n24\r\n\r\n\r\n1,214.2\r\n\r\n\r\n27.9\r\n\r\n\r\n38\r\n\r\n\r\n385.1\r\n\r\n\r\n13.5\r\n\r\n\r\n2016\r\n\r\n\r\n91\r\n\r\n\r\n2,080.8\r\n\r\n\r\n185.2\r\n\r\n\r\n13\r\n\r\n\r\n994.9\r\n\r\n\r\n11.9\r\n\r\n\r\n44\r\n\r\n\r\n353.5\r\n\r\n\r\n14.1\r\n\r\n\r\n2017\r\n\r\n\r\n66\r\n\r\n\r\n2,113.7\r\n\r\n\r\n139.5\r\n\r\n\r\n22\r\n\r\n\r\n638.8\r\n\r\n\r\n13.4\r\n\r\n\r\n57\r\n\r\n\r\n353.0\r\n\r\n\r\n18.0\r\n\r\n\r\n2018\r\n\r\n\r\n74\r\n\r\n\r\n3,060.0\r\n\r\n\r\n220.3\r\n\r\n\r\n37\r\n\r\n\r\n1,357.5\r\n\r\n\r\n50.2\r\n\r\n\r\n56\r\n\r\n\r\n354.2\r\n\r\n\r\n17.7\r\n\r\n\r\n2019\r\n\r\n\r\n94\r\n\r\n\r\n2,668.7\r\n\r\n\r\n237.5\r\n\r\n\r\n32\r\n\r\n\r\n1,142.2\r\n\r\n\r\n35.4\r\n\r\n\r\n78\r\n\r\n\r\n456.8\r\n\r\n\r\n31.5\r\n\r\n\r\nWhile buyout funds only make up 50% with regards to their numbers, they are responsible for 79% of the aggregate due to their larger size than VC and growth funds. Growth funds have raised a bit more capital overall in the sample than VC (USD 331bn vs. 289bn), despite much lower number of funds (see chart above). These funds can be quite sizable, too, with brands like Warburg Pincus and Insight Partners that raise multi-billion funds.\r\nWith regards to fund size growth over the years, VC had the steadiest and slowest out of all of them: VCs were raising on average a few hundred million in the 90s, and they continue to do so in the late 2010s. As VCs are backing founders and their visions and often not much more, the capital needs per investment simply don’t change much over the decades. That also explains why the strong VCs can be so access restricted: getting more capital is not their concern, it’s about finding the right teams to back. For buyout funds, the story is different: they can always write bigger checks or do larger M&As, etc. That’s why the fund sizes have grown much more over the years. However, fundraising is also more cyclical. For example, while a lot of buyout GPs collected money for large funds in 2006 and 2007, both the number and the size of funds plummeted in 2009/10. For VC funds, only the number of funds declined, while the fund sizes remained stable. My best guess is that some VCs with weaker track records were unable to raise follow-on funds, while the strong brands had a long line of LPs who were waiting to get it so that they could replace those LPs that suffered from the GFC.\r\nAnother interesting trend is that for buyout funds, both the fund size and the number of funds increased close to three times from 2010 to 2019. In contrast, the number of VC funds increased by a factor of 5.2x in the same period, while the fund size only increased by 28%. When things go well, LPs deploy more capital into the established buyout managers, while they give money to newer VCs as the capacity to the established brands is limited.\r\nSo far, I looked at average fund sizes per strategy, but what about the dispersion? The table below gives the answer.\r\n\r\n\r\nTable 3: Mean and different percentiles of fund sizes for buyout, growth, and VC funds in the Preqin data set. Numbers in USD million.\r\n\r\n\r\nStrategy\r\n\r\n\r\n5%\r\n\r\n\r\n10%\r\n\r\n\r\n25%\r\n\r\n\r\n50%\r\n\r\n\r\nMean\r\n\r\n\r\n75%\r\n\r\n\r\n90%\r\n\r\n\r\n95%\r\n\r\n\r\nVC\r\n\r\n\r\n28\r\n\r\n\r\n54\r\n\r\n\r\n115\r\n\r\n\r\n225\r\n\r\n\r\n337\r\n\r\n\r\n425\r\n\r\n\r\n677\r\n\r\n\r\n909\r\n\r\n\r\nBuyout\r\n\r\n\r\n121\r\n\r\n\r\n199\r\n\r\n\r\n360\r\n\r\n\r\n821\r\n\r\n\r\n1,933\r\n\r\n\r\n2,259\r\n\r\n\r\n4,859\r\n\r\n\r\n7,270\r\n\r\n\r\nGrowth\r\n\r\n\r\n45\r\n\r\n\r\n79\r\n\r\n\r\n199\r\n\r\n\r\n497\r\n\r\n\r\n1,094\r\n\r\n\r\n1,038\r\n\r\n\r\n2,100\r\n\r\n\r\n3,586\r\n\r\n\r\nFor all three strategies, there is a large spread between the 5th and the 95th percentile. Not surprisingly, the spread is much larger for buyout and growth funds (factor 60x and 79x) than for VC (factor 33x). Again, the most likely explanation is that successful VCs must limit their fund sizes much more, as capital needs per investment are not that dependent on the market cycle and they don’t want to dilute their returns by investing in more start-ups. Instead, they want to focus on those with the biggest chance of succeeding.\r\nAs a result, the distribution is more right-skewed for buyout and growth with a much larger mean than median. Yes, the average buyout fund size is USD 1,933 million, but the average buyout fund only has a size of USD 821 million.\r\nPerformance of PE\r\nTotal-Value to Paid-In (“TVPI”)\r\nBy vintage year\r\nThe table below shows the mean, median, and weighted mean TVPIs, or multiples, by strategy and vintage year from 1990 to 2019. It also shows the ratio of active to all funds for each group. It’s important to keep in mind that for the younger vintage years, many funds are still unrealized and TVPIs are expected to increase further. Hence, this is not the final result, just a snapshot. Hopefully, the 2019 vintage doesn’t end up slightly above 1x!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBuyout\r\n\r\n\r\n\r\n\r\nGrowth\r\n\r\n\r\n\r\n\r\nVC\r\n\r\n\r\n\r\n\r\nVintage\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean TVPI\r\n\r\n\r\n\r\n\r\nMedian TVPI\r\n\r\n\r\n\r\n\r\nMean TVPI (weighted by size)\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean TVPI\r\n\r\n\r\n\r\n\r\nMedian TVPI\r\n\r\n\r\n\r\n\r\nMean TVPI (weighted by size)\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean TVPI\r\n\r\n\r\n\r\n\r\nMedian TVPI\r\n\r\n\r\n\r\n\r\nMean TVPI (weighted by size)\r\n\r\n\r\n\r\nTable 4: Mean, median, and weighted mean TVPIs by strategy and vintage year in the Preqin data set. The weighting is based on fund size. As some funds do not have a fund size reported in the data set, the sample size of the weighted mean can be lower than for the mean and median. The ratio of active funds is based on the reported status in Preqin.\r\n\r\n\r\n1990\r\n\r\n\r\n0%\r\n\r\n\r\n2.05\r\n\r\n\r\n2.36\r\n\r\n\r\n1.98\r\n\r\n\r\n0%\r\n\r\n\r\n2.27\r\n\r\n\r\n2.27\r\n\r\n\r\n2.38\r\n\r\n\r\n0%\r\n\r\n\r\n2.15\r\n\r\n\r\n2.22\r\n\r\n\r\n2.15\r\n\r\n\r\n1991\r\n\r\n\r\n0%\r\n\r\n\r\n2.21\r\n\r\n\r\n2.21\r\n\r\n\r\n2.55\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n0%\r\n\r\n\r\n2.11\r\n\r\n\r\n2.28\r\n\r\n\r\n2.83\r\n\r\n\r\n1992\r\n\r\n\r\n0%\r\n\r\n\r\n1.59\r\n\r\n\r\n1.19\r\n\r\n\r\n2.15\r\n\r\n\r\n0%\r\n\r\n\r\n2.82\r\n\r\n\r\n2.82\r\n\r\n\r\n2.82\r\n\r\n\r\n0%\r\n\r\n\r\n3.61\r\n\r\n\r\n3.09\r\n\r\n\r\n3.54\r\n\r\n\r\n1993\r\n\r\n\r\n0%\r\n\r\n\r\n2.45\r\n\r\n\r\n2.19\r\n\r\n\r\n2.60\r\n\r\n\r\n0%\r\n\r\n\r\n2.30\r\n\r\n\r\n2.30\r\n\r\n\r\n2.22\r\n\r\n\r\n0%\r\n\r\n\r\n3.59\r\n\r\n\r\n3.11\r\n\r\n\r\n4.03\r\n\r\n\r\n1994\r\n\r\n\r\n0%\r\n\r\n\r\n2.07\r\n\r\n\r\n2.05\r\n\r\n\r\n2.15\r\n\r\n\r\n0%\r\n\r\n\r\n2.02\r\n\r\n\r\n2.02\r\n\r\n\r\n2.24\r\n\r\n\r\n0%\r\n\r\n\r\n3.77\r\n\r\n\r\n2.91\r\n\r\n\r\n3.89\r\n\r\n\r\n1995\r\n\r\n\r\n0%\r\n\r\n\r\n1.48\r\n\r\n\r\n1.35\r\n\r\n\r\n1.63\r\n\r\n\r\n0%\r\n\r\n\r\n1.92\r\n\r\n\r\n2.35\r\n\r\n\r\n2.38\r\n\r\n\r\n0%\r\n\r\n\r\n3.75\r\n\r\n\r\n2.13\r\n\r\n\r\n4.32\r\n\r\n\r\n1996\r\n\r\n\r\n0%\r\n\r\n\r\n1.52\r\n\r\n\r\n1.70\r\n\r\n\r\n1.61\r\n\r\n\r\n0%\r\n\r\n\r\n1.47\r\n\r\n\r\n1.26\r\n\r\n\r\n1.36\r\n\r\n\r\n5%\r\n\r\n\r\n3.21\r\n\r\n\r\n1.58\r\n\r\n\r\n3.52\r\n\r\n\r\n1997\r\n\r\n\r\n4%\r\n\r\n\r\n1.41\r\n\r\n\r\n1.46\r\n\r\n\r\n1.61\r\n\r\n\r\n0%\r\n\r\n\r\n1.57\r\n\r\n\r\n1.47\r\n\r\n\r\n1.48\r\n\r\n\r\n0%\r\n\r\n\r\n1.87\r\n\r\n\r\n1.35\r\n\r\n\r\n1.98\r\n\r\n\r\n1998\r\n\r\n\r\n5%\r\n\r\n\r\n1.45\r\n\r\n\r\n1.36\r\n\r\n\r\n1.44\r\n\r\n\r\n0%\r\n\r\n\r\n1.38\r\n\r\n\r\n1.40\r\n\r\n\r\n1.58\r\n\r\n\r\n18%\r\n\r\n\r\n1.65\r\n\r\n\r\n0.97\r\n\r\n\r\n1.66\r\n\r\n\r\n1999\r\n\r\n\r\n11%\r\n\r\n\r\n1.57\r\n\r\n\r\n1.56\r\n\r\n\r\n1.60\r\n\r\n\r\n0%\r\n\r\n\r\n1.82\r\n\r\n\r\n1.35\r\n\r\n\r\n1.40\r\n\r\n\r\n16%\r\n\r\n\r\n0.77\r\n\r\n\r\n0.65\r\n\r\n\r\n0.75\r\n\r\n\r\n2000\r\n\r\n\r\n15%\r\n\r\n\r\n1.94\r\n\r\n\r\n2.00\r\n\r\n\r\n1.99\r\n\r\n\r\n0%\r\n\r\n\r\n1.49\r\n\r\n\r\n1.52\r\n\r\n\r\n1.51\r\n\r\n\r\n18%\r\n\r\n\r\n0.92\r\n\r\n\r\n0.94\r\n\r\n\r\n0.96\r\n\r\n\r\n2001\r\n\r\n\r\n32%\r\n\r\n\r\n1.86\r\n\r\n\r\n1.83\r\n\r\n\r\n2.00\r\n\r\n\r\n14%\r\n\r\n\r\n2.12\r\n\r\n\r\n2.34\r\n\r\n\r\n2.24\r\n\r\n\r\n39%\r\n\r\n\r\n1.20\r\n\r\n\r\n1.04\r\n\r\n\r\n1.27\r\n\r\n\r\n2002\r\n\r\n\r\n12%\r\n\r\n\r\n1.81\r\n\r\n\r\n1.91\r\n\r\n\r\n1.98\r\n\r\n\r\n33%\r\n\r\n\r\n1.30\r\n\r\n\r\n1.42\r\n\r\n\r\n1.36\r\n\r\n\r\n33%\r\n\r\n\r\n0.96\r\n\r\n\r\n0.94\r\n\r\n\r\n1.13\r\n\r\n\r\n2003\r\n\r\n\r\n35%\r\n\r\n\r\n1.94\r\n\r\n\r\n1.77\r\n\r\n\r\n2.03\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n41%\r\n\r\n\r\n1.20\r\n\r\n\r\n1.23\r\n\r\n\r\n1.17\r\n\r\n\r\n2004\r\n\r\n\r\n22%\r\n\r\n\r\n1.71\r\n\r\n\r\n1.72\r\n\r\n\r\n1.78\r\n\r\n\r\n25%\r\n\r\n\r\n1.57\r\n\r\n\r\n1.73\r\n\r\n\r\n1.78\r\n\r\n\r\n39%\r\n\r\n\r\n1.40\r\n\r\n\r\n0.77\r\n\r\n\r\n1.23\r\n\r\n\r\n2005\r\n\r\n\r\n52%\r\n\r\n\r\n1.59\r\n\r\n\r\n1.54\r\n\r\n\r\n1.70\r\n\r\n\r\n38%\r\n\r\n\r\n2.24\r\n\r\n\r\n1.99\r\n\r\n\r\n1.92\r\n\r\n\r\n85%\r\n\r\n\r\n1.71\r\n\r\n\r\n1.40\r\n\r\n\r\n1.82\r\n\r\n\r\n2006\r\n\r\n\r\n57%\r\n\r\n\r\n1.54\r\n\r\n\r\n1.53\r\n\r\n\r\n1.49\r\n\r\n\r\n57%\r\n\r\n\r\n2.34\r\n\r\n\r\n1.75\r\n\r\n\r\n1.35\r\n\r\n\r\n70%\r\n\r\n\r\n1.31\r\n\r\n\r\n1.25\r\n\r\n\r\n1.35\r\n\r\n\r\n2007\r\n\r\n\r\n82%\r\n\r\n\r\n1.55\r\n\r\n\r\n1.51\r\n\r\n\r\n1.55\r\n\r\n\r\n57%\r\n\r\n\r\n1.70\r\n\r\n\r\n1.57\r\n\r\n\r\n1.70\r\n\r\n\r\n91%\r\n\r\n\r\n2.16\r\n\r\n\r\n1.86\r\n\r\n\r\n2.19\r\n\r\n\r\n2008\r\n\r\n\r\n79%\r\n\r\n\r\n1.74\r\n\r\n\r\n1.68\r\n\r\n\r\n1.72\r\n\r\n\r\n77%\r\n\r\n\r\n1.62\r\n\r\n\r\n1.22\r\n\r\n\r\n1.22\r\n\r\n\r\n74%\r\n\r\n\r\n2.08\r\n\r\n\r\n1.41\r\n\r\n\r\n1.62\r\n\r\n\r\n2009\r\n\r\n\r\n94%\r\n\r\n\r\n1.83\r\n\r\n\r\n1.74\r\n\r\n\r\n1.88\r\n\r\n\r\n88%\r\n\r\n\r\n1.57\r\n\r\n\r\n1.25\r\n\r\n\r\n1.70\r\n\r\n\r\n94%\r\n\r\n\r\n1.95\r\n\r\n\r\n1.70\r\n\r\n\r\n1.86\r\n\r\n\r\n2010\r\n\r\n\r\n74%\r\n\r\n\r\n1.84\r\n\r\n\r\n1.99\r\n\r\n\r\n1.89\r\n\r\n\r\n100%\r\n\r\n\r\n2.02\r\n\r\n\r\n1.57\r\n\r\n\r\n1.82\r\n\r\n\r\n93%\r\n\r\n\r\n1.94\r\n\r\n\r\n1.84\r\n\r\n\r\n1.94\r\n\r\n\r\n2011\r\n\r\n\r\n100%\r\n\r\n\r\n1.67\r\n\r\n\r\n1.69\r\n\r\n\r\n1.90\r\n\r\n\r\n87%\r\n\r\n\r\n2.19\r\n\r\n\r\n1.91\r\n\r\n\r\n2.29\r\n\r\n\r\n96%\r\n\r\n\r\n3.26\r\n\r\n\r\n2.44\r\n\r\n\r\n2.97\r\n\r\n\r\n2012\r\n\r\n\r\n92%\r\n\r\n\r\n1.84\r\n\r\n\r\n1.86\r\n\r\n\r\n1.86\r\n\r\n\r\n93%\r\n\r\n\r\n1.63\r\n\r\n\r\n1.69\r\n\r\n\r\n1.61\r\n\r\n\r\n100%\r\n\r\n\r\n3.20\r\n\r\n\r\n2.08\r\n\r\n\r\n3.20\r\n\r\n\r\n2013\r\n\r\n\r\n96%\r\n\r\n\r\n1.78\r\n\r\n\r\n1.73\r\n\r\n\r\n1.75\r\n\r\n\r\n80%\r\n\r\n\r\n1.62\r\n\r\n\r\n1.48\r\n\r\n\r\n1.75\r\n\r\n\r\n100%\r\n\r\n\r\n2.16\r\n\r\n\r\n1.89\r\n\r\n\r\n2.21\r\n\r\n\r\n2014\r\n\r\n\r\n100%\r\n\r\n\r\n1.99\r\n\r\n\r\n1.68\r\n\r\n\r\n1.91\r\n\r\n\r\n100%\r\n\r\n\r\n1.84\r\n\r\n\r\n1.60\r\n\r\n\r\n1.79\r\n\r\n\r\n97%\r\n\r\n\r\n2.61\r\n\r\n\r\n2.27\r\n\r\n\r\n2.85\r\n\r\n\r\n2015\r\n\r\n\r\n100%\r\n\r\n\r\n1.70\r\n\r\n\r\n1.53\r\n\r\n\r\n1.72\r\n\r\n\r\n100%\r\n\r\n\r\n1.69\r\n\r\n\r\n1.56\r\n\r\n\r\n1.77\r\n\r\n\r\n100%\r\n\r\n\r\n1.98\r\n\r\n\r\n1.72\r\n\r\n\r\n1.97\r\n\r\n\r\n2016\r\n\r\n\r\n99%\r\n\r\n\r\n1.53\r\n\r\n\r\n1.43\r\n\r\n\r\n1.56\r\n\r\n\r\n100%\r\n\r\n\r\n1.44\r\n\r\n\r\n1.46\r\n\r\n\r\n1.80\r\n\r\n\r\n100%\r\n\r\n\r\n2.08\r\n\r\n\r\n1.80\r\n\r\n\r\n1.94\r\n\r\n\r\n2017\r\n\r\n\r\n100%\r\n\r\n\r\n1.54\r\n\r\n\r\n1.48\r\n\r\n\r\n1.70\r\n\r\n\r\n100%\r\n\r\n\r\n1.51\r\n\r\n\r\n1.40\r\n\r\n\r\n1.73\r\n\r\n\r\n100%\r\n\r\n\r\n1.69\r\n\r\n\r\n1.44\r\n\r\n\r\n1.68\r\n\r\n\r\n2018\r\n\r\n\r\n100%\r\n\r\n\r\n1.25\r\n\r\n\r\n1.21\r\n\r\n\r\n1.32\r\n\r\n\r\n100%\r\n\r\n\r\n1.39\r\n\r\n\r\n1.30\r\n\r\n\r\n1.58\r\n\r\n\r\n100%\r\n\r\n\r\n1.57\r\n\r\n\r\n1.34\r\n\r\n\r\n1.60\r\n\r\n\r\n2019\r\n\r\n\r\n100%\r\n\r\n\r\n1.22\r\n\r\n\r\n1.11\r\n\r\n\r\n1.26\r\n\r\n\r\n100%\r\n\r\n\r\n1.18\r\n\r\n\r\n1.04\r\n\r\n\r\n1.25\r\n\r\n\r\n100%\r\n\r\n\r\n1.33\r\n\r\n\r\n1.18\r\n\r\n\r\n1.35\r\n\r\n\r\n\r\n\r\n\r\nThere is a lot to look at in this table, so let’s get to it. First, let’s stay with the ratio of active funds. Obviously, it’s increasing over time: most funds that started in the 90s are now fully realized, while almost none is from the last decade. Hence, the most interesting decade is the one from 2000 to 2010. While almost all funds have a ten-year term, almost none of them is realized within it. Even the majority of pre-GFC funds with vintages of 2006/7 are still active in 2021/22! This resonates well with an article I recently wrote together with a colleague in PGIM’s Best Ideas series where we show that a lot of the value of a PE fund is still unrealized after 10 years.\r\nThe difference across the strategies are also noteworthy: buyout and growth funds are fully liquidated earlier than VC funds. For example, in 14 out of 19 years from 1996 to 2015, the ratio of active VC funds is higher than for buyout funds.\r\nTurning the attention to the TVPIs, one finds that the volatility over time is rather low for buyout funds: looking at vintages 2015 or older, where most of the value creation has already happened, a typical buyout fund’s TVPI lies between 1.19x and 2.36x for each vintage year. In most years, the TVPIs are in a much smaller corridor from 1.4x to 2x. The pattern for growth funds is similar. This is a great result for investors: even if you picked mediocre buyout and growth funds each year, they all made money. This is not true for VC funds, where there are several vintage years (1998, 1999, 2000, 2002, 2004) where the median fund had a TVPI below 1x. On the plus side, returns can be much higher in good years as well, as is shown by the fact that VC is the only strategy where the median TVPI is above 3x for some vintage years (1992, 1993).\r\nIn 70% of observations, the mean TVPI is larger than the median. This is no coincidence, but the result of the right-skewed return distribution of PE funds. Look at the TVPI of all funds in the sample below: while most TVPIs lie between 0x to 2.5x, there are some outlier funds that perform substantially better. These exceptionally well-performing funds that return a multiple of the invested capital push up the average, while they have no meaningful impact on the median. In contrast, even the worst performing fund can only end up at 0x. This asymmetry explains the higher averages than medians.\r\n\r\n\r\n\r\nFigure 3: Histogram of PE fund TVPIs from 1990 to 2015. TVPIs over 10x not shown. Data from Preqin.\r\n\r\n\r\n\r\nThe table below shows different percentiles and the mean. It has the same message as the histogram, but sometimes it’s nice to see the numbers.\r\n\r\n\r\nTable 5: Mean and different percentiles of TVPIs for buyout, growth, and VC funds in the Preqin data set.\r\n\r\n\r\nStrategy\r\n\r\n\r\n5%\r\n\r\n\r\n10%\r\n\r\n\r\n25%\r\n\r\n\r\n50%\r\n\r\n\r\nMean\r\n\r\n\r\n75%\r\n\r\n\r\n90%\r\n\r\n\r\n95%\r\n\r\n\r\nGrowth\r\n\r\n\r\n0.69\r\n\r\n\r\n0.84\r\n\r\n\r\n1.19\r\n\r\n\r\n1.61\r\n\r\n\r\n1.80\r\n\r\n\r\n2.22\r\n\r\n\r\n2.65\r\n\r\n\r\n3.29\r\n\r\n\r\nBuyout\r\n\r\n\r\n0.74\r\n\r\n\r\n0.93\r\n\r\n\r\n1.31\r\n\r\n\r\n1.66\r\n\r\n\r\n1.72\r\n\r\n\r\n2.07\r\n\r\n\r\n2.54\r\n\r\n\r\n2.93\r\n\r\n\r\nVC\r\n\r\n\r\n0.22\r\n\r\n\r\n0.42\r\n\r\n\r\n0.79\r\n\r\n\r\n1.40\r\n\r\n\r\n1.87\r\n\r\n\r\n2.20\r\n\r\n\r\n3.49\r\n\r\n\r\n5.07\r\n\r\n\r\nInterestingly, VC has both the highest mean and lowest median TVPI. If you are investing in VC, you really want to make sure you get into those outlier funds, either by strong selection skills or by broadly diversifying.\r\n\r\n\r\n\r\nLet’s also compare the multiples with the ones reported by R. S. Harris, Jenkinson, and Kaplan (2014). I focus on vintages from 1990 to 2002 as many of their funds thereafter were mostly unrealized. I also limit my data set to only North American funds, as R. S. Harris, Jenkinson, and Kaplan (2014) focused on this region as well. Overall, the results are similar. The only exception is the average TVPIs for VC funds, which are much higher for some vintage years in the 1990s. However, these vintage years had a low number of funds overall and as I have explained above, the mean is rather sensitive to positive outliers. Their sample must have a few more of those exceptional VC funds included. Overall, the comparison gives me comfort that both samples lead to similar conclusions.\r\n\r\n\r\n\r\nFigure 4: Comparison of performance from North American funds in the Preqin data set compared with Harris et al. (2014) (see Table II).\r\n\r\n\r\n\r\nBy region\r\nAre there meaningful differences across regions? The table below gives the answer by showing the mean, median, and weighted mean for different regions for funds with vintage years 1990 to 2015. Starting with buyout funds, the performance of North American and European funds is similar, with a slight advantage of the former. Outside those regions though, the performance has been much lower.\r\nNorth American growth funds have been stronger than those in Europe and Rest of the World.\r\nFinally, the average of North American VC funds is higher than the average of their peers over the Atlantic, while the median is lower. It seems there have been some really strong VC funds in North America that drove the mean upwards. Interestingly, VC funds outside those two regions had by far the highest weighted mean, but the sample size is small.\r\n\r\n\r\nTable 6: Mean, median and weighted mean TVPIs per strategy for funds with vintage years between 1990 and 2015. Data from Preqin.\r\n\r\n\r\nGeography\r\n\r\n\r\nNr. of funds\r\n\r\n\r\nMean\r\n\r\n\r\nMedian\r\n\r\n\r\nWeighted mean\r\n\r\n\r\nBuyout\r\n\r\n\r\nEurope\r\n\r\n\r\n204\r\n\r\n\r\n1.68\r\n\r\n\r\n1.62\r\n\r\n\r\n1.72\r\n\r\n\r\nNorth America\r\n\r\n\r\n606\r\n\r\n\r\n1.78\r\n\r\n\r\n1.69\r\n\r\n\r\n1.78\r\n\r\n\r\nRest of World\r\n\r\n\r\n75\r\n\r\n\r\n1.40\r\n\r\n\r\n1.41\r\n\r\n\r\n1.42\r\n\r\n\r\nGrowth\r\n\r\n\r\nEurope\r\n\r\n\r\n29\r\n\r\n\r\n1.73\r\n\r\n\r\n1.60\r\n\r\n\r\n1.73\r\n\r\n\r\nNorth America\r\n\r\n\r\n126\r\n\r\n\r\n1.82\r\n\r\n\r\n1.73\r\n\r\n\r\n1.71\r\n\r\n\r\nRest of World\r\n\r\n\r\n64\r\n\r\n\r\n1.80\r\n\r\n\r\n1.45\r\n\r\n\r\n1.63\r\n\r\n\r\nVC\r\n\r\n\r\nEurope\r\n\r\n\r\n50\r\n\r\n\r\n1.78\r\n\r\n\r\n1.56\r\n\r\n\r\n1.69\r\n\r\n\r\nNorth America\r\n\r\n\r\n571\r\n\r\n\r\n1.89\r\n\r\n\r\n1.39\r\n\r\n\r\n1.70\r\n\r\n\r\nRest of World\r\n\r\n\r\n39\r\n\r\n\r\n1.77\r\n\r\n\r\n1.35\r\n\r\n\r\n2.13\r\n\r\n\r\nOverall, North American PE funds had a stronger performance than their international peers.\r\nOne issue with the above table is that differences in performance could simply be due to different weights of vintage years in the sample: it is fair to assume that the share of North American funds was larger at the beginning of the sample and declined over the years, when PE became more and more popular in other regions. North America might then simply have stronger performance overall as they are overweighted to some early vintages with very strong performance. Hence, let’s plot the TVPIs over time for the regions and strategies.\r\n\r\n\r\n\r\nFigure 5: Median TVPIs from 1990 to 2015 for different regions. Data from Preqin.\r\n\r\n\r\n\r\nFor buyout and growth, the chart pretty much confirms the above: North American and European buyout funds are comparable, while other buyout funds have weaker performance. North American growth funds are slightly better, but there are also some early vintages where only North America had growth funds in the sample and these vintages were very strong, implying that the difference might partially be driven by timing effects. For VC funds, I do not really see a stronger performance of North American funds overall. In some vintages, they are ahead, in others, European VC funds are. Overall, the patterns found for the whole sample also hold when looking at the performance over vintage years.\r\nHowever, the chart also shows that there are vintage years in the sample where there are only North American funds, especially in the early years. Hence, one has to be a bit careful to compare different regions as the sample construction changes fundamentally over time.\r\nInternal Rate of Return (“IRR”)\r\nLet’s look at the IRR next, again split by vintage year in the table below. Not surprisingly, the patterns are similar to the TVPI table above: vintage years with strong TVPIs tend to have strong IRRs and buyout and growth funds show less volatility than VC funds. The attentive reader might notice that the ratio of active funds is slightly different between the two samples, which is surprising as the sample universe is the same. The reason is that the IRR sometimes cannot be calculated for a fund as it is not uniquely defined, in which case this fund falls out of the sample. This is for example one explanation why the 1992 buyout cohort has a negative IRR for all statistics and a postive TVPI. There are only 8 buyout funds for this vintage year and one particularly strong one does not have an IRR and is therefore excluded. This pushes the IRRs for the cohort down.\r\n\r\n\r\n\r\n\r\n\r\nBuyout\r\n\r\n\r\n\r\n\r\nGrowth\r\n\r\n\r\n\r\n\r\nVC\r\n\r\n\r\n\r\n\r\nVintage\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean IRR\r\n\r\n\r\n\r\n\r\nMedian IRR\r\n\r\n\r\n\r\n\r\nMean IRR (weighted by size)\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean IRR\r\n\r\n\r\n\r\n\r\nMedian IRR\r\n\r\n\r\n\r\n\r\nMean IRR (weighted by size)\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean IRR\r\n\r\n\r\n\r\n\r\nMedian IRR\r\n\r\n\r\n\r\n\r\nMean IRR (weighted by size)\r\n\r\n\r\n\r\nTable 7: Mean, median, and weighted mean IRRs by strategy and vintage year in the Preqin data set. The weighting is based on fund size. As some funds do not have a fund size reported in the data set, the sample size of the weighted mean can be lower than for the mean and median. The ratio of active funds is based on the reported status in Preqin.\r\n\r\n\r\n1990\r\n\r\n\r\n0%\r\n\r\n\r\n10.50\r\n\r\n\r\n7.12\r\n\r\n\r\n11.75\r\n\r\n\r\n0%\r\n\r\n\r\n16.58\r\n\r\n\r\n16.58\r\n\r\n\r\n15.13\r\n\r\n\r\n0%\r\n\r\n\r\n17.01\r\n\r\n\r\n13.55\r\n\r\n\r\n17.66\r\n\r\n\r\n1991\r\n\r\n\r\n0%\r\n\r\n\r\n34.05\r\n\r\n\r\n34.05\r\n\r\n\r\n34.05\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n0%\r\n\r\n\r\n15.61\r\n\r\n\r\n14.39\r\n\r\n\r\n27.73\r\n\r\n\r\n1992\r\n\r\n\r\n0%\r\n\r\n\r\n-2.08\r\n\r\n\r\n-0.67\r\n\r\n\r\n-3.95\r\n\r\n\r\n0%\r\n\r\n\r\n26.43\r\n\r\n\r\n26.43\r\n\r\n\r\n26.43\r\n\r\n\r\n0%\r\n\r\n\r\n35.31\r\n\r\n\r\n34.84\r\n\r\n\r\n43.31\r\n\r\n\r\n1993\r\n\r\n\r\n0%\r\n\r\n\r\n28.09\r\n\r\n\r\n20.73\r\n\r\n\r\n30.34\r\n\r\n\r\n0%\r\n\r\n\r\n17.41\r\n\r\n\r\n17.41\r\n\r\n\r\n17.02\r\n\r\n\r\n0%\r\n\r\n\r\n36.86\r\n\r\n\r\n32.84\r\n\r\n\r\n43.06\r\n\r\n\r\n1994\r\n\r\n\r\n0%\r\n\r\n\r\n20.51\r\n\r\n\r\n18.47\r\n\r\n\r\n22.43\r\n\r\n\r\n0%\r\n\r\n\r\n15.55\r\n\r\n\r\n15.55\r\n\r\n\r\n18.40\r\n\r\n\r\n0%\r\n\r\n\r\n32.87\r\n\r\n\r\n20.75\r\n\r\n\r\n31.50\r\n\r\n\r\n1995\r\n\r\n\r\n0%\r\n\r\n\r\n10.24\r\n\r\n\r\n8.03\r\n\r\n\r\n11.24\r\n\r\n\r\n0%\r\n\r\n\r\n13.73\r\n\r\n\r\n21.04\r\n\r\n\r\n22.41\r\n\r\n\r\n0%\r\n\r\n\r\n46.68\r\n\r\n\r\n33.74\r\n\r\n\r\n56.52\r\n\r\n\r\n1996\r\n\r\n\r\n0%\r\n\r\n\r\n10.03\r\n\r\n\r\n11.64\r\n\r\n\r\n13.40\r\n\r\n\r\n0%\r\n\r\n\r\n7.40\r\n\r\n\r\n6.63\r\n\r\n\r\n6.49\r\n\r\n\r\n5%\r\n\r\n\r\n34.25\r\n\r\n\r\n11.35\r\n\r\n\r\n39.67\r\n\r\n\r\n1997\r\n\r\n\r\n4%\r\n\r\n\r\n7.11\r\n\r\n\r\n7.47\r\n\r\n\r\n10.12\r\n\r\n\r\n0%\r\n\r\n\r\n10.97\r\n\r\n\r\n7.63\r\n\r\n\r\n11.59\r\n\r\n\r\n0%\r\n\r\n\r\n30.22\r\n\r\n\r\n9.66\r\n\r\n\r\n31.33\r\n\r\n\r\n1998\r\n\r\n\r\n6%\r\n\r\n\r\n5.65\r\n\r\n\r\n5.62\r\n\r\n\r\n5.85\r\n\r\n\r\n0%\r\n\r\n\r\n8.88\r\n\r\n\r\n10.34\r\n\r\n\r\n9.61\r\n\r\n\r\n19%\r\n\r\n\r\n21.71\r\n\r\n\r\n0.70\r\n\r\n\r\n23.98\r\n\r\n\r\n1999\r\n\r\n\r\n12%\r\n\r\n\r\n7.78\r\n\r\n\r\n10.57\r\n\r\n\r\n8.91\r\n\r\n\r\n0%\r\n\r\n\r\n6.24\r\n\r\n\r\n7.60\r\n\r\n\r\n5.31\r\n\r\n\r\n17%\r\n\r\n\r\n-3.09\r\n\r\n\r\n-6.59\r\n\r\n\r\n-5.86\r\n\r\n\r\n2000\r\n\r\n\r\n17%\r\n\r\n\r\n15.52\r\n\r\n\r\n13.41\r\n\r\n\r\n15.85\r\n\r\n\r\n0%\r\n\r\n\r\n6.84\r\n\r\n\r\n8.43\r\n\r\n\r\n6.25\r\n\r\n\r\n18%\r\n\r\n\r\n-4.16\r\n\r\n\r\n-1.79\r\n\r\n\r\n-2.02\r\n\r\n\r\n2001\r\n\r\n\r\n37%\r\n\r\n\r\n23.52\r\n\r\n\r\n19.26\r\n\r\n\r\n27.53\r\n\r\n\r\n14%\r\n\r\n\r\n18.43\r\n\r\n\r\n16.28\r\n\r\n\r\n18.85\r\n\r\n\r\n41%\r\n\r\n\r\n0.77\r\n\r\n\r\n0.69\r\n\r\n\r\n3.01\r\n\r\n\r\n2002\r\n\r\n\r\n12%\r\n\r\n\r\n19.26\r\n\r\n\r\n18.82\r\n\r\n\r\n25.76\r\n\r\n\r\n33%\r\n\r\n\r\n3.78\r\n\r\n\r\n7.69\r\n\r\n\r\n5.95\r\n\r\n\r\n35%\r\n\r\n\r\n-5.97\r\n\r\n\r\n-2.90\r\n\r\n\r\n-0.31\r\n\r\n\r\n2003\r\n\r\n\r\n38%\r\n\r\n\r\n26.39\r\n\r\n\r\n17.63\r\n\r\n\r\n25.14\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n41%\r\n\r\n\r\n0.70\r\n\r\n\r\n3.15\r\n\r\n\r\n0.86\r\n\r\n\r\n2004\r\n\r\n\r\n22%\r\n\r\n\r\n9.22\r\n\r\n\r\n11.70\r\n\r\n\r\n13.57\r\n\r\n\r\n25%\r\n\r\n\r\n13.30\r\n\r\n\r\n10.74\r\n\r\n\r\n23.23\r\n\r\n\r\n48%\r\n\r\n\r\n-2.82\r\n\r\n\r\n1.17\r\n\r\n\r\n-0.64\r\n\r\n\r\n2005\r\n\r\n\r\n53%\r\n\r\n\r\n9.47\r\n\r\n\r\n8.37\r\n\r\n\r\n11.26\r\n\r\n\r\n38%\r\n\r\n\r\n11.15\r\n\r\n\r\n12.49\r\n\r\n\r\n10.82\r\n\r\n\r\n85%\r\n\r\n\r\n-0.09\r\n\r\n\r\n4.52\r\n\r\n\r\n1.45\r\n\r\n\r\n2006\r\n\r\n\r\n59%\r\n\r\n\r\n7.88\r\n\r\n\r\n8.29\r\n\r\n\r\n6.95\r\n\r\n\r\n57%\r\n\r\n\r\n13.60\r\n\r\n\r\n11.51\r\n\r\n\r\n1.85\r\n\r\n\r\n72%\r\n\r\n\r\n0.95\r\n\r\n\r\n3.57\r\n\r\n\r\n3.10\r\n\r\n\r\n2007\r\n\r\n\r\n84%\r\n\r\n\r\n8.96\r\n\r\n\r\n9.14\r\n\r\n\r\n8.82\r\n\r\n\r\n57%\r\n\r\n\r\n8.70\r\n\r\n\r\n8.96\r\n\r\n\r\n8.62\r\n\r\n\r\n91%\r\n\r\n\r\n11.28\r\n\r\n\r\n12.08\r\n\r\n\r\n12.81\r\n\r\n\r\n2008\r\n\r\n\r\n79%\r\n\r\n\r\n11.89\r\n\r\n\r\n12.46\r\n\r\n\r\n12.39\r\n\r\n\r\n77%\r\n\r\n\r\n5.64\r\n\r\n\r\n4.16\r\n\r\n\r\n-1.79\r\n\r\n\r\n76%\r\n\r\n\r\n6.13\r\n\r\n\r\n6.81\r\n\r\n\r\n3.05\r\n\r\n\r\n2009\r\n\r\n\r\n94%\r\n\r\n\r\n15.32\r\n\r\n\r\n14.21\r\n\r\n\r\n16.20\r\n\r\n\r\n88%\r\n\r\n\r\n7.61\r\n\r\n\r\n5.31\r\n\r\n\r\n10.02\r\n\r\n\r\n94%\r\n\r\n\r\n11.75\r\n\r\n\r\n11.43\r\n\r\n\r\n10.95\r\n\r\n\r\n2010\r\n\r\n\r\n74%\r\n\r\n\r\n12.57\r\n\r\n\r\n15.84\r\n\r\n\r\n12.25\r\n\r\n\r\n100%\r\n\r\n\r\n11.21\r\n\r\n\r\n8.10\r\n\r\n\r\n10.55\r\n\r\n\r\n93%\r\n\r\n\r\n11.25\r\n\r\n\r\n11.90\r\n\r\n\r\n12.60\r\n\r\n\r\n2011\r\n\r\n\r\n100%\r\n\r\n\r\n11.94\r\n\r\n\r\n13.42\r\n\r\n\r\n14.44\r\n\r\n\r\n93%\r\n\r\n\r\n18.88\r\n\r\n\r\n19.62\r\n\r\n\r\n16.83\r\n\r\n\r\n100%\r\n\r\n\r\n18.13\r\n\r\n\r\n17.63\r\n\r\n\r\n18.11\r\n\r\n\r\n2012\r\n\r\n\r\n93%\r\n\r\n\r\n15.75\r\n\r\n\r\n17.41\r\n\r\n\r\n17.87\r\n\r\n\r\n93%\r\n\r\n\r\n10.58\r\n\r\n\r\n9.74\r\n\r\n\r\n10.67\r\n\r\n\r\n100%\r\n\r\n\r\n16.43\r\n\r\n\r\n16.81\r\n\r\n\r\n20.59\r\n\r\n\r\n2013\r\n\r\n\r\n96%\r\n\r\n\r\n17.50\r\n\r\n\r\n16.05\r\n\r\n\r\n16.87\r\n\r\n\r\n80%\r\n\r\n\r\n9.93\r\n\r\n\r\n11.55\r\n\r\n\r\n10.01\r\n\r\n\r\n100%\r\n\r\n\r\n16.85\r\n\r\n\r\n13.97\r\n\r\n\r\n21.16\r\n\r\n\r\n2014\r\n\r\n\r\n100%\r\n\r\n\r\n19.98\r\n\r\n\r\n17.20\r\n\r\n\r\n18.36\r\n\r\n\r\n100%\r\n\r\n\r\n16.72\r\n\r\n\r\n17.40\r\n\r\n\r\n14.15\r\n\r\n\r\n97%\r\n\r\n\r\n32.13\r\n\r\n\r\n22.35\r\n\r\n\r\n36.10\r\n\r\n\r\n2015\r\n\r\n\r\n100%\r\n\r\n\r\n19.21\r\n\r\n\r\n17.64\r\n\r\n\r\n20.92\r\n\r\n\r\n100%\r\n\r\n\r\n15.78\r\n\r\n\r\n15.49\r\n\r\n\r\n16.10\r\n\r\n\r\n100%\r\n\r\n\r\n20.17\r\n\r\n\r\n18.21\r\n\r\n\r\n20.37\r\n\r\n\r\n2016\r\n\r\n\r\n99%\r\n\r\n\r\n16.62\r\n\r\n\r\n15.77\r\n\r\n\r\n18.96\r\n\r\n\r\n100%\r\n\r\n\r\n15.48\r\n\r\n\r\n15.50\r\n\r\n\r\n24.21\r\n\r\n\r\n100%\r\n\r\n\r\n26.83\r\n\r\n\r\n27.86\r\n\r\n\r\n26.58\r\n\r\n\r\n2017\r\n\r\n\r\n100%\r\n\r\n\r\n22.79\r\n\r\n\r\n21.86\r\n\r\n\r\n26.03\r\n\r\n\r\n100%\r\n\r\n\r\n25.40\r\n\r\n\r\n20.13\r\n\r\n\r\n35.30\r\n\r\n\r\n100%\r\n\r\n\r\n25.50\r\n\r\n\r\n22.04\r\n\r\n\r\n29.15\r\n\r\n\r\n2018\r\n\r\n\r\n100%\r\n\r\n\r\n14.62\r\n\r\n\r\n15.92\r\n\r\n\r\n25.57\r\n\r\n\r\n100%\r\n\r\n\r\n23.94\r\n\r\n\r\n20.27\r\n\r\n\r\n37.98\r\n\r\n\r\n100%\r\n\r\n\r\n28.54\r\n\r\n\r\n22.69\r\n\r\n\r\n32.94\r\n\r\n\r\n2019\r\n\r\n\r\n100%\r\n\r\n\r\n20.84\r\n\r\n\r\n11.77\r\n\r\n\r\n27.08\r\n\r\n\r\n100%\r\n\r\n\r\n18.37\r\n\r\n\r\n5.78\r\n\r\n\r\n27.64\r\n\r\n\r\n100%\r\n\r\n\r\n33.10\r\n\r\n\r\n20.86\r\n\r\n\r\n36.34\r\n\r\n\r\nNext, let’s look at the distribution of IRRs over the full sample. Comparing this one with the table for TVPIs, there is one crucial difference: average IRRs are considerably higher for buyout funds than for the other two strategies. As the IRR is a performance measure that considers the timing of the cashflows, while the TVPI does not, the culprit for the difference is clear: buyout funds return capital back faster to the investors than growth and VC funds. I will show this in more detail in the DPI section.\r\n\r\n\r\nTable 8: Mean and different percentiles of IRRs for buyout, growth, and VC funds in the Preqin data set.\r\n\r\n\r\nStrategy\r\n\r\n\r\n5%\r\n\r\n\r\n10%\r\n\r\n\r\n25%\r\n\r\n\r\n50%\r\n\r\n\r\nMean\r\n\r\n\r\n75%\r\n\r\n\r\n90%\r\n\r\n\r\n95%\r\n\r\n\r\nGrowth\r\n\r\n\r\n-5.98\r\n\r\n\r\n-2.53\r\n\r\n\r\n4.16\r\n\r\n\r\n11.05\r\n\r\n\r\n12.00\r\n\r\n\r\n19.04\r\n\r\n\r\n26.25\r\n\r\n\r\n32.33\r\n\r\n\r\nBuyout\r\n\r\n\r\n-5.72\r\n\r\n\r\n-1.24\r\n\r\n\r\n6.51\r\n\r\n\r\n12.90\r\n\r\n\r\n13.45\r\n\r\n\r\n20.23\r\n\r\n\r\n28.24\r\n\r\n\r\n35.56\r\n\r\n\r\nVC\r\n\r\n\r\n-20.45\r\n\r\n\r\n-12.72\r\n\r\n\r\n-3.05\r\n\r\n\r\n6.07\r\n\r\n\r\n10.88\r\n\r\n\r\n18.05\r\n\r\n\r\n32.97\r\n\r\n\r\n50.52\r\n\r\n\r\nFinally, I compare the IRRs in my North American sample with the ones shown in R. S. Harris, Jenkinson, and Kaplan (2014). The correlation between the two is very high.\r\n\r\n\r\n\r\nFigure 6: Comparison of IRRs from North American funds in the Preqin data set compared with Harris et al. (2014) (see Table II).\r\n\r\n\r\n\r\nDistributed to Paid-In (“DPI”)\r\nAs the previous sections have shown, buyout funds perform better compared to the other strategies when performance is measured with the IRR instead of the TVPI. As the former takes the timing of cash flows into consideration, while the latter does not, the reason must be that buyout funds return money to investors quicker. If we look at the J-curves for the three different strategies, this is exactly what we find. A typical buyout fund, shown as the black line in the middle of the violet area, has a DPI above 1x around 8 years. For a growth fund, it takes a bit longer, while a typical VC fund struggles to make it to 1x within a 15-year window!\r\n\r\n\r\n\r\nFigure 7: Dispersion of J-curves for three main strategies. Only funds with vintage year 1990 or younger considered that are at least 15 years old. Data from Preqin.\r\n\r\n\r\n\r\nOf course, for VC funds there are two effects at work: First, it simply takes longer till liquidity events materialize. Second, a typical VC funds has a lower performance than a typical buyout or growth fund and therefore it takes longer to get to a DPI of 1x. Simple example: if it takes two funds ten years to be fully realized and one fund ends up with a final return of 1x and one with 3x, the former one will need the ten years to pay back the invested capital, while the latter one will have mostly likely returned the capital much earlier.\r\nPublic Market Equivalent (“PME”)\r\nLast but not least, let’s get to the PMEs. This measure discounts the cash flows of a PE fund with a discount rate, often taken as a broad public market index. If it is above 1x, an investor was better off investing in the fund; if it is below 1x, he should have rather invested in the index. Conceptually, this measure is the most appealing one as it considers the opportunity costs of capital. It sounds good that you made 2x money investing in a fund, but what if you would have made 3x investing in the stock market instead?\r\nSorensen and Jagannathan (2015) is a nice paper that shows that the PME method can be justified on theoretical grounds as a measure of PE performance.\r\nOne key question is what market indices to use. One could adjust for all sorts of factors, but to keep things simple I use broad market indices obtained from Kenneth French’s website. Later, I also use the NASDAQ index as a benchmark, as it had much stronger returns.\r\nThe table below shows that PE funds have consistently outperformed broad public stock markets. Not surprisingly, a few VC vintages ended up well below 1x, but otherwise it’s hard to find a PME starting with 0 in the table. This is in line with a large literature on PE performance that shows this outperformance.5\r\n\r\n\r\n\r\n\r\n\r\nBuyout\r\n\r\n\r\n\r\n\r\nGrowth\r\n\r\n\r\n\r\n\r\nVC\r\n\r\n\r\n\r\n\r\nVintage\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean PME\r\n\r\n\r\n\r\n\r\nMedian PME\r\n\r\n\r\n\r\n\r\nMean PME (weighted by size)\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean PME\r\n\r\n\r\n\r\n\r\nMedian PME\r\n\r\n\r\n\r\n\r\nMean PME (weighted by size)\r\n\r\n\r\n\r\n\r\nRatio active funds\r\n\r\n\r\n\r\n\r\nMean PME\r\n\r\n\r\n\r\n\r\nMedian PME\r\n\r\n\r\n\r\n\r\nMean PME (weighted by size)\r\n\r\n\r\n\r\nTable 9: Mean, median, and weighted mean PMEs by strategy and vintage year in the Preqin data set. The weighting is based on fund size. As some funds do not have a fund size reported in the data set, the sample size of the weighted mean can be lower than for the mean and median. The ratio of active funds is based on the reported status in Preqin. Only North American and European funds considered. Public market returns taken from Kenneth French’s website.\r\n\r\n\r\n1990\r\n\r\n\r\n0%\r\n\r\n\r\n1.15\r\n\r\n\r\n1.16\r\n\r\n\r\n1.50\r\n\r\n\r\n0%\r\n\r\n\r\n1.11\r\n\r\n\r\n1.11\r\n\r\n\r\n1.02\r\n\r\n\r\n0%\r\n\r\n\r\n1.21\r\n\r\n\r\n1.26\r\n\r\n\r\n1.25\r\n\r\n\r\n1991\r\n\r\n\r\n0%\r\n\r\n\r\n1.27\r\n\r\n\r\n1.27\r\n\r\n\r\n1.63\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n0%\r\n\r\n\r\n1.47\r\n\r\n\r\n1.47\r\n\r\n\r\n1.75\r\n\r\n\r\n1992\r\n\r\n\r\n0%\r\n\r\n\r\n0.87\r\n\r\n\r\n0.77\r\n\r\n\r\n1.06\r\n\r\n\r\n0%\r\n\r\n\r\n1.47\r\n\r\n\r\n1.47\r\n\r\n\r\n1.47\r\n\r\n\r\n0%\r\n\r\n\r\n1.95\r\n\r\n\r\n1.74\r\n\r\n\r\n2.00\r\n\r\n\r\n1993\r\n\r\n\r\n0%\r\n\r\n\r\n1.48\r\n\r\n\r\n1.30\r\n\r\n\r\n1.51\r\n\r\n\r\n0%\r\n\r\n\r\n1.25\r\n\r\n\r\n1.25\r\n\r\n\r\n1.25\r\n\r\n\r\n0%\r\n\r\n\r\n2.16\r\n\r\n\r\n1.81\r\n\r\n\r\n2.45\r\n\r\n\r\n1994\r\n\r\n\r\n0%\r\n\r\n\r\n1.41\r\n\r\n\r\n1.31\r\n\r\n\r\n1.45\r\n\r\n\r\n0%\r\n\r\n\r\n1.19\r\n\r\n\r\n1.19\r\n\r\n\r\n1.34\r\n\r\n\r\n0%\r\n\r\n\r\n2.25\r\n\r\n\r\n1.72\r\n\r\n\r\n2.29\r\n\r\n\r\n1995\r\n\r\n\r\n0%\r\n\r\n\r\n1.14\r\n\r\n\r\n1.01\r\n\r\n\r\n1.23\r\n\r\n\r\n0%\r\n\r\n\r\n1.36\r\n\r\n\r\n1.60\r\n\r\n\r\n1.69\r\n\r\n\r\n0%\r\n\r\n\r\n2.39\r\n\r\n\r\n1.72\r\n\r\n\r\n2.76\r\n\r\n\r\n1996\r\n\r\n\r\n0%\r\n\r\n\r\n1.29\r\n\r\n\r\n1.50\r\n\r\n\r\n1.39\r\n\r\n\r\n0%\r\n\r\n\r\n1.42\r\n\r\n\r\n1.42\r\n\r\n\r\n1.33\r\n\r\n\r\n5%\r\n\r\n\r\n2.37\r\n\r\n\r\n1.28\r\n\r\n\r\n2.56\r\n\r\n\r\n1997\r\n\r\n\r\n5%\r\n\r\n\r\n1.38\r\n\r\n\r\n1.48\r\n\r\n\r\n1.49\r\n\r\n\r\n0%\r\n\r\n\r\n1.18\r\n\r\n\r\n1.07\r\n\r\n\r\n1.24\r\n\r\n\r\n0%\r\n\r\n\r\n1.59\r\n\r\n\r\n1.06\r\n\r\n\r\n1.70\r\n\r\n\r\n1998\r\n\r\n\r\n5%\r\n\r\n\r\n1.36\r\n\r\n\r\n1.22\r\n\r\n\r\n1.34\r\n\r\n\r\n0%\r\n\r\n\r\n1.28\r\n\r\n\r\n1.15\r\n\r\n\r\n1.57\r\n\r\n\r\n19%\r\n\r\n\r\n1.62\r\n\r\n\r\n1.04\r\n\r\n\r\n1.60\r\n\r\n\r\n1999\r\n\r\n\r\n11%\r\n\r\n\r\n1.37\r\n\r\n\r\n1.41\r\n\r\n\r\n1.33\r\n\r\n\r\n0%\r\n\r\n\r\n1.50\r\n\r\n\r\n0.96\r\n\r\n\r\n1.09\r\n\r\n\r\n17%\r\n\r\n\r\n0.70\r\n\r\n\r\n0.59\r\n\r\n\r\n0.66\r\n\r\n\r\n2000\r\n\r\n\r\n15%\r\n\r\n\r\n1.55\r\n\r\n\r\n1.51\r\n\r\n\r\n1.61\r\n\r\n\r\n0%\r\n\r\n\r\n1.17\r\n\r\n\r\n1.16\r\n\r\n\r\n1.19\r\n\r\n\r\n19%\r\n\r\n\r\n0.68\r\n\r\n\r\n0.65\r\n\r\n\r\n0.70\r\n\r\n\r\n2001\r\n\r\n\r\n32%\r\n\r\n\r\n1.40\r\n\r\n\r\n1.44\r\n\r\n\r\n1.50\r\n\r\n\r\n17%\r\n\r\n\r\n1.53\r\n\r\n\r\n1.47\r\n\r\n\r\n1.62\r\n\r\n\r\n40%\r\n\r\n\r\n0.82\r\n\r\n\r\n0.72\r\n\r\n\r\n0.88\r\n\r\n\r\n2002\r\n\r\n\r\n12%\r\n\r\n\r\n1.34\r\n\r\n\r\n1.38\r\n\r\n\r\n1.46\r\n\r\n\r\n33%\r\n\r\n\r\n0.92\r\n\r\n\r\n1.02\r\n\r\n\r\n0.94\r\n\r\n\r\n33%\r\n\r\n\r\n0.68\r\n\r\n\r\n0.64\r\n\r\n\r\n0.78\r\n\r\n\r\n2003\r\n\r\n\r\n35%\r\n\r\n\r\n1.55\r\n\r\n\r\n1.39\r\n\r\n\r\n1.62\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n47%\r\n\r\n\r\n0.78\r\n\r\n\r\n0.78\r\n\r\n\r\n0.79\r\n\r\n\r\n2004\r\n\r\n\r\n24%\r\n\r\n\r\n1.41\r\n\r\n\r\n1.35\r\n\r\n\r\n1.46\r\n\r\n\r\n25%\r\n\r\n\r\n1.25\r\n\r\n\r\n1.27\r\n\r\n\r\n1.47\r\n\r\n\r\n41%\r\n\r\n\r\n0.85\r\n\r\n\r\n0.42\r\n\r\n\r\n0.73\r\n\r\n\r\n2005\r\n\r\n\r\n55%\r\n\r\n\r\n1.24\r\n\r\n\r\n1.16\r\n\r\n\r\n1.39\r\n\r\n\r\n17%\r\n\r\n\r\n1.63\r\n\r\n\r\n1.43\r\n\r\n\r\n1.40\r\n\r\n\r\n88%\r\n\r\n\r\n0.97\r\n\r\n\r\n0.78\r\n\r\n\r\n1.05\r\n\r\n\r\n2006\r\n\r\n\r\n56%\r\n\r\n\r\n1.10\r\n\r\n\r\n1.07\r\n\r\n\r\n1.04\r\n\r\n\r\n70%\r\n\r\n\r\n1.08\r\n\r\n\r\n0.96\r\n\r\n\r\n0.76\r\n\r\n\r\n69%\r\n\r\n\r\n0.68\r\n\r\n\r\n0.71\r\n\r\n\r\n0.71\r\n\r\n\r\n2007\r\n\r\n\r\n80%\r\n\r\n\r\n1.02\r\n\r\n\r\n1.04\r\n\r\n\r\n1.10\r\n\r\n\r\n50%\r\n\r\n\r\n1.07\r\n\r\n\r\n1.11\r\n\r\n\r\n0.90\r\n\r\n\r\n93%\r\n\r\n\r\n1.22\r\n\r\n\r\n1.11\r\n\r\n\r\n1.25\r\n\r\n\r\n2008\r\n\r\n\r\n77%\r\n\r\n\r\n1.09\r\n\r\n\r\n1.06\r\n\r\n\r\n1.15\r\n\r\n\r\n67%\r\n\r\n\r\n0.68\r\n\r\n\r\n0.51\r\n\r\n\r\n0.61\r\n\r\n\r\n72%\r\n\r\n\r\n1.05\r\n\r\n\r\n0.74\r\n\r\n\r\n0.84\r\n\r\n\r\n2009\r\n\r\n\r\n93%\r\n\r\n\r\n1.16\r\n\r\n\r\n1.17\r\n\r\n\r\n1.20\r\n\r\n\r\n80%\r\n\r\n\r\n0.91\r\n\r\n\r\n0.76\r\n\r\n\r\n1.08\r\n\r\n\r\n93%\r\n\r\n\r\n0.93\r\n\r\n\r\n0.91\r\n\r\n\r\n0.90\r\n\r\n\r\n2010\r\n\r\n\r\n75%\r\n\r\n\r\n1.14\r\n\r\n\r\n1.19\r\n\r\n\r\n1.11\r\n\r\n\r\n100%\r\n\r\n\r\n1.18\r\n\r\n\r\n1.22\r\n\r\n\r\n1.21\r\n\r\n\r\n93%\r\n\r\n\r\n1.02\r\n\r\n\r\n0.82\r\n\r\n\r\n0.99\r\n\r\n\r\n2011\r\n\r\n\r\n100%\r\n\r\n\r\n1.21\r\n\r\n\r\n1.24\r\n\r\n\r\n1.25\r\n\r\n\r\n89%\r\n\r\n\r\n1.42\r\n\r\n\r\n1.48\r\n\r\n\r\n1.35\r\n\r\n\r\n95%\r\n\r\n\r\n1.52\r\n\r\n\r\n1.19\r\n\r\n\r\n1.44\r\n\r\n\r\n2012\r\n\r\n\r\n90%\r\n\r\n\r\n1.21\r\n\r\n\r\n1.19\r\n\r\n\r\n1.29\r\n\r\n\r\n100%\r\n\r\n\r\n1.11\r\n\r\n\r\n0.97\r\n\r\n\r\n0.99\r\n\r\n\r\n100%\r\n\r\n\r\n1.53\r\n\r\n\r\n1.26\r\n\r\n\r\n1.48\r\n\r\n\r\n2013\r\n\r\n\r\n95%\r\n\r\n\r\n1.32\r\n\r\n\r\n1.26\r\n\r\n\r\n1.33\r\n\r\n\r\n100%\r\n\r\n\r\n0.99\r\n\r\n\r\n0.92\r\n\r\n\r\n1.06\r\n\r\n\r\n100%\r\n\r\n\r\n1.23\r\n\r\n\r\n1.14\r\n\r\n\r\n1.32\r\n\r\n\r\n2014\r\n\r\n\r\n100%\r\n\r\n\r\n1.34\r\n\r\n\r\n1.20\r\n\r\n\r\n1.31\r\n\r\n\r\n100%\r\n\r\n\r\n1.33\r\n\r\n\r\n1.37\r\n\r\n\r\n1.17\r\n\r\n\r\n97%\r\n\r\n\r\n1.49\r\n\r\n\r\n1.44\r\n\r\n\r\n1.48\r\n\r\n\r\n2015\r\n\r\n\r\n100%\r\n\r\n\r\n1.22\r\n\r\n\r\n1.14\r\n\r\n\r\n1.22\r\n\r\n\r\n100%\r\n\r\n\r\n1.19\r\n\r\n\r\n1.15\r\n\r\n\r\n1.17\r\n\r\n\r\n100%\r\n\r\n\r\n1.27\r\n\r\n\r\n1.15\r\n\r\n\r\n1.21\r\n\r\n\r\n2016\r\n\r\n\r\n99%\r\n\r\n\r\n1.13\r\n\r\n\r\n1.03\r\n\r\n\r\n1.14\r\n\r\n\r\n100%\r\n\r\n\r\n1.23\r\n\r\n\r\n1.18\r\n\r\n\r\n1.20\r\n\r\n\r\n100%\r\n\r\n\r\n1.42\r\n\r\n\r\n1.30\r\n\r\n\r\n1.34\r\n\r\n\r\n2017\r\n\r\n\r\n100%\r\n\r\n\r\n1.16\r\n\r\n\r\n1.08\r\n\r\n\r\n1.24\r\n\r\n\r\n100%\r\n\r\n\r\n1.20\r\n\r\n\r\n1.15\r\n\r\n\r\n1.33\r\n\r\n\r\n100%\r\n\r\n\r\n1.27\r\n\r\n\r\n1.15\r\n\r\n\r\n1.25\r\n\r\n\r\n2018\r\n\r\n\r\n100%\r\n\r\n\r\n0.96\r\n\r\n\r\n0.93\r\n\r\n\r\n1.04\r\n\r\n\r\n100%\r\n\r\n\r\n1.10\r\n\r\n\r\n1.06\r\n\r\n\r\n1.25\r\n\r\n\r\n100%\r\n\r\n\r\n1.17\r\n\r\n\r\n1.04\r\n\r\n\r\n1.21\r\n\r\n\r\n2019\r\n\r\n\r\n100%\r\n\r\n\r\n0.98\r\n\r\n\r\n0.89\r\n\r\n\r\n1.00\r\n\r\n\r\n100%\r\n\r\n\r\n0.99\r\n\r\n\r\n1.09\r\n\r\n\r\n1.02\r\n\r\n\r\n100%\r\n\r\n\r\n1.09\r\n\r\n\r\n0.94\r\n\r\n\r\n1.09\r\n\r\n\r\nLooking at North American funds only, we can also compare the results again with those reported by R. S. Harris, Jenkinson, and Kaplan (2014). Overall, the results are comparable. Again, we have a few outlier VC years in their sample that drive the mean higher for them, but otherwise both the level and patterns over time are similar.\r\n\r\n\r\n\r\nFigure 8: Comparison of PMEs from North American funds in the Preqin data set compared with Harris et al. (2014) (see Table III).\r\n\r\n\r\n\r\nLet’s now compare the PMEs if we use the NASDAQ index instead as a comparison, which has performed substantially better than the broader stock market over the last decades. I also use this index to benchmark European funds with it, although one could of course argue how relevant such a benchmark is. The chart below shows the PMEs with the different benchmarks.\r\n\r\n\r\n\r\nFigure 9: Median PMEs over vintage years, split across strategies. The PMEs are calculated both with a broad North American or European stock market index, taken from Kenneth French’s website, or with the NASDAQ. Funds outside North America and Europe ignored.\r\n\r\n\r\n\r\nChanging the benchmark index to the NASDAQ reduces the vintage year median PMEs considerably for all three strategies. A typical buyout funds from 2005 to 2015 pretty much performed like the NASDAQ. Growth funds have a higher volatility, but mostly failed to beat the NASDAQ. A typical VC fund did not manage to beat the NASDAQ index. This also improves only marginally when we use the average PME instead. As explained at length above, the improvement is strongest for the VC strategy, where recent funds since 2011 have performed in line with the NASDAQ.\r\n\r\n\r\n\r\nFigure 10: Average PMEs over vintage years, split across strategies. The PMEs are calculated both with a broad North American or European stock market index, taken from Kenneth French’s website, or with the NASDAQ. Funds outside North America and Europe ignored.\r\n\r\n\r\n\r\nSummary\r\nIn this blog post, I have revisited the question of PE performance and have shown that overall this asset class continues to deliver attractive returns for its investors. The devil is in the detail though and investors should be aware at what statistic (mean, median, or something else?), vintage year composition, measure (TVPI, IRR, PME?), etc. to look at. This blog post hopefully illustrated differences arising from these choices.\r\n\r\n\r\n\r\nBrown, Gregory W, Oleg R Gredil, and Steven N Kaplan. 2019. “Do Private Equity Funds Manipulate Reported Returns?” Journal of Financial Economics 132 (2): 267–97.\r\n\r\n\r\nCavagnaro, Daniel R, Berk A Sensoy, Yingdi Wang, and Michael S Weisbach. 2019. “Measuring Institutional Investors’ Skill at Making Private Equity Investments.” The Journal of Finance 74 (6): 3089–3134.\r\n\r\n\r\nDiller, Christian, and Christoph Jäckel. 2015. “Risk in Private Equity.” BVCA Research Paper.\r\n\r\n\r\nHarris, Robert S, Tim Jenkinson, and Steven N Kaplan. 2014. “Private Equity Performance: What Do We Know?” The Journal of Finance 69 (5): 1851–82.\r\n\r\n\r\nHarris, Robert, Tim Jenkinson, and Rüdiger Stucke. 2012. “Are Too Many Private Equity Funds Top Quartile?” Journal of Applied Corporate Finance 24 (4): 77–89.\r\n\r\n\r\nHooke, Jeffrey, and Ken Yook. 2016. “The Relative Performances of Large Buyout Fund Groups.” The Journal of Private Equity 20 (1): 25–34.\r\n\r\n\r\nKaplan, Steven N, and Antoinette Schoar. 2005. “Private Equity Performance: Returns, Persistence, and Capital Flows.” The Journal of Finance 60 (4): 1791–1823.\r\n\r\n\r\nKorteweg, Arthur. 2019. “Risk Adjustment in Private Equity Returns.” Annual Review of Financial Economics 11: 131–52.\r\n\r\n\r\nKorteweg, Arthur, and Morten Sorensen. 2017. “Skill and Luck in Private Equity Performance.” Journal of Financial Economics 124 (3): 535–62.\r\n\r\n\r\nMcKinsey. 2021. “A Year of Disruption in the Private Markets.” McKinsey Global Private Markets Review 2021.\r\n\r\n\r\nPhalippou, Ludovic. 2014. “Performance of Buyout Funds Revisited?” Review of Finance 18 (1): 189–218.\r\n\r\n\r\nPhalippou, Ludovic, and Oliver Gottschalg. 2009. “The Performance of Private Equity Funds.” The Review of Financial Studies 22 (4): 1747–76.\r\n\r\n\r\nSorensen, Morten, and Ravi Jagannathan. 2015. “The Public Market Equivalent and Private Equity Performance.” Financial Analysts Journal 71 (4): 43–50.\r\n\r\n\r\nSorensen, Morten, Neng Wang, and Jinqiang Yang. 2014. “Valuing Private Equity.” The Review of Financial Studies 27 (7): 1977–2021.\r\n\r\n\r\nStucke, Rüdiger. 2011. “Updating History.” Available at SSRN 1967636.\r\n\r\n\r\nI use the terms NAV and residual value interchangeably throughout this post.↩︎\r\nIn R. Harris, Jenkinson, and Stucke (2012), he and his co-authors looks further into this issue and and also shows other reasons why more than 25% of funds can claim to be top-quartile.↩︎\r\nOf course, they had good reasons to do so when looking at their data as it was indeed very peculiar that very mature funds would report large residual values without any changes in activity. As Stucke (2011) subsequently showed, this was due to a failure of the data provider to receive updates on those funds and flag this accordingly.↩︎\r\nFor example, one could hypothesize that GPs’ likelihood to report to data providers changes over time, so instead of observing trends in the overall PE market, we are looking at a trend of GPs’ willingness to report. However, this is unlikely for two reasons. First, the trends resonate well with the overall market cycle. It just seems logical that the number of VC funds exploded in 2000, rather than GP’s just willing to report those funds. Second, the data providers use different means for data collection - Burgiss, for example, relies on LPs, not GPs.↩︎\r\nNote that I use PE data that is aggregated on a quarterly basis. A more correct approach would be to use the unaggregated data that shows the cash flows on a daily basis. However, as my comparison with the results from R. S. Harris, Jenkinson, and Kaplan (2014) shows, it seems that this simplification has an overall negligible effect on the results.↩︎\r\n",
    "preview": "posts/2021-04-24-the-performance-of-private-equity/the-performance-of-private-equity_files/figure-html5/NumberOfFunds-1.png",
    "last_modified": "2022-02-18T08:13:49+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-28-value-creation-of-a-private-equity-fund-over-time/",
    "title": "Value creation of a private equity fund over time",
    "description": "When does a private equity fund generate most of its value?",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-12-08",
    "categories": [
      "Private Equity",
      "Data",
      "Research"
    ],
    "contents": "\r\nOverall, private equity funds generate attractive returns over the course of their lives.1 In this blog post, I’m looking further into the question when this value creation happens? This is an important question to ask, especially in my line of work as a secondary buyer: as returns can only come from the price a buyer pays for a fund or further value creation, knowing at what age of a fund the value creation is most likely to happen helps a buyer to better price fund interests.\r\nIn the following, I use cash flow and NAV data of private equity funds from Preqin. I focus on buyout funds and exclude all funds with a vintage year younger than 2010 as such funds still have meaningful value creation ahead of them.\r\nTo start with, I calculate the annual return in year \\(t\\) for a fund \\(i\\) as follows:\r\n\\[ Ret_{i,t} = \\frac{NAV_{i,t} + Dist_{i, t}}{NAV_{i,t-1} + CC_{i,t}} - 1, \\]\r\nwhere \\(NAV_{i,t}\\) is the reported NAV of fund \\(i\\) at the end of period \\(t\\), and \\(CC_{i,t}\\) and \\(Dist_{i,t}\\) are the capital calls and distributions from and to the investors by the fund from \\(t-1\\) to \\(t\\).\r\n\r\n\r\n\r\nThe boxplot chart below shows the dispersion of annual returns over the first ten years of a buyout fund.2 In the first three years, a typical buyout fund does not report positive returns. As a matter of fact, most funds have a negative return in their first year when not many investments have been made yet, these that have been made are valued at cost, and the fund already charges management fees and asks investors to pay for the establishment expenses.\r\n\r\n\r\n\r\nFigure 1: Dispersion of annual returns of buyout funds over the first 10 years of their lifetime. The lower and upper hinges correspond to the first and third quartiles of the annual returns, the line in the middle to the median. Data from Preqin.\r\n\r\n\r\n\r\nStarting with year 4, funds start reporting positive returns with consistently strong years from year 5 to 8 where the median fund reports a return of 10% p.a. and more than one quarter of funds having returns of 10% p.a. or higher. This is the most important phase of value creation, often called the “harvesting phase” for a private equity fund. In this phase, the fund is out of the investment period, the fund managers are working on their operational initiatives and preparing the underlying investments for their exits. In year 9, returns come down meaningfully, although they continue to be positive for most of the funds. At this age of the fund, the strongest performing investments have already been sold successfully and the remaining ones have an issue in one way or the other. These funds are often described as “tail-end.”\r\nNadauld et al. (2019) report in their paper that secondary buyers pay the highest prices, in relation to the NAV, for funds that are four to nine years old. This resonates well with the results shown here: such funds are the most attractive, as they still have lots of value creation ahead of them, while most of the fee leakage has already been paid as it is front-loaded by private equity funds. For tail-end funds, secondary buyers require a larger discount as they cannot expect their returns to come from further value creation of the funds.\r\nSo far, I have looked at the returns independently of the current state of the economy. An interesting follow-up question is if the value creation pattern described above changes in case of recessions. One hypothesis could be that more mature funds are not so much impacted anymore by the overall economy, as a larger share of the remaining value is non-investment related items such as cash, escrows, carry provisions, etc. To test this hypothesis I re-plot the boxplot for two particularly bad years of private equity, 2002 and 2009.\r\n\r\n\r\n\r\nFigure 2: Dispersion of annual returns of buyout funds over the first 10 years of their lifetime. The lower and upper hinges correspond to the first and third quartiles of the annual returns, the line in the middle to the median. Data from Preqin. In this chart, only the annual return of 2002 is considered.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Dispersion of annual returns of buyout funds over the first 10 years of their lifetime. The lower and upper hinges correspond to the first and third quartiles of the annual returns, the line in the middle to the median. Data from Preqin. In this chart, only the annual return of 2009 is considered.\r\n\r\n\r\n\r\nLet’s start with 2002, where funds still in the investment period reported rather large negative returns on average, while more mature funds had better median returns around 0%. Overall, the patterns is very comparable to the one described above. For the 2009 cohort, the results are more mixed with both verry young and quite mature funds being strongly impacted, while funds around 5 to 6 years did reasonably well.\r\nTo summarize, investors in buyout funds should not expect great returns early on, but should get excited at the end of the investment period of what is yet to come. This pattern seems to hold up even in times of market mayhem.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHarris, Robert S, Tim Jenkinson, and Steven N Kaplan. 2014. “Private Equity Performance: What Do We Know?” The Journal of Finance 69 (5): 1851–82.\r\n\r\n\r\nNadauld, Taylor D, Berk A Sensoy, Keith Vorkink, and Michael S Weisbach. 2019. “The Liquidity Cost of Private Equity Investments: Evidence from Secondary Market Transactions.” Journal of Financial Economics 132 (3): 158–81.\r\n\r\n\r\nFor example, Harris, Jenkinson, and Kaplan (2014) report average multiples and IRRs for their sample of buyout funds of 14% and 2.0x across the vintage years.↩︎\r\nNote: I had a bug in my code that I fixed on Jan 30, 2022, which changed the plots. The overall patterns were unchanged though.↩︎\r\n",
    "preview": "posts/2021-11-28-value-creation-of-a-private-equity-fund-over-time/value-creation-of-a-private-equity-fund-over-time_files/figure-html5/BoxplotAnnualReturns-1.png",
    "last_modified": "2022-01-30T11:53:39+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-29-do-managers-mark-up-more-aggressively/",
    "title": "Do managers mark up their funds more aggressively?",
    "description": "If they do, a comparison of the unrealized performance over vintage years might be meaningless.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-09-25",
    "categories": [
      "Valuation",
      "Private Equity"
    ],
    "contents": "\r\nOne of the key differences between private and public equity is that the latter provides investors with daily updates on their asset values based on actual transactions as the stocks are sold and purchased on exchanges around the globe, taking into consideration all available information, both on the specific company and the overall market. In contrast, a fund investment’s valuation is typically only based on an actual transaction between a seller and a buyer when fund managers acquire the company and when they sell it.\r\nIn between, the valuations that fund managers provide to their investors are based on their internal valuation methodology, but there is no guarantee whatsoever if a buyer would be willing to pay this valuation. They might get it right, or they might under- or overestimate the value of the investment. But why would they do this? Well, first of all, just as all of us, fund managers are humans and they make mistakes. It might be that despite their best efforts, they misjudge what others would pay for their investment. In addition, they might have their reasons to distort the valuations. For example, Brown, Gredil, and Kaplan (2019) find that top-performing fund managers likely understate their valuations - why annoy your investors with higher valuations than subsequent proceeds? - while some struggling fund managers inflate valuations to increase their fundraising chances.1 Also, the percentage of fund managers using mark-to-market valuation has most likely also increased over time.\r\nTo summarize, unrealized investments are valued by fund managers and these valuations will most likely deviate in one way or the other from the true, fair market value that could be achieved in an arm’s length transaction. There is evidence that this deviation is not just independently distributed around the true value, but that there are systematic biases in these deviations that could inflate or underestimate the true performance of a fund. Finally, historically mark-to-market valuation was much less established in the early days of private equity, which might have resulted in much lower unrealized returns early on in a fund’s lifetime.\r\nHence, in this blog post, I look closer at the mark-ups of private equity funds early on in their lifetime up to 5 years. After 5 years, a fund is typically in the harvesting period and many valuations are already based on ongoing exit processes with third parties.\r\nTo do so, let’s look at the TVPIs of all U.S. buyout funds per vintage year after 4, 8, 12, 16, and 20 quarters. The idea is not to look at every boxplot individually, but rather try to identify any positive relationship between the vintage year and the TVPIs. Interestingly, and against my own prior belief, I don’t see such a relationship.\r\n\r\nI’m using data provided by Preqin and downloaded in March 2021.\r\n\r\n\r\n\r\nFigure 1: Dispersion of TVPIs for U.S. buyout funds, grouped by vintage year at different points in time over the fund life: after 4, 8, 12, 16, and 20 quarters (from top to bottom). Only funds with TVPI 2x or lower considered. Data from Preqin.\r\n\r\n\r\n\r\nLooking at the first row in the chart, funds in all vintage year groups were marked very close to 1x after 4 quarters. There is no clear trend that younger funds are valued up earlier than their predecessors have been in the past. One change that is apparent over time though is that the dispersion of TVPIs becomes wider for younger funds. Up till 2006, the fund valuations after 4 quarters have been very close to 1x for almost all funds within each vintage year group as there is barely a box visible and the whiskers are very small. Since then, the boxes have become wider and 2018 and 2019 vintages have whiskers extending above 1.25x.\r\nAfter 8 quarters, the story doesn’t change much. Funds are still valued close to 1x and the dispersion within each vintage year group is relatively small. One change compared to the top row is that there is some variability across the vintage year groups. Most importantly, one could detect recessions from this chart as both the 2000 and 2006 median fund were marked below 1x.\r\nOne year later, there is now a clear pattern to identify poor vintage years such as 1999/2000 funds, which were hit by the burst of the dot-com bubble, and 2006/7 funds, which invested prior to the global financial crisis. However, in relation to the question of this blog post - Do managers mark up their funds more aggressively? - there is again no indication that they do. Sure, funds with vintages 2008 and younger are typically marked above 1x after 3 years in their lifetime. But they also invested during times of good overall market sentiment and the valuations do not look aggressive by any standard. For none of the vintage year groups is the median TVPI over 1.25x. And more importantly, there are also plenty of older vintage years in which funds stood well above 1x after 3 years, such as funds from 1995 to 1997 and from 2001 to 2004.\r\nThe picture does not change much when we look after 16 and 20 quarters: the performance per group seems to be mostly driven by the overall economic environment, rather than by changes in the valuation process over time.\r\nOf course, this analysis cannot prove that fund managers have not become more aggressive or pro-active in managing their valuations. For example, it could be that older fund investments performed much better than younger funds and only the fact that fund managers have become more aggressive with their valuations led to similar multiples early on in the fund’s lifetime. Furthermore, I have shown that 2018 and 2019 vintages have seen a very strong start and it will be interesting to repeat the analysis in the future. Maybe fund managers have become much more aggressive, but it doesn’t show quite yet in the data. I also cannot exclude the possibility that the mark-up in earlier funds were simply driven by actual exits of some underlying investments, rather than by write-ups by the fund manager without any imminent exits.\r\nIdeally, one would have underlying investment level data to understand better the drivers of the valuations of unrealized investments in private equity to look further into this topic. However, just by looking at the fund TVPIs over time, there is no apparent trend that younger funds are marked up to higher multiples early on in their life.\r\n\r\n\r\n\r\nBrown, Gregory W, Oleg R Gredil, and Steven N Kaplan. 2019. “Do Private Equity Funds Manipulate Reported Returns?” Journal of Financial Economics 132 (2): 267–97.\r\n\r\n\r\nInteresting side note: investors see through this!↩︎\r\n",
    "preview": "posts/2021-08-29-do-managers-mark-up-more-aggressively/do-managers-mark-up-more-aggressively_files/figure-html5/DataPreparation-1.png",
    "last_modified": "2022-01-30T11:38:37+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-09-closing-process-of-a-secondary-transaction/",
    "title": "Closing process of a secondary transaction",
    "description": "How to get from the base purchase price to the final purchase price.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-05-16",
    "categories": [
      "Private Equity"
    ],
    "contents": "\r\nPrivate equity is an illiquid asset class for a reason: everything needs time. While you can read a negative article about a listed company in your portfolio in the newspaper and sell its stock the very same day, good luck trying that with a private equity fund you hold.\r\nOver the last decades, a large secondary market for private equity assets has evolved. Despite its rapid growth, the closing process continues to take time, at least a couple of weeks and often a couple of months. In this post, I’m exploring the implications of the prolonged closing process on the purchase price calculations.\r\nLet’s start with a schematic representation of the closing process that shows the three important dates: the reference or cut-off date, the forecast date, and the closing date.\r\nSchematic representation of the closing process in a secondary transaction.The reference date is the date at which it is assumed that the economic benefits and risks of owning the fund interest are transferred from the seller to the buyer. Put simply, until this date, the seller should pay for capital calls and receive distributions; after this date, the buyer should pay for capital calls and receive distributions. This date lies in the past and is a date at which the seller legally still owns the fund interest. Why use a date in the past? The reason is the reporting lag of private equity funds: the buyer must base his analysis on information provided by the underlying fund manager, which typically report quarterly and with a lag of 60 to 90 days. So when a seller wants to sell his fund interest, say, in May 2021, the most recent information he most likely has on the fund is the Q4 2020 reporting. The buyer then bases his analysis on the Q4 2020 and also offers a base purchase price as of this reference date.\r\nThe price the buyer is willing to pay is dependent on the cash flows he gets from holding the fund interest. As he does the analysis at the forecast date, but it is assumed that he held the interest already at the reference date, he considers the known capital calls and distributions from the reference to the forecast date. In the above picture, these are the distribution and capital call boxes marked in green.\r\nIf the seller accepts the buyer’s price, a sales and purchase agreement is negotiated and signed and the fund interest is transferred from the seller to the buyer. With the transfer and the simultaneous payment of the purchase price, the transaction is closed; and the date the closing happens is called the closing date. It typically takes another few weeks from a principal agreement of the purchase price between the seller and the buyer to the closing. The underlying fund manager also has to approve the transfer and often requires the transfer to happen at a quarter-end date. Hence, in the above example, if the negotiations start in May 2021 as the seller waited for December 31, 2020 numbers, it might very well be that the transaction only closes on September 30, 2021, a nine months time gap between the reference date and the closing date.\r\n\r\nCompare this with buying a stock: the buyer bases his price on all the information available up until the day he purchases the stock, so the reference, forecast and closing date are all on the same date!\r\nTo recap: the buyer offers to buy the fund interest as of the reference date, while he actually pays the seller at the closing date. So what happens with all the capital calls and distributions that happen in between, those known to the buyer when he did his underwriting (green boxes) and those additional ones before the closing (yellow boxes)? Well, in the first place, since the transfer has not happened yet, the seller continues to pay the capital calls and to receive the distributions. However, as it is assumed that the buyer is the owner of the fund interest as of the reference date these cash flows should have been the buyer’s, so he should have paid for the capital calls and he should have received the distributions. There is a simple fix to this issue: at closing, the final or net purchase price can be calculated by adjusting for all these interim or post-cut-off cash flows: all capital calls increase the purchase price - as the seller has paid for them, but the buyer should have - and all distributions reduce the purchase price - as the seller has received them and the buyer should have.\r\n\r\nAs part of the closing process, the seller sends the buyer a so-called pre-closing notice a few days before the actual closing to let him know the net purchase price to pay. Why does the seller have to send it? Remember that he still is legally the owner of the fund interest, so he is also the one that knows all the capital calls and distributions that came in.\r\nFor sellers without much experience in the secondary market, this purchase price adjustment can lead to unexpected results, especially in case of larger discounts and post-cut-off distributions, often seen for tail-end positions. Consider a situation where a buyer offers a 30% discount to the Q4 2020 NAV of a tail-end private equity fund with only one remaining company. This company is now exited for the latest valuation and therefore1 100% of the Q4 2020 NAV are distributed to the seller before closing. In this situation, the seller must pay the buyer at closing! Why? Because the buyer acquired the stake for 70% at the cut-off date Q4 2020 and should have received 100% of the NAV subsequently from the distribution. The net purchase price is \\(70\\%-100\\%=-30\\%\\).\r\n\r\nAssuming no other items on the balance sheet of the fund.↩︎\r\n",
    "preview": "posts/2021-05-09-closing-process-of-a-secondary-transaction/Closing_process.PNG",
    "last_modified": "2021-05-16T16:22:06+02:00",
    "input_file": {},
    "preview_width": 2506,
    "preview_height": 1320
  },
  {
    "path": "posts/2021-02-14-how-to-get-to-the-private-equity-target-allocation/",
    "title": "How to get to the private equity target allocation",
    "description": "A simple heuristic to help investors understand how to get to and maintain a target allocation in private equity. In a nutshell: steady state is your friend.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-04-19",
    "categories": [
      "Private Equity"
    ],
    "contents": "\r\nTo get to a target allocation for private equity, an investor has to understand how their commitments will translate to valuations over time. Unfortunately, this is not an easy exercise. Experienced investors often have their proprietary models to come up with commitment proposals. Often, these models are based on Monte Carlo simulations (MCS), such as a study by Pinebridge or an article I wrote together with a colleague about the risks in private equity.\r\nMore complicated models, such as Monte Carlo simulations, are essential for an investor who wants to build up and manage a private equity portfolio professionally. However, such models are not well suited to grasp the results intuitively and communicate them to others. And this is often as important as the validity of the results. People, me included, don’t feel comfortable making decisions about large sums of money that are based on opaque models. In addition, decision makers in larger organizations are not very familiar with private equity.\r\nHow can you give such decision makers an intuitive answer to the question how much one has to commit annually to funds to end up at a certain portfolio value? I have run dozens of private equity portfolios through Monte Carlo simulations, have written my diploma thesis about the allocation of private equity into an overall portfolio by using simulations; yet, if someone used to ask me this question, I could not come up with a simple answer.\r\nHence, I thought about the issue a bit more and came up with one. Without further ado, let’s get into it.\r\nThe idea is to start with the development of the net asset value (NAV) of a typical private equity fund over time. In the following, I use cash flow data from Preqin and for illustrative purposes, only focus on mature buyout funds with a vintage year of 2005 or older. The chart below shows the NAV in relation to the commitment over time for a median buyout fund.\r\n\r\n\r\n\r\nFigure 1: Data provided by Preqin. Only buyout funds with vintage year 2005 or older considered. Median statistics shown.\r\n\r\n\r\n\r\nDuring the investment period, the NAV increases quickly as the fund manager invests into underlying companies. The NAV peaks in year 5, at the end of the investment period for most buyout funds. Interestingly though, it does not get to anywhere close to the commitment size but starts decreasing around the 75% mark. This might come as a surprise to some, but if you commit USD 10 million to a buyout fund, the unrealized value in this fund will most likely not get close to USD 10 million at any point in the lifetime of the fund. This is not because buyout funds don’t perform, but because they start generating distributions well before the end of their lifetime.\r\n\r\nAs a reminder, I use funds with vintage years of 2005 or older and it might be that youngers funds took longer to generate liquidity. Maybe something I can look into more detail in another post.\r\n\r\n\r\n\r\nWhat we can also see from the chart is that a typical buyout fund takes 15 years to be fully realized. To reach a steady state, an investor must commit each year the same amount. After 15 years, the portfolio is then at the point where it consists of a fund in each stage (one-year old fund, two-year old fund, etc.).\r\nAll we got to do now is to string together the relevant curves for the funds, with a year difference per fund. The table below shows the built up of the portfolio for the first 10 years.\r\n\r\n\r\nTable 1: Built up of private equity portfolio\r\n\r\n\r\n\r\n\r\n\r\nYear\r\n\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\n7\r\n\r\n\r\n8\r\n\r\n\r\n9\r\n\r\n\r\n10\r\n\r\n\r\nFund 1\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\n62%\r\n\r\n\r\n66%\r\n\r\n\r\n70%\r\n\r\n\r\n63%\r\n\r\n\r\n56%\r\n\r\n\r\n46%\r\n\r\n\r\n36%\r\n\r\n\r\n27%\r\n\r\n\r\nFund 2\r\n\r\n\r\n0%\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\n62%\r\n\r\n\r\n66%\r\n\r\n\r\n70%\r\n\r\n\r\n63%\r\n\r\n\r\n56%\r\n\r\n\r\n46%\r\n\r\n\r\n36%\r\n\r\n\r\nFund 3\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\n62%\r\n\r\n\r\n66%\r\n\r\n\r\n70%\r\n\r\n\r\n63%\r\n\r\n\r\n56%\r\n\r\n\r\n46%\r\n\r\n\r\nFund 4\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\n62%\r\n\r\n\r\n66%\r\n\r\n\r\n70%\r\n\r\n\r\n63%\r\n\r\n\r\n56%\r\n\r\n\r\nFund 5\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\n62%\r\n\r\n\r\n66%\r\n\r\n\r\n70%\r\n\r\n\r\n63%\r\n\r\n\r\nFund 6\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\n62%\r\n\r\n\r\n66%\r\n\r\n\r\n70%\r\n\r\n\r\nFund 7\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\n62%\r\n\r\n\r\n66%\r\n\r\n\r\nFund 8\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\n62%\r\n\r\n\r\nFund 9\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n0%\r\n\r\n\r\n20%\r\n\r\n\r\n42%\r\n\r\n\r\nWe can also plot the NAV curves for individual funds and show the sum of these individual curves, which is nothing else as the value of the whole portfolio. The value of the portfolio reaches its steady state after 15 years, which is when the commitment into the first fund is fully realized.\r\n\r\n\r\n\r\nThe value of the portfolio is slightly above 5, based on a one unit of annual commitments. As an example, an investor who wants to get to a roughly USD 500 million private equity portfolio (in terms of NAV) should commit around USD 100 million each year. Of course, this rule is dependent on the assumption about the development of the NAV for a private equity fund. Above, I used the median curve of buyout funds from the Preqin universe.\r\nBelow, I show the NAV development for different percentiles from the Preqin data set. Notice the large dispersion: while the median curve reaches it maximum before 75%, there are close to 10% of buyout funds that actually end up with an NAV larger than the commitment size.\r\n\r\nThe black curve in this chart is the same as the red NAV curve in the first chart.\r\n\r\n\r\n\r\nThis is why diversification is so important: if an investor commits to only one fund, the outcome could be anything. However, the more diversified the portfolio becomes, the higher the likelihood that the portfolio ends up closer to the average.1\r\nTo summarize: In this post, I have shown a simple trick to understand what annual commitments are required to end up with a certain private equity allocation. The trick is to string together the same NAV curve of a representative fund until a steady state is reached. In the steady state, the sum of all individual NAVs is the portfolio NAV. I have further shown that in case of a typical buyout fund, the factor from annual commitments to portfolio NAV is 5.3. An investor who commits USD 10 million per year would end up with a portfolio NAV of USD 53 million when the steady state is reached.\r\nAnother way to derive this result is by calculating the mean of the NAV curve for one fund over the 15 years, which is 35% for the median buyout fund in our sample. In absolute numbers and for a USD 10 million commitment, that means that throughout the lifetime of the fund the NAV is on average USD 3.5 million. As we have 15 funds in the steady state in parallel, the portfolio NAV is USD 53 million.2\r\nOf course, an investor who wants to start investing in private equity should rely on more sophisticated models. But this simple heuristic is a good starting point to get a better understanding how commitments and portfolio valuations are linked.\r\n\r\nCareful: the average is not the same as the median and due to the right-skewed distributions in private equity, the average will be higher. However, we want here a big picture analysis: Which ballpark commitments do I have to write to end up at a certain allocation? To answer this question, the subtle difference between median and average is not that important.↩︎\r\nDon’t get caught up on the number of funds. All you need is an average ratio of NAV to commitment. If you have that, it is irrelevant if the commitment is given to one or ten funds per year. The ratio might change depending on the number of funds, but I want to emphasize again that this approach just wants to support an investor getting a simple understanding of why certain commitment amounts are needed to get to a certain private equity allocation.↩︎\r\n",
    "preview": "posts/2021-02-14-how-to-get-to-the-private-equity-target-allocation/how-to-get-to-the-private-equity-target-allocation_files/figure-html5/Combine_curves-1.png",
    "last_modified": "2021-04-21T21:59:19+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-28-why-does-private-equity-outperform-public-equity/",
    "title": "Why does private equity outperform public equity?",
    "description": "According to recent studies, it's a combination of selection skills and leverage.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "Private Equity",
      "Research"
    ],
    "contents": "\r\nOver the last decades, buyout funds have consistently performed better than the broader public equity markets.1 What factors explain this outperformance? A new study by Erik Stafford finds that it’s a combination of strong selection skills by PE managers and leverage.\r\nStafford starts with a data set of 667 public-to-private transactions from 1984 to 2017 collected from Thomson Reuters and CapitalIQ. He then identifies factors that increase the likelihood of a company to be taken private. The results are shown in his Table 4:\r\nScreenshot of Table 4 of Stafford’s paper. The table reports the results of Fama-MacBeth regressions of a binary variable indicating a public equity was taken private on various lagged firm characteristics.Not surprisingly, take-private transactions happen with relatively small public firms. Emphasis on relative: for private equity, these are still some of the biggest deals ever pulled off and only the largest PE firms can get them done; for public equity, those are the smaller companies though. Don’t expect a take-private of Apple or Amazon anytime soon! Furthermore, companies taken private by PE firms tend to be value stocks, as they have a lower book-to-market ratio, and they tend to be cash generative. ISS is a three-year net equity issuance measure. As it is more negative for take-private transactions, it implies that these firms have repurchased their own shares, rather than issued new ones. Finally, take-private firms have lower profitability than the firms of the overall sample.\r\nOf course, small and value stocks are known to have generated persistently higher returns over decades, so much so that they became their own risk factors.2 So the question is if the outperformance is simply a combination of a value and size effect. The short answer is no.\r\nTo show that, Stafford classifies stocks into groups, either based on their EBITDA multiple (Table 5), or based on their likelihood to be PE-targets based on the characteristics he identified previously (Table 6).3\r\nLet’s look at the results of Table 6:\r\nScreenshot of Table 6 of Stafford’s paper. The table groups stocks based on their likelihood to be PE take-private targets and compares the returns.In short, firms that a typically PE firm targets have outperformed other listed firms considerably. The difference in returns between the group with the highest likelihood of being a PE target and the lowest likelihood is 18.4% per year for the equal-weighted portfolio and 16.3% for the value-weighted portfolio. Even when adjusting for common risk-factors (CAPM, FF 3-and-5 factor), there continues to be a statistically significant alpha.\r\nAfter Stafford has shown that PE managers did a great job selecting take-private targets and that he is able to run a replicating strategy to identify such targets, he creates a replicating portfolio by investing in the top quintile stocks of the predicted PE-targets (essentially, he tries to get into the Group High stocks in Table 6). On top of that, he leverages the positions 2x the target weight, with 25% of debt amortizing over 4 years and repayment of remaining debt at the time that the position is liquidated. This is done because private equity investments are substantially higher levered than publicly listed firms and he wants to ensure that he makes an apple-to-apple comparison. In addition, he considers management fees of 1% per year on the portfolio assets (i.e., 2% on equity levered 2x) and calculates the unrealized value not by using the stock price, but by calculating a book value with hold-to-maturity accounting.\r\nIn Table 8, he compares this replicating portfolio with a portfolio from the Burgiss data set, so actual private equity funds:\r\nScreenshot of Table 8 of Stafford’s paper. Comparison of the annual cash flows of private equity buyouts from a Burgiss data set with Stafford’s replicating portfolio.The table shows that the replicating portfolio can closely mimic the performance of private equity funds. He concludes (my emphasis):\r\n\r\nThe key result is that the asset selection identified from the public-to-private transactions has been excellent historically. The selection analysis of public buyouts finds that characteristics beyond those used to construct common factors are important. Portfolios managed to match these characteristics earn positive risk-adjusted returns after controlling for common factors. This suggests that PE buyout managers may be skilled at asset selection. There is no direct evidence that their individual stock selection improves returns over their style-tilts, but these managers were early to value investing, using a selection strategy that performed better than other commonly available versions, and in the process, improved investor access to these difficult to invest in exposures. To the extent that PE fund managers are skilled at asset selection, they may optimally deviate from their historical rules in the future. Looking back, an allocator is almost surely pleased to have allocated to PE buyouts.\r\n\r\nIt is important to keep in mind that Stafford is not able to look into other important factors of PE investments, such as differences in the ownership structure and, in turn, governance as well as operational changes initiated by the PE manager. However, as his replicating portfolio already performs slightly better than the Burgiss data, he argues that other factors should have a negative impact on the returns, most likely because they reduce risk.\r\nI’m a bit surprised that Stafford doesn’t cite the study by L’Her et al. from 2016, which in my opinion is comparable both by the approach and the results. The starting point of this study is also the outperformance of private equity compared to a broad stock market index such as the S&P 500, as measured with the public market equivalent (PME). The authors then look at 3,492 buyout transactions4 that happened between 1993 and 2013 from CapitalIQ and obtained information about the leverage from LCD on over 1,400 of those transactions. Finally, they created alternative public market indices adjusted for the leverage, the size, and the sector composition of the buyout transactions. When comparing the performance of private equity funds, again measured with the Burgiss data set, with these adjusted indices, they show that the outperformance disappears. The PME is reduced from 1.22/1.17 (equal- and value-weighted) in case of the unadjusted S&P 500 to 1.10/0.96 in case of the fully adjusted stock market index:\r\nScreenshot of Table 3 of the paper by L’Her, Stoyanova, Shaw, Scott and Lai (2016). Buyout funds’ PMEs (based on Burgiss data) and implied annualized excess returns with alternative public market indexes.To summarize, both studies show that private equity returns could have been partially replicated historically by focusing on smaller firms and using leverage. Both studies further imply that PE managers had great selection skills. In the case of the study of L’Her et al., this is shown by the fact that PE managers have chosen better performing sectors, in case of Stafford’s study, by replicating the PE selection with a model that is based on several characteristics.\r\nIt will be interesting to see if PE managers continue to select the same firms going forward or if they deviate from their historical rules in the future.\r\n\r\nHarris et al. (2014) is a recent study that documents the outperformance.↩︎\r\nI’m of course referring to the Fama-French 3-factor model, based on their landmark 1992 paper.↩︎\r\nIn case you wonder: he makes sure that he only uses information that is known at the time.↩︎\r\nNot only take-privates, in contrast to Stafford’s study.↩︎\r\n",
    "preview": "posts/2021-02-28-why-does-private-equity-outperform-public-equity/Table_4_Stafford.PNG",
    "last_modified": "2021-02-28T14:29:04+01:00",
    "input_file": {},
    "preview_width": 2093,
    "preview_height": 1489
  },
  {
    "path": "posts/2021-01-31-the-value-bridge/",
    "title": "The LBO value bridge",
    "description": "One of the most important valuation tools in private equity.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-02-06",
    "categories": [
      "Valuation",
      "R",
      "Private Equity"
    ],
    "contents": "\r\nA key question in private equity is how a fund manager (general partner, or “GP”) generates value. With most key questions in life, there is no simple answer and fund managers around the globe will come up with their own ones. By having a great network, by selecting the right deals, by being able to generate proprietary deal flow, by improving the businesses operationally, by prettying them up for a successful exit, by identifying sector trends, etc. There are so many answers to the question and each GP has their own answer in how they try to return more capital to their investors than they took.\r\nHowever, for investors it is important to have a framework to measure value creation consistently. Only then can they make meaningful comparisons between different LBOs, funds and GPs. Fortunately, such a framework exists: the LBO value bridge, which splits the overall gain or loss made on an investment into a combination of the following factors:\r\nEBITDA growth: either by top-line growth or margin improvements;\r\nChange in the EV/EBITDA valuation multiple: by selling for a higher or lower multiple than what the business was acquired for;\r\nChanges in net debt/cash: by reducing net debt so that a larger share of the enterprise value ends up with the fund (FCF effect);\r\nLeverage\r\nIn this blog post, I show how to calculate these factors for an LBO. I then run through a couple of examples.\r\nHow to calculate the value creation factors for an LBO\r\nDerivation\r\nThe goal is to explain the total net proceeds or gains to equity investors during the holding period, \\(TNP\\), i.e., the difference between the equity proceeds to equity holders at exit, \\(E_T\\), and the invested cost of equity at entry, \\(E_t\\), as well as any interim dividends and equity injections:1\r\n\r\nThroughout this blog post, I draw heavily from Daniel Pindur’s and Benjamin Puche’s dissertations, which explain the methodology and an example in greater detail. They also show empirical results for larger data sets.\r\n\\[ TNP=E_T - E_t + \\sum_{n=t+1}^{T-1} E_n. \\]\r\nThe equity value at entry and exit is the difference between the enterprise value, \\(EV\\), and the net debt, \\(ND\\). For example, we can calculate \\(E_T\\) as:\r\n\\[ E_T = EV_T - ND_T.\\]\r\nThe enterprise value is typically expressed as the product of the EBITDA and the EV/EBITDA multiple. More generally though, we can use any cash flow measure, \\(C\\), and the accompanying multiple, \\(m\\):2\r\n\\[ E_T = m_T C_T - ND_T.\\]\r\nPlugging the \\(E_T\\) and \\(E_t\\) equations into the equation to calculate \\(TNP\\), we get:\r\n\\[ \r\n\\begin{aligned}\r\nTNP &=E_T - E_t + \\sum_{n=t+1}^{T-1} E_n \\\\\r\n   &=(m_T C_T - ND_T) - (m_t C_t - ND_t) + \\sum_{n=t+1}^{T-1} E_n \\\\\r\n   &=(m_T C_T - m_t C_t) - (ND_t - ND_t) + \\sum_{n=t+1}^{T-1} E_n \\\\\r\n\\end{aligned}\r\n\\]\r\nLet’s further define \\(\\Delta\\) as the change of a variable from \\(t\\) to \\(T\\). For example, \\(\\Delta m = m_T - m_t\\). Then we can rewrite the above equation:\r\n\\[ \r\n\\begin{aligned}\r\nTNP   &=((\\Delta m + m_t) (\\Delta C + C_t) - m_t C_t) - \\Delta ND + \\sum_{n=t+1}^{T-1} E_n \\\\\r\n      &= m_t \\Delta C + \\Delta m C_t + \\Delta m \\Delta C + m_t C_t -m_t C_t - \\Delta ND + \\sum_{n=t+1}^{T-1} E_n \\\\\r\n      &= m_t \\Delta C + \\Delta m C_t + \\Delta m \\Delta C - \\Delta ND + \\sum_{n=t+1}^{T-1} E_n \\\\\r\n\\end{aligned}\r\n\\] While the above formula might look scary, it has a pretty straightforward interpretation that is easiest to see with a small example. Let’s assume a 100% equity financed transaction without any dividends or cash injections between the entry and the exit.3 Let’s assume further a fund invested 100 and got 200 back, so the total net proceeds \\(TNP\\) are 100. Where did this gain of 100 come from? The above formula shows that it must come either from an increase of the EBITDA, an increase of the multiple, or a combination of both. There is no other explanation.\r\nLet’s assume that the company had an EBITDA at entry of 10 (\\(C_t=10\\)). As \\(EV_t=E_t=100\\) in this example, it follows that \\(m_t=10\\). What happens if the EBITDA was still the same at exit, i.e., \\(C_T=10\\)? In this case, \\(m_T\\) must be 20, as we know that \\(EV_T=E_T=200\\). The value creation came in this case exclusively from multiple expansion, the \\(\\Delta m C_t\\) part of the equation.\r\nIf we assume instead that the EBITDA increased to 20 until exit, the exit multiple would have been \\(m_T=10\\) and the value creation would have come exclusively from the EBITDA increase, what is often called operational improvements.\r\nIn reality, it is almost always a combination of both an increase in EBITDA and multiple expansion. As a result, there is a part of the value creation that cannot be allocated unambiguously to either the EBITDA increase or the multiple expansion. This is the \\(\\Delta m \\Delta C\\) part of the equation.\r\nFurther required adjustments\r\nIn the previous section, I have shown how to break up the total net gains for an equity investor into three components (and a combination component):4\r\nEBITDA growth: \\(m_t \\Delta C\\)\r\nChange in the EV/EBITDA valuation multiple: \\(\\Delta m C_t\\)\r\nChanges in net debt/cash: \\(\\Delta ND\\)\r\nA combination of 1. and 2.: \\(\\Delta m \\Delta C\\)\r\nSo far, all calculations are in absolute values. Practitioners though typically think in terms of money multiples, as it makes a comparison between LBOs much easier. Did a GP do a good job that generated net gains of USD 100 million? Well, we don’t know, as we don’t know how much money was invested in the first place. However, if we scale the net gains by the invested capital and calculate the multiple, it becomes much easier: if they invested USD 10 million, and made 11x, it looks like a great deal; if they invested USD 1 billion, and made 1.1x, it doesn’t look so good anymore.\r\nHowever, even looking at the multiple can be misleading as we have so far ignored another very important value driver of LBOs, leverage. To understand why leverage is so important, let’s compare the two LBOs in the table below.5\r\n\r\nShow code\r\nlibrary(knitr)\r\nlibrary(kableExtra)\r\nlibrary(data.table)\r\nDT <- data.table(Item        = c(\"EV\", \"Equity\", \"Net debt\", \"EBITDA\", \"Interim cash flows\"),\r\n                 LBO_A_entry = c(100, 100, 0, 10, 0),\r\n                 LBO_A_exit  = c(200, 200, 0, 20, 0),\r\n                 LBO_B_entry = c(100, 10, 90, 10, 0),\r\n                 LBO_B_exit  = c(200, 110,90, 20, 0))\r\nkbl(DT, col.names = c(\"Item\", \"Entry\", \"Exit\", \"Entry\", \"Exit\")) %>%\r\n  kable_classic(full_width = FALSE) %>%\r\n  add_header_above(c(\" \" = 1, \"LBO by Fund A\" = 2, \"LBO by Fund B\" = 2))\r\n\r\n\r\n\r\n\r\n\r\n\r\nLBO by Fund A\r\n\r\n\r\n\r\n\r\nLBO by Fund B\r\n\r\n\r\n\r\nItem\r\n\r\n\r\nEntry\r\n\r\n\r\nExit\r\n\r\n\r\nEntry\r\n\r\n\r\nExit\r\n\r\n\r\nEV\r\n\r\n\r\n100\r\n\r\n\r\n200\r\n\r\n\r\n100\r\n\r\n\r\n200\r\n\r\n\r\nEquity\r\n\r\n\r\n100\r\n\r\n\r\n200\r\n\r\n\r\n10\r\n\r\n\r\n110\r\n\r\n\r\nNet debt\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n90\r\n\r\n\r\n90\r\n\r\n\r\nEBITDA\r\n\r\n\r\n10\r\n\r\n\r\n20\r\n\r\n\r\n10\r\n\r\n\r\n20\r\n\r\n\r\nInterim cash flows\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nIn both LBOs, the exact same value creation happened: the enterprise value increased by 100, or 2x, due to a 2x increase of the EBITDA. There was no multiple expansion. Furthermore, in both LBOs, the equity value increased by 100. The only difference is the financing: the LBO of Fund A was fully equity financed, the LBO of Fund B was financed 10% with equity and 90% with debt.\r\nNow, if an investor in Fund A and one in Fund B would meet and exchange notes, which LBO would steal the show? Most certainly the LBO of Fund B, where investors made a 10x return, while the multiple on the LBO in Fund A was a measly 2x. Such is the power of leverage…However, rather than chiming in on the praise for the GP of Fund B, we want a methodology that simply shows that almost all of the value creation was due to the leverage.\r\nTo do so, let’s start with the multiple to equity investors, or the levered multiple, \\(TM_L\\):\r\n\\[ TM_L = \\frac{TNP - \\sum_{n=t+1}^{T-1} E_n}{\\sum_{n=t}^{T-1} E_{n}^{+}}. \\] This formula includes now the capital injection and distribution parts to calculate the total proceeds to equity holders, as they are just as relevant: distributions during the holding period increase an investor’s gains, just as capital injections reduce them (let’s denote them with a \\(+\\) superscript, i.e., \\(E^+\\)). Next, I divide by all capital injections, including the initial investment at \\(E_t\\).6 These formulas always look a bit complicated, so let’s have a simple example to clarify: 100% of equity in a company is acquired for 100; during the holding period, additional 50 are injected in the business and 60 are paid out (e.g., through a dividend); at exit, the equity holders get 200. Then the multiple is calculated as:\r\n\\[ TM_L = \\frac{200-100-50+60}{100+50} = \\frac{110}{150} = 0.7x. \\] It is important to keep in mind that this is the multiple on the net gains, i.e., ignoring the invested cost, in the numerator. This is different to the multiple used in every pitch book or in my example above, where the invested cost, or 1x, is added (so 1.7x in this example). The reason I use this slightly different definition, following the literature in this field, is that it allows me to calculate an unlevered multiple, \\(TM_U\\), from the levered multiple, \\(TM_L\\), applying the famous Miller/Modigliani theorem:\r\n\\[ TM_U = \\frac{TM_L + r_D \\frac{D}{E}}{1+\\frac{D}{E}}. \\]\r\n\\(r_D\\) are the costs of the debt over the entire investment time and \\(D/E\\) is the average debt/equity ratio.7\r\nApplying this formula to the above example shows that the unlevered multiple is exactly the same: \\((10 + 0\\times 90/10)/(1+90/10)=1\\), or \\(2\\), if you include the invested cost.\r\nNow I have all the different parts to explain the levered multiple. To summarize, the procedure is as follows:\r\nStart by calculating the levered multiple, \\(TM_L\\). Always keep in mind that this multiple does not include the invested cost, so a multiple of 0x does not mean a full loss of invested capital, but zero gains. A full loss would be -1x.\r\nCalculate the unlevered multiple, \\(TM_U\\), using the Miller/Modigliani theorem.\r\nCalculate the other value creation factors. As the sum of all value creation factors must add up to the overall net gains, \\(TNP - \\sum_{n=t+1}^{T-1} E_n\\), dividing them by the net gains will give the relative split of each value creation lever.\r\nMultiplying the percentages per value creation lever with the unlevered multiple shows it as a multiple, instead of as a percentage.\r\nLet’s walk through a few more realistic examples and let’s also plot the value bridge, which gives a much better intuition of the results. I promise that the next part will be more fun than this one.\r\nR implementation\r\nI implemented functions to calculate and plot the value creation factors in R. Let’s run through an example, taken from Puche’s dissertation.8\r\n\r\nShow code\r\n#' Calculate the value creation bridge for a PE investment, splitting it up in leverage, multiple and EBITDA effect\r\n#'\r\n#' The function creates the value creation for LBOs; detailed explanation about methodology can be found in Puche (2016) and Pindur (2007).\r\n#' The split-up of the total proceeds (which are net of invested cost, i.e. the net capital gain) in multiple effect, EBITDA effect,\r\n#' combination effect and FCF effect is straightforward and best explained\r\n#' in Pindur (2007). Pindur then scales everything simply by dividing by the total proceeds so that everything adds to 1.\r\n#'\r\n#' In addition, Puche (2016, see footnote 23) focuses on the multiple, which is partly a result of leverage; hence, he\r\n#' unlevers first the multiple and the difference between the levered and unlevered mutiple is the multiple effect.\r\n#'\r\n#'@param .startEquity numeric; equity valuation at start date of the observation period (typically at entry), i.e. EV minus the debt at start point.\r\n#'                    Careful: if fund does not acquire 100 per cent of equity, you have to scale the fund's investment up.\r\n#'@param .endEquity numeric; equity valuation at end date of observation period.\r\n#'@param .startDebt numeric; debt at start date.\r\n#'@param .endDebt numeric; debt at end date.\r\n#'@param .startRev numeric; revenue at start date.\r\n#'@param .endRev numeric; revenue at end date.\r\n#'@param .startEBITDA numeric; EBITDA at start date.\r\n#'@param .endEBITDA numeric; EBITDA at end date.\r\n#'@param .interimCC numeric; interim additional equity investments. Careful with scaling, should be 100 per cent of equity.\r\n#'@param .interimDist numeric; interim distributions to equity from investments. Careful with scaling, should be 100  per cent of equity.\r\n#'@param .interimIntRate numeric; the interest rate during the holding period for that specific deal. \r\n#'@param .holdingPeriod numeric; holding period (in years) from start to end date of the observation.\r\n#'@return List including the following: 1) A vector with the levered and unlevered multiple (TM_lev and TM_unlev), \r\n#'        the overall gain / (loss) on the deal, and \r\n#'        the leverage effect, which is the difference between TM_lev and TM_unlev; it then includes the different effects (FCF, EBITDA, etc.) \r\n#'        that explain the gain. Important: those effects are shown in absolute numbers. 2) A data.table with the relevant input parameters. \r\n#'        3) A data.table with the relevant results.\r\n#'@export\r\n#'\r\n#'@references\r\n#' Puche (2016): Essays on Value Creation and its Determinants in Private Equity, Dissertation, p. 29ff\r\n#' Pindur (2007): Value Creation in Successful LBOs, Dissertation, p. 69ff\r\n#'\r\n#'@examples\r\n#' ##### Use numerical example from Puche (2016, Table 2-1)\r\n#' list_results <- value_creation_LBO(.startEquity         = 50,\r\n#'                    .endEquity           = 135,\r\n#'                    .startDebt           = 50,\r\n#'                    .endDebt             = 30,\r\n#'                    .startRev            = 100,\r\n#'                    .endRev              = 120,\r\n#'                    .startEBITDA         = 10,\r\n#'                    .endEBITDA           = 15,\r\n#'                    .interimCC           = 15,\r\n#'                    .interimDist         = 20,\r\n#'                    .interimIntRate      = 0.09,\r\n#'                    .holdingPeriod       = 4)\r\nvalue_creation_LBO <- function(.startEquity, .endEquity, .startDebt, .endDebt, .startRev, .endRev, .startEBITDA, .endEBITDA,\r\n                               .interimCC, .interimDist, .interimIntRate, .holdingPeriod) {\r\n\r\n  ### Calculations of additional inputs\r\n  startEV <- .startEquity + .startDebt\r\n  endEV   <- .endEquity   + .endDebt\r\n  startEV_EBITDA_mult <- startEV/.startEBITDA\r\n  endEV_EBITDA_mult   <- endEV/.endEBITDA\r\n  costDebt <- (1+.interimIntRate)^.holdingPeriod - 1\r\n  invCapital <- .startEquity + .interimCC\r\n  deltaEquity <- .endEquity - .startEquity\r\n  gain <- deltaEquity + .interimDist - .interimCC\r\n  avgDebtEquity <- (.startDebt/.startEquity + .endDebt/.endEquity)/2 #TODO: Understand why you need the average here\r\n  startMargin <- .startEBITDA/.startRev\r\n  endMargin   <- .endEBITDA/.endRev\r\n\r\n  ### Calculations of output\r\n  TM_lev                  <- gain/invCapital\r\n  TM_unlev                <- (TM_lev + costDebt * avgDebtEquity)/(1 + avgDebtEquity)\r\n  lev_effect              <- TM_lev - TM_unlev\r\n  mult_effect             <- .startEBITDA * (endEV_EBITDA_mult - startEV_EBITDA_mult)\r\n  mult_EBITDA_comb_effect <- (.endEBITDA - .startEBITDA) * (endEV_EBITDA_mult - startEV_EBITDA_mult)\r\n  FCF_effect              <- -(.endDebt - .startDebt) + .interimDist - .interimCC\r\n  EBITDA_effect           <- (.endEBITDA - .startEBITDA) * startEV_EBITDA_mult\r\n  Rev_effect              <- (.endRev - .startRev) * startMargin * startEV_EBITDA_mult\r\n  margin_effect           <- (endMargin - startMargin) * .startRev  * startEV_EBITDA_mult\r\n  SM_comb_effect          <- (.endRev - .startRev) * (endMargin - startMargin) * startEV_EBITDA_mult\r\n\r\n  vec_results <-c(TM_lev                  = TM_lev,\r\n                  TM_unlev                = TM_unlev,\r\n                  gain                    = gain,\r\n                  lev_effect              = lev_effect,\r\n                  mult_effect             = mult_effect,\r\n                  mult_EBITDA_comb_effect = mult_EBITDA_comb_effect,\r\n                  EBITDA_effect           = EBITDA_effect,\r\n                  FCF_effect              = FCF_effect,\r\n                  Rev_effect              = Rev_effect,\r\n                  margin_effect           = margin_effect,\r\n                  SM_comb_effect          = SM_comb_effect,\r\n                  startEV_EBITDA_mult     = startEV_EBITDA_mult,\r\n                  endEV_EBITDA_mult       = endEV_EBITDA_mult,\r\n                  startEV                 = startEV,\r\n                  endEV                   = endEV,\r\n                  totalInv                = .startEquity + .interimCC,\r\n                  TM_mult                 = mult_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_mult_EBITDA_comb     = mult_EBITDA_comb_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_EBITDA               = EBITDA_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_FCF                  = FCF_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_Rev                  = Rev_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_margin               = margin_effect/abs(gain) * abs(TM_unlev),\r\n                  TM_SM_comb              = SM_comb_effect/abs(gain) * abs(TM_unlev))\r\n  \r\n    print_table_inputs <- data.table(\r\n     Item  = c(\"Revenue\", \"EBITDA\", \"Equity\", \"Net Debt\", \"EV\", \"EV/EBITDA\", \"CC\", \"Dist\",\r\n               \"Holding period\", \"Interest rate p.a.\",  \"Avg debt/equity ratio\", \"Cost of debt\"),\r\n     Entry = c(.startRev, .startEBITDA, .startEquity, .startDebt, startEV, startEV_EBITDA_mult,\r\n               rep(NA, 6)),\r\n     Interim = c(rep(NA, 6), .interimCC, .interimDist, \r\n                 .holdingPeriod, .interimIntRate, avgDebtEquity, costDebt),\r\n     Exit    = c(.endRev, .endEBITDA, .endEquity, .endDebt, endEV, endEV_EBITDA_mult, rep(NA, 6))\r\n  )\r\n  \r\n  print_table_outputs <- data.table(\r\n     Item  = c(\"TM (levered)\", \r\n               \"Gain / TM (unlevered)\",\r\n               \"FCF\",\r\n               \"Comb. multiple / EBITDA\",\r\n               \"Multiple\",\r\n               \"EBITDA\",\r\n               \"Comb. revenue/margin\",\r\n               \"Revenue\",\r\n               \"Margin\"),\r\n     Absolute = c(NA, vec_results[c(3,8,6,5,7,11,9,10)]),\r\n     TM       = c(vec_results[c(1,2,20,18,17,19,23,21,22)])\r\n  )\r\n  \r\n  \r\n  ### Output\r\n  return(list(vec_results         = vec_results,\r\n              print_table_inputs  = print_table_inputs,\r\n              print_table_outputs = print_table_outputs))\r\n\r\n}\r\n\r\n\r\n\r\n\r\nShow code\r\n#' Wrapper function for ggplot to plot value bridge for function \\code{\\link{value_creation_LBO}}\r\n#'\r\n#' @param .list_value_creation_LBO list; output from function \\code{\\link{value_creation_LBO}}.\r\n#' @return ggplot2 bar plot that shows the value creation bridge.\r\n#'\r\n#' @import ggplot2\r\n#' @export\r\n#'\r\n#' @references\r\n#' \\url{https://learnr.wordpress.com/2010/05/10/ggplot2-waterfall-charts/}\r\n#'\r\n#' @examples\r\n#' #### Puche example\r\n#' plotValueBridge(value_creation_LBO(\r\n#'                    .startEquity         = 50,\r\n#'                    .endEquity           = 135,\r\n#'                    .startDebt           = 50,\r\n#'                    .endDebt             = 30,\r\n#'                    .startRev            = 100,\r\n#'                    .endRev              = 120,\r\n#'                    .startEBITDA         = 10,\r\n#'                    .endEBITDA           = 15,\r\n#'                    .interimCC           = 15,\r\n#'                    .interimDist         = 20,\r\n#'                    .interimIntRate      = 0.09,\r\n#'                    .holdingPeriod       = 4)$vec_results)\r\n#' #### How does it look with negative multiples\r\n#' plotValueBridge(value_creation_LBO(.startEquity         = 50,\r\n#'                    .endEquity           = 60,\r\n#'                    .startDebt           = 50,\r\n#'                    .endDebt             = 30,\r\n#'                    .startRev            = 100,\r\n#'                    .endRev              = 80,\r\n#'                    .startEBITDA         = 10,\r\n#'                    .endEBITDA           = 6,\r\n#'                    .interimCC           = 5,\r\n#'                    .interimDist         = 0,\r\n#'                    .interimIntRate      = 0.09,\r\n#'                    .holdingPeriod       = 4)$vec_results)\r\n#' #### Yet another example\r\n#' plotValueBridge(value_creation_LBO(.startEquity         = 37.333,\r\n#'                    .endEquity           = 50.67,\r\n#'                    .startDebt           = 7.1,\r\n#'                    .endDebt             = 0.7,\r\n#'                    .startRev            = 91,\r\n#'                    .endRev              = 117.3,\r\n#'                    .startEBITDA         = 7.7,\r\n#'                    .endEBITDA           = 10.9,\r\n#'                    .interimCC           = 0,\r\n#'                    .interimDist         = 1.44,\r\n#'                    .interimIntRate      = 0.09,\r\n#'                    .holdingPeriod       = 2)$vec_results)\r\n#' ##### Next example where multiple is increased because the business is struggling and GP wants to moderate decrease in valuation\r\n#' plotValueBridge(value_creation_LBO(.startEquity         = 19.13,\r\n#'                    .endEquity           = 12.05,\r\n#'                    .startDebt           = 0.8,\r\n#'                    .endDebt             = 4.53,\r\n#'                    .startRev            = 87.12,\r\n#'                    .endRev              = 58.98,\r\n#'                    .startEBITDA         = 5.54,\r\n#'                    .endEBITDA           = 3.93,\r\n#'                    .interimCC           = 0,\r\n#'                    .interimDist         = 0,\r\n#'                    .interimIntRate      = 0.09,\r\n#'                    .holdingPeriod       = 2)$vec_results)\r\nplot_value_bridge <- function(.vec_value_creation_LBO, .fillColors = c(\"#FFC300\", \"#581845\")) {\r\n\r\n  #Calculate inbetween multiples\r\n  mult_OpImpr <- .vec_value_creation_LBO[\"TM_unlev\"] - .vec_value_creation_LBO[\"TM_mult\"] - .vec_value_creation_LBO[\"TM_mult_EBITDA_comb\"]\r\n\r\n  strDesc <- c(\"Combination S/M effect\", \"Margin effect\", \"Sales effect\", \"EBITDA effect\", \"FCF effect\", \"Operating improvements\",\r\n               \"Combination effect\", \"Multiple effect\", \"TM (unlev)\", \"Leverage effect\", \"TM (lev)\")\r\n\r\n  plotDF <- data.frame(Desc  = factor(strDesc, levels = strDesc),\r\n                       Type  = c(\"Bridge\", \"Bridge\", \"Bridge\", \"Full\", \"Bridge\", \"Full\", \"Bridge\", \"Bridge\", \"Full\", \"Bridge\", \"Full\"),\r\n                       Start = c(0,\r\n                                 .vec_value_creation_LBO[\"TM_SM_comb\"], \r\n                                 .vec_value_creation_LBO[\"TM_SM_comb\"] + .vec_value_creation_LBO[\"TM_margin\"], \r\n                                 0,\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"], \r\n                                 0,\r\n                                 mult_OpImpr,\r\n                                 mult_OpImpr + .vec_value_creation_LBO[\"TM_mult_EBITDA_comb\"], \r\n                                 0,\r\n                                 .vec_value_creation_LBO[[\"TM_unlev\"]],\r\n                                 0),\r\n                       End   = c(.vec_value_creation_LBO[\"TM_SM_comb\"],\r\n                                 .vec_value_creation_LBO[\"TM_SM_comb\"] + .vec_value_creation_LBO[\"TM_margin\"],\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"],\r\n                                 .vec_value_creation_LBO[\"TM_EBITDA\"],\r\n                                 mult_OpImpr,\r\n                                 mult_OpImpr,\r\n                                 mult_OpImpr + .vec_value_creation_LBO[\"TM_mult_EBITDA_comb\"],\r\n                                 .vec_value_creation_LBO[[\"TM_unlev\"]],\r\n                                 .vec_value_creation_LBO[[\"TM_unlev\"]],\r\n                                 .vec_value_creation_LBO[[\"TM_lev\"]],\r\n                                 .vec_value_creation_LBO[[\"TM_lev\"]]))\r\n  plotDF$Amount <- plotDF$End - plotDF$Start\r\n  plotDF$ID <- 1:nrow(plotDF)\r\n\r\n  #https://stackoverflow.com/questions/50688764/r-ggplot2-ignoring-unknown-aesthetics-with-geom-rect\r\n  suppressWarnings(ggplot(plotDF, aes(x=Desc, fill = Type)) +\r\n                     scale_fill_manual(values = .fillColors) +\r\n                     geom_rect(aes(x=Desc, xmin = ID - 0.45, xmax = ID + 0.45, ymin = Start, ymax = End)) +\r\n                     xlab(\"\") +\r\n                     theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1)))\r\n\r\n}\r\n\r\n\r\n\r\nThe screenshot below summarizes the assumptions.\r\nScreenshot of Panel A in Table 2-1 in Puche’s dissertationPassing the inputs to the function value_creation_LBO() provides us with all the relevant results.\r\n\r\nShow code\r\nlibrary(ggplot2)\r\n##### Use numerical example from Puche (2016, Table 2-1)\r\nlist_results <- value_creation_LBO(.startEquity = 50,\r\n                   .endEquity           = 135,\r\n                   .startDebt           = 50,\r\n                   .endDebt             = 30,\r\n                   .startRev            = 100,\r\n                   .endRev              = 120,\r\n                   .startEBITDA         = 10,\r\n                   .endEBITDA           = 15,\r\n                   .interimCC           = 15,\r\n                   .interimDist         = 20,\r\n                   .interimIntRate      = 0.09,\r\n                   .holdingPeriod       = 4)\r\nkbl(list_results$print_table_outputs,\r\n    caption=\"Results of value creation analysis of Puche's example\") %>%\r\n  kable_classic(full_width = FALSE)\r\n\r\n\r\n\r\nTable 1: Results of value creation analysis of Puche’s example\r\n\r\n\r\nItem\r\n\r\n\r\nAbsolute\r\n\r\n\r\nTM\r\n\r\n\r\nTM (levered)\r\n\r\n\r\nNA\r\n\r\n\r\n1.3846154\r\n\r\n\r\nGain / TM (unlevered)\r\n\r\n\r\n90\r\n\r\n\r\n1.0155336\r\n\r\n\r\nFCF\r\n\r\n\r\n25\r\n\r\n\r\n0.2820927\r\n\r\n\r\nComb. multiple / EBITDA\r\n\r\n\r\n5\r\n\r\n\r\n0.0564185\r\n\r\n\r\nMultiple\r\n\r\n\r\n10\r\n\r\n\r\n0.1128371\r\n\r\n\r\nEBITDA\r\n\r\n\r\n50\r\n\r\n\r\n0.5641853\r\n\r\n\r\nComb. revenue/margin\r\n\r\n\r\n5\r\n\r\n\r\n0.0564185\r\n\r\n\r\nRevenue\r\n\r\n\r\n20\r\n\r\n\r\n0.2256741\r\n\r\n\r\nMargin\r\n\r\n\r\n25\r\n\r\n\r\n0.2820927\r\n\r\n\r\nWith the function plot_value_bridge(), it is also easy to visualize the results.\r\n\r\nShow code\r\nplot_value_bridge(list_results$vec_results)\r\n\r\n\r\n\r\n\r\nArchetypes of LBOs\r\nLet’s run through some examples to better understand how to interpret the results.\r\nGrowing the business\r\nLook no further than Puche’s example above and you see what a an LBO looks like in which all factors of value creation play their role. The EBITDA was grown during ownership, both by top-line growth (Sales effect) and by improving the margins (Margin effect). Overall, this generated a c. 0.6x return. In addition, free cash flows reduced the net debt and enabled a dividend payment, adding another 0.3x. The multiple effect was minor with 0.1x. As 50% of the LBO were financed with debt, the leverage effect added another 0.4x, bringing the overall multiple to 1.4x (2.4x including invested cost).\r\nFinancial engineering\r\nLet’s have an extreme case of financial engineering. As you can see from the table below, the EBITDA of the company only grew by 10% over 4 years. Net debt remained constant, so the cash flow was just enough to pay for taxes, interests, CAPEX, and net working capital. The new buyer wasn’t willing to pay a higher multiple.\r\nNevertheless, the LBO made a 2x return, equally split by the EBITDA increase and the leverage effect. 0.5x increase sounds like a lot for a 10% increase of the EBITDA, but bear in mind that it is in relation to the invested equity. As the invested equity was very low in relation to debt, even small operational improvements on the overall business lead to larger increases in the multiple for equity investors.\r\n\r\nShow code\r\nlist_results <- value_creation_LBO(\r\n                   .startEquity         = 10,\r\n                   .endEquity           = 20,\r\n                   .startDebt           = 90,\r\n                   .endDebt             = 90,\r\n                   .startRev            = 100,\r\n                   .endRev              = 110,\r\n                   .startEBITDA         = 10,\r\n                   .endEBITDA           = 11,\r\n                   .interimCC           = 0,\r\n                   .interimDist         = 0,\r\n                   .interimIntRate      = 0.09,\r\n                   .holdingPeriod       = 4)\r\nkbl(list_results$print_table_inputs,\r\n    caption=\"Inputs of financial engineering example\") %>%\r\n  kable_classic(full_width = FALSE)\r\n\r\n\r\n\r\nTable 2: Inputs of financial engineering example\r\n\r\n\r\nItem\r\n\r\n\r\nEntry\r\n\r\n\r\nInterim\r\n\r\n\r\nExit\r\n\r\n\r\nRevenue\r\n\r\n\r\n100\r\n\r\n\r\nNA\r\n\r\n\r\n110\r\n\r\n\r\nEBITDA\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n11\r\n\r\n\r\nEquity\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n20\r\n\r\n\r\nNet Debt\r\n\r\n\r\n90\r\n\r\n\r\nNA\r\n\r\n\r\n90\r\n\r\n\r\nEV\r\n\r\n\r\n100\r\n\r\n\r\nNA\r\n\r\n\r\n110\r\n\r\n\r\nEV/EBITDA\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n10\r\n\r\n\r\nCC\r\n\r\n\r\nNA\r\n\r\n\r\n0.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nDist\r\n\r\n\r\nNA\r\n\r\n\r\n0.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nHolding period\r\n\r\n\r\nNA\r\n\r\n\r\n4.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nInterest rate p.a.\r\n\r\n\r\nNA\r\n\r\n\r\n0.0900000\r\n\r\n\r\nNA\r\n\r\n\r\nAvg debt/equity ratio\r\n\r\n\r\nNA\r\n\r\n\r\n6.7500000\r\n\r\n\r\nNA\r\n\r\n\r\nCost of debt\r\n\r\n\r\nNA\r\n\r\n\r\n0.4115816\r\n\r\n\r\nNA\r\n\r\nShow code\r\nplot_value_bridge(list_results$vec_results)\r\n\r\n\r\n\r\n\r\nOf course, finance theory 101 tells us that with high leverage comes high risk. To see this, let’s look what would have happened with this deal if the EBITDA would have shrunk by 10% instead of growing. The answer: a full write-off of the equity. Note that the value bridge can’t be calculated anymore as the average debt/equity ratio is undefined. In absolute values though, the loss is fully attributable to the EBITDA decrease.\r\n\r\nShow code\r\n##### Use numerical example from Puche (2016, Table 2-1)\r\nlist_results <- value_creation_LBO(\r\n                   .startEquity         = 10,\r\n                   .endEquity           = 0,\r\n                   .startDebt           = 90,\r\n                   .endDebt             = 90,\r\n                   .startRev            = 100,\r\n                   .endRev              = 90,\r\n                   .startEBITDA         = 10,\r\n                   .endEBITDA           = 9,\r\n                   .interimCC           = 0,\r\n                   .interimDist         = 0,\r\n                   .interimIntRate      = 0.09,\r\n                   .holdingPeriod       = 4)\r\nkbl(list_results$print_table_inputs,\r\n    caption=\"Inputs of financial engineering example in case of underperformance\") %>%\r\n  kable_classic(full_width = FALSE)\r\n\r\n\r\n\r\nTable 3: Inputs of financial engineering example in case of underperformance\r\n\r\n\r\nItem\r\n\r\n\r\nEntry\r\n\r\n\r\nInterim\r\n\r\n\r\nExit\r\n\r\n\r\nRevenue\r\n\r\n\r\n100\r\n\r\n\r\nNA\r\n\r\n\r\n90\r\n\r\n\r\nEBITDA\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n9\r\n\r\n\r\nEquity\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n0\r\n\r\n\r\nNet Debt\r\n\r\n\r\n90\r\n\r\n\r\nNA\r\n\r\n\r\n90\r\n\r\n\r\nEV\r\n\r\n\r\n100\r\n\r\n\r\nNA\r\n\r\n\r\n90\r\n\r\n\r\nEV/EBITDA\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n10\r\n\r\n\r\nCC\r\n\r\n\r\nNA\r\n\r\n\r\n0.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nDist\r\n\r\n\r\nNA\r\n\r\n\r\n0.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nHolding period\r\n\r\n\r\nNA\r\n\r\n\r\n4.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nInterest rate p.a.\r\n\r\n\r\nNA\r\n\r\n\r\n0.0900000\r\n\r\n\r\nNA\r\n\r\n\r\nAvg debt/equity ratio\r\n\r\n\r\nNA\r\n\r\n\r\nInf\r\n\r\n\r\nNA\r\n\r\n\r\nCost of debt\r\n\r\n\r\nNA\r\n\r\n\r\n0.4115816\r\n\r\n\r\nNA\r\n\r\nShow code\r\n### Not meaningful in this case\r\n#plot_value_bridge(list_results$vec_results)\r\n\r\n\r\n\r\nThe multiple expansion miracle\r\nSo far, I have looked at the LBO value bridge from entry to exit. However, nothing prevents me to use the same framework to understand the valuations of unrealized deals of a GP. Instead of plugging the values at exit, I can simply plug in the values at a reporting date, provided by a GP. In an ideal world, these valuations should match the values that would be realized in an actual sale. In reality though, the valuations will likely differ, either because there is some idiosyncratic valuation error by the GP or because there are some more systematic valuation mistakes. One reason could be that a GP is conservative in their valuations so that there are positive surprises for the investors. Another one is that some GPs might inflate their valuations to increase their chances in fundraising. This hypothesis was tested and confirmed by a paper from Brown, Gredil and Kaplan.\r\nLook at the below example. It is hard to imagine that an LBO actually ends up with such a value bridge at realization. The EBITDA actually decreased over the four-year holding period and the debt paydown has been very minor. Nevertheless, the unlevered / levered multiple is 0.5x / 0.6x, indicating that meaningful value creation has happened. As EBITDA growth and FCF effect are out of the equation, the only other factor, the EV/EBITDA multiple change, has to explain this value creation. In this example, it jumped from 10x to 14x, which is difficult to justify with the operational development the firm has taken.\r\n\r\nShow code\r\n##### Use numerical example from Puche (2016, Table 2-1)\r\nlist_results <- value_creation_LBO(\r\n                   .startEquity         = 50,\r\n                   .endEquity           = 80,\r\n                   .startDebt           = 50,\r\n                   .endDebt             = 45,\r\n                   .startRev            = 100,\r\n                   .endRev              = 95,\r\n                   .startEBITDA         = 10,\r\n                   .endEBITDA           = 9,\r\n                   .interimCC           = 0,\r\n                   .interimDist         = 0,\r\n                   .interimIntRate      = 0.09,\r\n                   .holdingPeriod       = 4)\r\nkbl(list_results$print_table_inputs,\r\n    caption=\"Inputs of multiple expansion example\") %>%\r\n  kable_classic(full_width = FALSE)\r\n\r\n\r\n\r\nTable 4: Inputs of multiple expansion example\r\n\r\n\r\nItem\r\n\r\n\r\nEntry\r\n\r\n\r\nInterim\r\n\r\n\r\nExit\r\n\r\n\r\nRevenue\r\n\r\n\r\n100\r\n\r\n\r\nNA\r\n\r\n\r\n95.00000\r\n\r\n\r\nEBITDA\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n9.00000\r\n\r\n\r\nEquity\r\n\r\n\r\n50\r\n\r\n\r\nNA\r\n\r\n\r\n80.00000\r\n\r\n\r\nNet Debt\r\n\r\n\r\n50\r\n\r\n\r\nNA\r\n\r\n\r\n45.00000\r\n\r\n\r\nEV\r\n\r\n\r\n100\r\n\r\n\r\nNA\r\n\r\n\r\n125.00000\r\n\r\n\r\nEV/EBITDA\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\n13.88889\r\n\r\n\r\nCC\r\n\r\n\r\nNA\r\n\r\n\r\n0.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nDist\r\n\r\n\r\nNA\r\n\r\n\r\n0.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nHolding period\r\n\r\n\r\nNA\r\n\r\n\r\n4.0000000\r\n\r\n\r\nNA\r\n\r\n\r\nInterest rate p.a.\r\n\r\n\r\nNA\r\n\r\n\r\n0.0900000\r\n\r\n\r\nNA\r\n\r\n\r\nAvg debt/equity ratio\r\n\r\n\r\nNA\r\n\r\n\r\n0.7812500\r\n\r\n\r\nNA\r\n\r\n\r\nCost of debt\r\n\r\n\r\nNA\r\n\r\n\r\n0.4115816\r\n\r\n\r\nNA\r\n\r\nShow code\r\nkbl(list_results$print_table_outputs,\r\n    caption=\"Results of value creation analysis of multiple expansion example\") %>%\r\n  kable_classic(full_width = FALSE)\r\n\r\n\r\n\r\nTable 4: Results of value creation analysis of multiple expansion example\r\n\r\n\r\nItem\r\n\r\n\r\nAbsolute\r\n\r\n\r\nTM\r\n\r\n\r\nTM (levered)\r\n\r\n\r\nNA\r\n\r\n\r\n0.6000000\r\n\r\n\r\nGain / TM (unlevered)\r\n\r\n\r\n30.0000000\r\n\r\n\r\n0.5173604\r\n\r\n\r\nFCF\r\n\r\n\r\n5.0000000\r\n\r\n\r\n0.0862267\r\n\r\n\r\nComb. multiple / EBITDA\r\n\r\n\r\n-3.8888889\r\n\r\n\r\n-0.0670652\r\n\r\n\r\nMultiple\r\n\r\n\r\n38.8888889\r\n\r\n\r\n0.6706523\r\n\r\n\r\nEBITDA\r\n\r\n\r\n-10.0000000\r\n\r\n\r\n-0.1724535\r\n\r\n\r\nComb. revenue/margin\r\n\r\n\r\n0.2631579\r\n\r\n\r\n0.0045382\r\n\r\n\r\nRevenue\r\n\r\n\r\n-5.0000000\r\n\r\n\r\n-0.0862267\r\n\r\n\r\nMargin\r\n\r\n\r\n-5.2631579\r\n\r\n\r\n-0.0907650\r\n\r\nShow code\r\nplot_value_bridge(list_results$vec_results)\r\n\r\n\r\n\r\n\r\nIn such situations, it is important for an investor to question the valuation of the GP and to get a satisfying justification for a multiple increase despite decreasing EBITDA. Interestingly, Brown, Gredil and Kaplan show in their paper that investors are pretty good in identifying GPs that value their investments too aggressively, as such GPs are less successful in fundraising.\r\nSummary\r\nIn this blog post, I have shown how to break up the total net equity proceeds or gains into different elements of value creation: operational improvements, i.e., EBITDA growth; multiple expansion; free cash flow generation; and leverage.\r\nI have derived the formulas, implemented them in R and run through a few examples.\r\nThe LBO value bridge is a very important tool to understand how value was created for fully realized deals and to justify the valuations for unrealized deals.\r\n\r\nFor simplicity, I’m ignoring transaction costs. Furthermore, I do not go into detail about the ownership structure, which is another complication. For example, a fund’s total net gains might be lower because the management has received an equity as part of an equity incentive plan and, as a consequence, their pro-rata share of the total net proceeds was reduced over the holding period.↩︎\r\nI follow Pindur’s notation here, see p. 69ff.↩︎\r\nEssentially, this means we can ignore the last two elements of the last equation…I know, not really a leveraged buyout…↩︎\r\nAgain, the derivations are more general and can be applied to any cash flow measure, but I focus on the EBITDA as the by far the most prominent measure in private equity.↩︎\r\nFor simplicity, I ignore interest on the debt.↩︎\r\nThat’s why the sum starts now at \\(t\\), not \\(t+1\\).↩︎\r\nNote that the Miller/Modigliani theorem is derived for single period return, not multi-period multiples. Hence, the above formula is a simplification, a topic that is for example discussed in a paper by my PhD advisor.↩︎\r\nI also further split up the EBITDA effect in a revenue growth and margin effect. The calculations are in spirit the same as the ones to split up the total net proceeds \\(TNP\\) and straightforward. Hence, I do not show them.↩︎\r\n",
    "preview": "posts/2021-01-31-the-value-bridge/the-value-bridge_files/figure-html5/Puche_example_plot-1.png",
    "last_modified": "2021-02-08T18:02:28+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-17-evebitda-valuation-multiple-the-data/",
    "title": "EV/EBITDA valuation multiple: some data",
    "description": "A closer empirical look at the EV/EBITDA multiple.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-01-25",
    "categories": [
      "Valuation",
      "Data"
    ],
    "contents": "\r\n\r\nContents\r\nDownload and formatting the dataPreparation of the data\r\nCheck that different currencies are adjusted correctly\r\nFormatting of data\r\n\r\nDescriptive statistics\r\nEV/EBITDA multiples by sector\r\nDrivers of valuation multiplesOperative cash conversion\r\n\r\nSummary\r\n\r\nIn the blog post EV/EBITDA valuation multiple: the theory, I derived the EV/EBITDA multiple from a FCFF-DCF model under some simplifying assumptions. In this post, I want to dive deeper into some empirical characteristics of the EV/EBITDA multiple. To do so, I use the RB1 Robur data set available on Quandl, which is not freely available.\r\nIn the section “Download and formatting the data”, I describe how I got the data into R. Readers that are only interested in the results can skip this section. As the documentation of the data set is rather extensive, I will not go into too much detail about it in this post (which companies are covered, for how long, etc.).\r\nDownload and formatting the data\r\nI downloaded the data from Quandl on 10/24/2020. I struggled to pull all the data directly into one data set with the R package, so I ran a for loop to download the data sets for each individual ticker, which ran half a day. The code is saved below for reference, but there is probably an easier and better way to do this. If you know it, let me know!\r\n\r\nShow code\r\nlibrary(Quandl)\r\n#You also have to set your personal Quandl key with Quandl.api_key('ENTER YOUR KEY')\r\nlibrary(data.table)\r\n#### Download RB1 full data set\r\n\r\ntickers <- as.character(1:9197)\r\ntickers[nchar(tickers)==1] <- paste0(\"000\", tickers[nchar(tickers)==1])\r\ntickers[nchar(tickers)==2] <- paste0(\"00\",  tickers[nchar(tickers)==2])\r\ntickers[nchar(tickers)==3] <- paste0(\"0\",   tickers[nchar(tickers)==3])\r\n\r\ni <- 0\r\nfor (ticker in tickers) { \r\n  for (str_type in c(\"INCOME\", \"CAGR\", \"SIGNALS\", \"VALUES\",\r\n                     \"FORECAST\", \"BALANCE\", \"CASHFLOW\")) {\r\n    \r\n    if (i==0) {\r\n      str_eval <- paste0(\"dt_\", str_type, \"<- as.data.table(Quandl('RB1/\", ticker, \"_\", str_type, \"'))\")\r\n      eval(parse(text=str_eval))\r\n      set(eval(parse(text=paste0(\"dt_\", str_type))), j=\"Ticker\", value=ticker)\r\n    } else {\r\n      tryCatch({\r\n        str_eval <- paste0(\"intDT <- as.data.table(Quandl('RB1/\", ticker, \"_\", str_type, \"'))\")\r\n        eval(parse(text=str_eval))\r\n        set(intDT, j=\"Ticker\", value=ticker)\r\n        eval(parse(text=paste0(\"dt_\", str_type, \"<- rbind(intDT, dt_\", str_type, \")\")))\r\n      }, error=function(e){cat(\"ERROR :\",ticker, conditionMessage(e), \"\\n\")})\r\n    }\r\n\r\n  }\r\n  i <- i + 1\r\n}\r\n\r\nl_RB1 <- list(dt_BALANCE  = dt_BALANCE,\r\n              dt_CAGR     = dt_CAGR,\r\n              dt_CASHFLOW = dt_CASHFLOW,\r\n              dt_INCOME   = dt_INCOME,\r\n              dt_FORECAST = dt_FORECAST,\r\n              dt_SIGNALS  = dt_SIGNALS,\r\n              dt_VALUES   = dt_VALUES)\r\n#Change the folder based on your computer set up\r\nsave(l_RB1, file=\"C:/Users/Christoph Jaeckel/Desktop/Quandl/list_RB1.RData\")\r\n\r\n\r\n\r\nAfter this code has run once, I can now load the data easily with the following code. The different tables are saved in a list named l_RB1. In addition, there is a table that links the tickers with the companies, industries, etc.\r\n\r\nShow code\r\nlibrary(data.table)\r\nlibrary(lubridate)\r\nlibrary(Quandl)\r\nlibrary(ggplot2)\r\n#Quandl.api_key('ENTER YOUR KEY') #We will need this later to get FX rates from Quandl\r\nload(\"C:/Users/Christoph Jaeckel/Desktop/Quandl/list_RB1.RData\")\r\nDT <- fread(\"C:/Users/Christoph Jaeckel/Desktop/Quandl/RB1_tickers.csv\")\r\n\r\n\r\n\r\nPreparation of the data\r\nAfter inspecting the different data sets, it became clear that dt_VALUE, which seemed the most important initially, is rather useless as it seems to only hold the valuations from 2020, so I could not do an analysis over time. However, fortunately, dt_INCOME includes information about the share price “at the accounting date for each period” as well as the diluted shares outstanding. Hence, I can calculate the market cap. dt_BALANCE has information about the debt and dt_CASHFLOW gives me information about the depreciation and cash. As dt_INCOME also includes operating income (equivalent of EBIT), I can therefore easily calculate the EBITDA. I also merge dt_CAGR, which includes growth rates for companies; however, there is much less coverage so using those inputs will substantially reduce the sample size.\r\nLet’s merge the four data sets together.\r\n\r\nShow code\r\n### Start with dt_INCOME\r\nintDT <- l_RB1$dt_INCOME\r\n### Merge it with dt_CASHFLOW and dt_BALANCE\r\nsetkey(intDT, Ticker, Date)\r\n#dt_CASHFLOW\r\nsetkey(l_RB1$dt_CASHFLOW, Ticker, Date)\r\nintDT <- intDT[l_RB1$dt_CASHFLOW]\r\n#dt_BALANCE\r\nsetkey(l_RB1$dt_BALANCE, Ticker, Date)\r\nintDT <- intDT[l_RB1$dt_BALANCE]\r\n#dt_CAGR (not required, so that's why you have it outside)\r\nsetkey(l_RB1$dt_CAGR, Ticker, Date)\r\nintDT <- l_RB1$dt_CAGR[intDT]\r\n### Finally, get also static data in there\r\nintDT[, Ticker:=as.integer(Ticker)] #Change type of Ticker as it is integer in the DT data set\r\nsetkey(intDT, Ticker)\r\nsetnames(DT, \"company_code\", \"Ticker\")\r\nsetkey(DT, Ticker)\r\nDT <- DT[intDT]\r\nmarketCap_Arctic <- round(DT[Ticker==8971 & Date==dmy(\"31122019\"), `Share Price at EoP` * `Diluted Shares OS`])\r\n\r\n\r\n\r\nCheck that different currencies are adjusted correctly\r\nI looked closer into Arctic Paper which has a share currency of SEK and a reference currency of PLN. I confirmed with CapitalIQ that the balance sheet items were in PLN. According the company page on Quandl, the market cap of this company is currently USD 0.07bn. I calculate a market cap of 593 as of 12/31/2019. Note that the stock price didn’t move much in 2020 and that the current SEK/USD FX rate is 0.11. Hence, I would calculate a market cap of 65.23, which is exactly what I find at the company page. The PLN/USD FX rate stands at 0.26 and would therefore not match.\r\nTo summarize: It looks like all values are in the reference currency with the exception of the share price, which is in the share currency. I will convert all numbers to a consistent currency in the next section.\r\nFormatting of data\r\nIn a next step, I make some adjustments to the data set, i.e. rename some columns and calculate some items such as EBITDA. Concretely:\r\nRename the following columns:\r\nOperating Income to EBIT\r\nShare Price at EoP to Share_price\r\nDiluted Shares OS to Nr_shares\r\nEnd Cash to Cash_end\r\nCurrent Assets to Current_assets\r\nCash from Operations to OpCF\r\nCash from Financing to FinCF\r\nCash from Investing to InvCF\r\nConvert currencies: See the section above, in which I provide evidence that all values are in reference currency except for the share price, which is in share currency. I convert everything to USD.\r\nCalculate EBITDA as EBIT / Operating Income plus depreciation.\r\nCalculate net debt as the sum of total short-term and long-term debt minus cash.\r\nMarket cap as share price multiplied with number of shares.\r\nLink countries to continents\r\nThe code to get the FX rate is a bit more complicated. I start by obtaining an FX data set from the Swiss national bank, which is freely available on Quandl. This data set has the dates in the rows and the FX rate of the Swiss francs to other currencies in each column starting at column 2. Some FX rates are scaled by 100, such as SEK or JPY. Fortunately, in these cases, the column header ends with “100”, so I can easily check for it and divide these columns by 100. I then convert each currency to USD and finally bring the data set into the long format.\r\n\r\nShow code\r\n############################################################################\r\n############################## 1. Rename columns\r\nsetnames(DT,\r\n         old = c(\"Operating Income\", \"Share Price at EoP\", \"Diluted Shares OS\", \"End Cash\", \"Current Assets\",\r\n                 \"Cash from Operations\", \"Cash from Financing\", \"Cash from Investing\"),\r\n         new = c(\"EBIT\",             \"Share_price\",        \"Nr_shares\",         \"Cash_end\", \"Current_assets\",\r\n                 \"OpCF\",                  \"FinCF\",              \"InvCF\"))\r\n############################################################################\r\n############################## 2. Convert FX rates\r\n\r\n#--------------------- a) Get an FX rate matching table\r\nfxDT    <- as.data.table(Quandl(\"SNB/DEVKUM\"))\r\n# Only keep end of month data\r\ncols <- c(\"Date\", grep(\"End of month\", names(fxDT), value = TRUE))\r\nfxDT <- fxDT[, ..cols]\r\n# Loop through columns to adjust name and scale \r\nfor (j in 2:ncol(fxDT)) {\r\n  \r\n  #If there is a 100, scale FX rate accordingly\r\n  col_name <- names(fxDT)[j]\r\n  if (substr(col_name, start = nchar(col_name)-2, stop = nchar(col_name))==\"100\") {\r\n    set(fxDT, j=j, value = fxDT[,..j]/100)\r\n    #Also, get the currency code\r\n    new_col_name <- substr(col_name, start = nchar(col_name)-6, stop = nchar(col_name)-4)\r\n  } else {\r\n    #If it doesn't end with 100, this looks different\r\n    new_col_name <- substr(col_name, start = nchar(col_name)-4, stop = nchar(col_name)-2)\r\n  }\r\n  setnames(fxDT, old=col_name, new=new_col_name)\r\n}\r\n#Loop again through it to convert to USD\r\nfxUSD <- fxDT$USD\r\nfor (j in 2:ncol(fxDT)) {\r\n  set(fxDT, j=j, value = fxDT[,..j]/fxUSD)\r\n}\r\n#Add CHF (needed as you get the data from the SNB and hence everything so far was linked to the CHF)\r\nfxDT[, CHF:=1/fxUSD]\r\n\r\n#--------------------- b) Melt and merge\r\nfxDT <- melt(fxDT, id.vars=\"Date\")\r\nsetkey(fxDT, Date, variable)\r\n#Exclude ARS because the data looks wrong\r\n#ggplot(fxDT[variable==\"ARS\"], aes(x=Date, y=value)) + geom_line()\r\nfxDT <- fxDT[variable!=\"ARS\"]\r\n### First with share_currency\r\nsetkey(DT, Date, share_currency)\r\nDT <- fxDT[DT]\r\nsetnames(DT, c(\"variable\", \"value\"), c(\"share_currency\", \"fx_share_CUR_to_USD\"))\r\nDT[, Share_price := Share_price * fx_share_CUR_to_USD]\r\n### First with reference_currency\r\nsetkey(DT, Date, reference_currency)\r\nDT <- fxDT[DT]\r\nsetnames(DT, c(\"variable\", \"value\"), c(\"reference_currency\", \"fx_reference_CUR_to_USD\"))\r\nfor (cols in c(\"Revenue\", \"EBIT\", \"Net Income exc. extra\", \"Net Income inc. extra\", \"OpCF\",\r\n               \"Depreciation\", \"Capex\", \"InvCF\", \"Issuance of Stock\", \"Issuance of Debt\", \"FinCF\",\r\n               \"Start Cash\", \"Cash_end\", \"Current_assets\", \"Goodwill\", \"Intangibles\", \"Total Assets\", \"Current Liabilities\",\r\n               \"Long Term Debt\", \"Total Liabilities\", \"Shareholder Equity\")) {\r\n  \r\n  set(DT, j=cols, value = DT[,..cols] * DT$fx_reference_CUR_to_USD)  \r\n\r\n}\r\n\r\n############################################################################\r\n############################## 3. Calculate EBITDA\r\nDT[, EBITDA := EBIT + Depreciation]\r\n############################################################################\r\n############################## 4. Calculate Net Debt\r\nDT[, Net_debt := `Current Liabilities` + `Long Term Debt` - Cash_end]\r\n############################################################################\r\n############################## 5. Market cap\r\nDT[, Market_cap := Share_price * Nr_shares]\r\n############################################################################\r\n############################## 6. EV/EBITDA multiple\r\nDT[,EV        := Market_cap + Net_debt]\r\nDT[,EV_EBITDA := EV/EBITDA]\r\nDT[!is.finite(EV_EBITDA), EV_EBITDA:=NA]\r\n############################################################################\r\n############################## 7. Continent matching\r\ncountryDT <- fread(\"C:/Users/Christoph Jaeckel/Desktop/Quandl/Country_continent_list.csv\")\r\n#Get rid of everything after the comma: https://stackoverflow.com/questions/25307899/r-remove-anything-after-comma-from-column\r\ncountryDT[, Country_Name:=gsub(\"(.*),.*\", \"\\\\1\", Country_Name)]\r\ncountryDT <- countryDT[, list(Country_Name, Continent_Name)]\r\nsetkey(countryDT, Country_Name)\r\nsetkey(DT, country)\r\nDT <- countryDT[DT, allow.cartesian=TRUE]\r\n#Some didn't match, change manually\r\n#DT[is.na(Continent_Name), unique(Country_Name)]\r\nDT[Country_Name==\"China inc HK\",     Continent_Name:=\"Asia\"]\r\nDT[Country_Name==\"Falkland Islands\", Country_Name:=\"UK\"]\r\nDT[Country_Name==\"UK\",               Continent_Name:=\"Europe\"]\r\nDT[Country_Name==\"Russia\",           Continent_Name:=\"Asia\"]\r\nDT[Country_Name==\"USA\",              Continent_Name:=\"North America\"]\r\n\r\n\r\n\r\nDescriptive statistics\r\nOnce we have obtained the data, we can finally look at it. Let’s start by looking at some summary statistics.\r\n\r\nShow code\r\nlibrary(gtsummary)\r\n# make dataset with a few variables to summarize\r\ntrial2 <- trial %>% select(age, grade, response, trt)\r\n\r\ntbl_summary(\r\n    data=DT[, list(Continent_Name, Market_cap, Net_debt, EBITDA, Revenue, EV_EBITDA)],\r\n    by = Continent_Name, # split table by group,\r\n    digits = starts_with(\"EV\") ~ 1,\r\n    label = list(Market_cap ~ \"Market cap\", Net_debt ~ \"Net debt\", EV_EBITDA ~ \"EV/EBITDA\"),\r\n    statistic = all_continuous() ~ \"{min}  / {median} / {mean} / {max} ({sd})\",\r\n    missing = \"no\" # don't list missing data separately\r\n  ) %>%\r\n  add_n() %>% # add column with total number of non-missing observations\r\n  modify_header(label = \"**Variable**\") %>% # update the column header\r\n  bold_labels() # %>%\r\n\r\n\r\nVariable\r\n      N\r\n      Africa, N = 1,1461\r\n      Asia, N = 16,8761\r\n      Europe, N = 33,6601\r\n      North America, N = 11,9911\r\n      Oceania, N = 4,8081\r\n      South America, N = 5941\r\n    Market cap\r\n      63,098\r\n      0  / 1,282 / 4,567 / 356,006 (16,874)\r\n      0  / 2,725 / 7,710 / 924,083 (22,174)\r\n      0  / 215 / 3,357 / 1,403,390 (17,101)\r\n      0  / 4,615 / 16,495 / 1,128,825 (46,860)\r\n      0  / 357 / 2,746 / 186,590 (10,749)\r\n      1  / 2,098 / 7,480 / 148,157 (17,496)\r\n    Net debt\r\n      48,802\r\n      -2,945  / 592 / 1,469 / 334,068 (11,599)\r\n      -127,506  / 1,571 / 6,250 / 616,550 (17,992)\r\n      -162,770  / 160 / 2,895 / 1,066,106 (20,156)\r\n      -51,648  / 2,054 / 6,630 / 1,413,126 (20,466)\r\n      -1,183  / 173 / 1,016 / 73,204 (3,245)\r\n      0  / 1,816 / 8,973 / 967,805 (55,408)\r\n    EBITDA\r\n      60,494\r\n      -745  / 151 / 407 / 6,484 (817)\r\n      -3,740  / 384 / 1,380 / 76,832 (4,442)\r\n      -24,757  / 26 / 499 / 75,952 (2,301)\r\n      -16,690  / 557 / 1,850 / 139,407 (5,048)\r\n      -4,956  / 39 / 346 / 36,645 (1,662)\r\n      -153  / 340 / 3,176 / 557,907 (30,150)\r\n    Revenue\r\n      59,826\r\n      0  / 1,015 / 2,235 / 53,302 (3,433)\r\n      0  / 2,395 / 8,631 / 471,268 (22,306)\r\n      -678  / 194 / 3,225 / 484,748 (15,030)\r\n      -31  / 3,328 / 10,460 / 510,329 (27,061)\r\n      0  / 188 / 1,400 / 71,330 (5,001)\r\n      0  / 1,481 / 16,988 / 3,317,699 (179,287)\r\n    EV/EBITDA\r\n      45,380\r\n      -4,301.0  / 10.3 / 6.9 / 614.2 (172.4)\r\n      -7,428.6  / 11.1 / 59.6 / 453,332.3 (4,493.2)\r\n      -18,141.0  / 10.7 / 21.3 / 88,542.6 (686.7)\r\n      -10,681.5  / 11.8 / 12.9 / 5,872.3 (157.0)\r\n      -733.8  / 10.4 / 13.9 / 4,860.3 (106.7)\r\n      -283.7  / 11.2 / 14.3 / 333.3 (28.7)\r\n    \r\n        \r\n          1\r\n          \r\n           \r\n          Minimum  / Median / Mean / Maximum (SD)\r\n          \r\n      \r\n    \r\nShow code\r\n  #as_kable_extra() %>%\r\n  #kable_classic()\r\n\r\n\r\n\r\nThe table above already allows for some interesting observations:\r\nThere are large size differences between the companies across the continents. For example, while North American companies have a market cap of over USD 4.5 billion in the median case, it is just above USD 200 million for the European sample. On the information page for this data set, it is mentioned that “coverage emphasizes Asia/Europe”, which is most likely the explanation for the large differences.\r\nThere are quite a few outliers in the EV/EBITDA multiple, so going forward I filter observations for which the EV/EBITDA multiple is either negative, above 100x or not available.\r\n\r\nShow code\r\nDT <- DT[EV_EBITDA>0 & EV_EBITDA <100 & !is.na(EV_EBITDA)]\r\n\r\n\r\n\r\nLet’s also look at the distributions, scatter plots and correlations of some of the key variables.\r\n\r\nShow code\r\nlibrary(GGally)\r\nggpairs(DT[!is.na(log(Market_cap)) & !is.na(log(Net_debt)) &\r\n           !is.na(log(EBITDA)) & !is.na(log(Revenue)) & !is.na(EV_EBITDA) & \r\n           is.finite(log(Market_cap)) & is.finite(log(Net_debt)) & is.finite(log(Revenue)),\r\n           list(Market_cap_log = log(Market_cap), \r\n                 Net_debt_log   = log(Net_debt), \r\n                 EBITDA_log     = log(EBITDA), \r\n                 Revenue_log    = log(Revenue), \r\n                 EV_EBITDA)])\r\n\r\n\r\n\r\n\r\nNot surprisingly, there is a strong correlation between the market cap, net debt, EBITDA and Revenue. These values are in absolute numbers and a company that has 10x larger revenues than another company is very likely to have a much higher EBITDA, even if it is less profitable.\r\nMost of the observations for the EV/EBITDA multiple are in the range from 0x to 25x; however, there is a long right-tail that goes all the way up to 100x.\r\nEV/EBITDA multiples by sector\r\nNext, I look at the EV/EBITDA multiples per industry or sector. I thereby focus on the multiple range from 0x to 30x and focus on Oceania, Europe, Asia, North America.\r\n\r\nShow code\r\nggplot(data = DT[!is.na(Continent_Name) & Continent_Name!=\"Africa\" & Continent_Name!=\"South America\"], \r\n       aes(x = reorder(sector_group, EV_EBITDA, na.rm=TRUE), y = EV_EBITDA, )) + #fill = Continent_Name)) +\r\n geom_boxplot() + ylim(c(0,30))  + ylab(\"EV/EBITDA\") + xlab(\"Sector\") + coord_flip()\r\n\r\n\r\n\r\n\r\nFor me, the biggest take-away from this chart is the large intra-sector dispersion: even if we only look at the 25th to 75th percentile (the lower and upper end of the rectangles), we have a spread of at least 5x, but more often around 10x. The 75th percentile company of one of the sectors with the lowest multiples overall, telecommunications, has around the same multiple as the 25th percentile of the highest multiple sector, healthcare.\r\nFor valuing companies, this means that the sector alone is not sufficient to estimate reasonable multiples. Don’t make the mistake to assume that a company should be valued at a double-digit multiple, simply because it is a healthcare company. And as we know from Tesla, just because a company is in the vehicles sector does not necessarily mean that it must trade at a single-digit multiple.\r\n\r\nLet’s not start the discussion here at what multiple Tesla should be valued…maybe another time.\r\nAt the very least, it is important to link the EV/EBITDA multiple back to a more comprehensive free cash flow valuation: Is the company cash generative? What are the growth prospects? Do you expect large net working capital changes? And what are the tax rates in relation to peers? More on this further below.\r\nNext, let’s look at the multiple development over time. Before we show the median development, let’s first check that we have enough observations per year/sector. The chart below shows the number of data points.\r\n\r\nShow code\r\n#https://stackoverflow.com/questions/38722202/how-do-i-change-the-number-of-decimal-places-on-axis-labels-in-ggplot2/53362011\r\nscaleFUN <- function(x) sprintf(\"%.0f\", x)\r\n#https://stackoverflow.com/questions/14290364/heatmap-with-values-ggplot2\r\n## plot data\r\nggplot(DT[,.N,by=list(Year=year(Date), sector_group)], aes(Year, sector_group)) +\r\n    geom_tile(aes(fill = N)) + \r\n    geom_text(aes(label = N)) +\r\n    ylab(\"Sector\") +\r\n    scale_x_continuous(labels=scaleFUN) + \r\n    scale_fill_gradient(low = \"white\", high = \"blue\")\r\n\r\n\r\n\r\n\r\nThe table reveals that for 2010 and 2020, there are not many observations for some sectors and hence the results could be a bit misleading. I therefore focus on the period from 2011 to 2019.\r\n\r\nShow code\r\nlibrary(directlabels)\r\nggplot(DT[year(Date)>2010 & year(Date)<2020, list(Median = median(EV_EBITDA, na.rm=TRUE)),by=list(sector_group, Year=year(Date))],\r\n       aes(x=Year, y=Median, color=sector_group)) + geom_line()  +\r\n  scale_colour_discrete(guide = 'none') + \r\n  scale_x_continuous(labels=scaleFUN, expand = c(0.2, 0)) + \r\n  geom_dl(aes(label = sector_group), method = list(dl.combine(\"first.points\", \"last.points\"), cex = 0.7)) \r\n\r\n\r\n\r\n\r\nBy and large, the sector multiples saw a simultaneous increase over the last decade, but there are also some changes in the ordering. For example, healthcare and IT have seen large increases in their median multiples over the last decade, catapulting them to the top, while construction saw a decline.\r\nDrivers of valuation multiples\r\nIn my opinion, the aggregation to sectors is just a shortcut of deducting valuation multiples. Software companies by and large have high cash conversion, and as high cash conversion is linked with high multiples, one can argue that a software company should trade for a high multiple. Of course, as the analysis above has shown, this is a dangerous deduction. In the case in which a company is classified as a software company but does not have high a cash conversions for some reason, simply assuming a mean or median software multiple leads to upward biased valuations.\r\nIn the blog post EV/EBITDA valuation multiple: the theory, I derived the EV/EBITDA multiple from a FCFF-DCF model under some simplifying assumptions. I showed that the EV/EBITDA multiple should the higher,\r\nthe higher the operational cash conversion (OCC);\r\nthe higher the growth of the business;\r\nthe lower the tax rate; and\r\nthe lower the discount rate, i.e., the risk of the business.\r\nWith the data set I have at hand, I can unfortunately not look into most of these factors. However, as the data set includes information about the CAPEX, D&A and operational cash flow, I can at least investigate the operational cash conversion.\r\n\r\nMaybe I find a more comprehensive and easier to obtain data set in the future, in which case I will write another post. Unfortunately, most data analysis starts with data collection, which is still cumbersome, despite huge improvements over the years thanks to offers such as Quandl.\r\nOperative cash conversion\r\nLet’s first compare the CAPEX and the depreciation in the data set. Ideally, the two are highly correlated, so I can use one for the other and the assumption that D&A and CAPEX measure the same thing is reasonable.\r\n\r\nShow code\r\n### Calculate correlation in relation to revenues\r\nDT[, Capex:=-Capex]\r\nDT[,DA_Rev    := Depreciation/Revenue]\r\nDT[,Capex_Rev := Capex/Revenue]\r\n#TODO make prettier\r\nstatDT <-  DT[, lapply(.SD, quantile, probs=c(0.05,0.25,0.5,0.75,0.95),                \r\n                         na.rm=TRUE),\r\n                .SDcols=c(\"Depreciation\", \"Capex\", \"DA_Rev\", \"Capex_Rev\")]\r\nstatDT <- cbind(data.table(Percentile = c(\"5th\",\r\n                                          \"25th\",\r\n                                          \"Median\",\r\n                                          \"75th\",\r\n                                          \"95th\")),\r\n                statDT)\r\nstatDT[, c(\"Depreciation\", \"Capex\"):=list(format(Depreciation, \r\n                                                 digits=1, \r\n                                                 big.mark=\",\",\r\n                                                 scientific = FALSE),\r\n                                          format(Capex, \r\n                                                 digits=1, \r\n                                                 big.mark=\",\",\r\n                                                 scientific = FALSE))]\r\nstatDT[, c(\"DA_Rev\", \"Capex_Rev\"):=list(paste0(format(DA_Rev*100, \r\n                                                 digits=1), \"%\"),\r\n                                        paste0(format(Capex_Rev*100, \r\n                                                 digits=1), \"%\"))]\r\nsetnames(statDT, \r\n         c(\"Depreciation\", \"DA_Rev\", \"Capex_Rev\"),\r\n         c(\"D&A\",          \"D&A/Rev\",    \"Capex/Rev\"))\r\n\r\nlibrary(kableExtra)\r\nstatDT %>%\r\n  kbl(caption = \"Percentiles for D&A and Capex (absolute in USD million and in relation to revenues)\") %>%\r\n  kable_classic(full_width = FALSE) \r\n\r\n\r\n\r\nTable 1: Percentiles for D&A and Capex (absolute in USD million and in relation to revenues)\r\n\r\n\r\nPercentile\r\n\r\n\r\nD&A\r\n\r\n\r\nCapex\r\n\r\n\r\nD&A/Rev\r\n\r\n\r\nCapex/Rev\r\n\r\n\r\n5th\r\n\r\n\r\n0.6\r\n\r\n\r\n0.8\r\n\r\n\r\n0.5%\r\n\r\n\r\n0.5%\r\n\r\n\r\n25th\r\n\r\n\r\n9.5\r\n\r\n\r\n12.2\r\n\r\n\r\n2.2%\r\n\r\n\r\n2.2%\r\n\r\n\r\nMedian\r\n\r\n\r\n60.0\r\n\r\n\r\n77.1\r\n\r\n\r\n4.1%\r\n\r\n\r\n4.6%\r\n\r\n\r\n75th\r\n\r\n\r\n249.2\r\n\r\n\r\n319.7\r\n\r\n\r\n7.7%\r\n\r\n\r\n10.5%\r\n\r\n\r\n95th\r\n\r\n\r\n1,671.2\r\n\r\n\r\n2,151.6\r\n\r\n\r\n23.1%\r\n\r\n\r\n48.2%\r\n\r\n\r\nThere is quite a strong relationship between the two with a correlation of 92.6% between CAPEX/revenues and D&A/revenues. I use normalized numbers, i.e. in relation to revenues, otherwise I might simply pick up a size effect. A company that is ten times as large as another company will have both much higher CAPEX and depreciation, even if the two variables are not strongly correlated. That’s why looking at the normalized numbers is so important.\r\nNext, let’s look at the scatterplot. There are so many observations that I have to make each point transparent with alpha blending. That way the regions with more observations appear darker, revealing a strong correlation between the two. Furthermore, I limit the plotting area from 0% to 10%, which holds most of the observations. This is another interesting result for me. CAPEX/D&A rarely makes up more than 10% of a company’s revenues, if it does, better check why.\r\n\r\nShow code\r\n#https://stackoverflow.com/questions/7714677/scatterplot-with-too-many-points\r\nggplot(DT[is.finite(DA_Rev) & is.finite(Capex_Rev) & DA_Rev<1 & DA_Rev>0 & Capex_Rev<1 & Capex_Rev>0, list(DA_Rev, Capex_Rev)],\r\n       aes(x=DA_Rev, y=Capex_Rev)) + geom_point(alpha=0.05)  +\r\n  scale_x_continuous(limits = c(0,0.1), labels = scales::percent) + scale_y_continuous(limits = c(0,0.1), labels = scales::percent) +\r\n  xlab(\"Depreciation in relation to revenues\") + ylab(\"CAPEX in relation to revenues\")\r\n\r\n\r\n\r\n\r\nLet’s now look at the relationship of the OCC, defined as 1 minus D&A divided by EBITDA, in comparison to the EV/EBITDA multiple. As we have the depreciation directly and as it looks a bit smoother than the CAPEX, I will use the depreciation in the analysis as the input to calculate the OCC. I will also require the following:\r\n\r\nShow code\r\nDT[EBITDA>0 & Depreciation>0 & EBITDA>50, OCC := pmax(0,pmin(1, 1-Depreciation/EBITDA))]\r\ncor_DA_Rev_OCC <- DT[is.finite(DA_Rev) & is.finite(OCC) & DA_Rev<1 & DA_Rev>0, cor(DA_Rev, OCC)]\r\n\r\n\r\n\r\nEBITDA must be larger than USD 50m; this guarantees that the EBITDA is positive, otherwise the OCC has no meaningful interpretation; furthermore, it excludes listed companies for which the EBITDA is small, for example due to a terrible year, which makes the EV/EBITDA multiple difficult to interpret.\r\nD&A has to be positive; if it is set to 0, it is most likely a data measurement error as it is unrealistic that a company has no D&A whatsoever.\r\nThe OCC is limited to 0 and 1; 0 implies that the depreciation is as large as the EBITDA; +1 implies that the depreciation is 0; hence, this limitation shouldn’t get rid of many reasonable values that lie outside of them; however, it filters for many unreasonable values that could be a result of very low EBITDA numbers that inflate the ratio.\r\nFor the scatter plot chart, I limit the analysis further to an OCC from 30% to 100% as ca. 93% fall in that range. Including observations with lower OCC might bias the view towards such outliers.\r\n\r\nShow code\r\nDT[, Year:=year(Date)]\r\ncor_EV_EBITDA_OCC <- DT[is.finite(EV_EBITDA) & is.finite(OCC) & EV_EBITDA<30 & OCC>0 & OCC<1, cor(EV_EBITDA, OCC)]\r\nggplot(DT[is.finite(EV_EBITDA) & is.finite(OCC) & EV_EBITDA>0 & EV_EBITDA<30 & OCC>0.3 & OCC<1 &\r\n          Year>2010 & Year<2020 & EBITDA>10], \r\n       aes(x=OCC, y=EV_EBITDA, color=as.factor(Year))) + #, color=sector_group)) +\r\n  scale_x_continuous(labels = scales::percent) +\r\n  geom_point(alpha=0.05) + xlab(\"OCC\") + ylab(\"EV/EBITDA\") +\r\n  geom_smooth(method=lm, se=FALSE) +\r\n  theme(legend.title = element_blank())\r\n\r\n\r\n\r\n\r\nNext I look at the correlation between the OCC and the EV/EBITDA, the relationship I’m ultimately interested in. As before, I exclude EV/EBITDA multiples above 30x. As expected, the correlation is positive with 15%. The scatter plot also reveals a positive relationship for each year. I condition on years to partially get rid of overall changes in valuation levels, essentially market sentiment, which have a large impact on the EV/EBITDA multiple, but should be independent from OCC.\r\nThe chart reveals that an increase of the OCC from 30% to 100% is accompanied by an increase in the EV/EBITDA multiple of 3x to 5x in most years. Hence, operational cash conversion is an important factor of the EV/EBITDA multiple\r\nSummary\r\nIn this post I have looked into the EV/EBITDA multiple in more detail with an international data set of companies from 2010 to 2020. I have shown that there is large dispersion in the EV/EBITDA multiple across and within sectors. Furthermore, I have established a positive correlation between the operational cash conversion and the EV/EBITDA multiple.\r\nI would have liked to look in much more detail into the underlying data set. I have some question marks, for example why there are many observations with zero depreciation. Did these companies really have no depreciation to report or is it more an issue of reporting quality? In general, international data sets are more challenging as it is harder to ensure consistency in reporting. Selection bias could also be an issue, as it is not clear to me based on which criteria the companies make it into the data set.\r\nFurthermore, I could not condition for other important factors of the EV/EBITDA multiple, such as the tax rate, the discount rate, and growth expectations, as these factors were not easily available in the data set and I did not want to make too many arbitrary assumptions (I already made a few). For a more scientific analysis, one would have to dig into these issues in much more detail.\r\nIn the future, I might get my hands on a more concise data set, for example for the U.S. market. Data sets from the U.S. typically have the highest quality, as the reporting rules are strict and enforced and as the market is by far the largest in the world. I write another blog post if I’m successful on this front.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-17-evebitda-valuation-multiple-the-data/evebitda-valuation-multiple-the-data_files/figure-html5/ggpairs_Plot-1.png",
    "last_modified": "2021-01-31T16:14:43+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-17-evebitda-valuation-multiple-the-theory/",
    "title": "EV/EBITDA valuation multiple: the theory",
    "description": "How to justify the use of EV/EBITDA valuation multiples with the discounted cash flow model.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [
      "Valuation"
    ],
    "contents": "\r\nMultiples, most notably the EV/EBITDA multiple, are universally used in private equity to value companies. In contrast, the theoretical correct valuation approach, the discounted cash flow model (DCF), is rarely found in LBO models, pitch books, etc. Of course, there are good reasons for this, nicely summarized by Liu/Nissim/Thomas (2002):\r\n\r\nWhile the multiple approach bypasses explicit projections and present value calculations, it relies on the same principles underlying the more comprehensive approach: value is an increasing function of future payoffs and a decreasing function of risk. Therefore, multiples are used often as a substitute for comprehensive valuations, because they communicate efficiently the essence of those valuations. Multiples are also used to complement comprehensive valuations, typically to calibrate those valuations to obtain terminal values.\r\n\r\nPut differently, multiples can be thought of as a summary statistic of a comprehensive valuation that is helpful in communicating the essence. A summary statistic typically does not contain the same amount of information as the data that is summarized, and one therefore has to be careful with regards to how and when to use it. Otherwise, one can end up such as the statistician who drowned in a lake averaging only 2 inches in depth…\r\nIn this post, I derive under certain simplifying assumptions the EV/EBITDA multiple from a DCF model and discuss the factors that drive the EV/EBITDA multiple. I then discuss the consequences if those simplifying assumptions do not hold. In the blog post “EV/EBITDA valuation multiple: the data”, I look at the issue empirically.\r\n\r\nIf you want to dive into this issue a bit further, Damodaran has some helpful lecture slides.\r\nStarting with a DCF of all expected free cash flows \\(FCFF\\), the enterprise value at time \\(t\\), \\(EV_t\\), can be written as:\r\n\\[ EV_t = \\sum_{i=t+1}^{\\infty} \\frac{FCFF_i}{(1+r)^i}, \\] where \\(r\\) is the firm-specific discount rate. For simplicity, I ignore the expectation operator \\(E()\\).\r\n\\(FCFF_i\\) can be rewritten as follows:1\r\n\\[ \r\nFCFF_i = (EBITDA_i - DA_i) \\times (1 - \\tau) + DA_i - \\Delta NWC_i - CAPEX_i,  \\\\\r\n\\]\r\nwhere \\(DA\\) is depreciation & amortization, \\(CAPEX\\) is capital expenditure and \\(\\tau\\) is the tax-rate. \\(\\Delta NWC\\) is the change of the net working capital in the period. If \\(NWC\\) increases, it will reduce \\(FCFF\\).\r\nThis formula shows nicely why EBITDA is so prominent in private equity in the first place: it is a proxy for cash flows that is less impacted by periodic items such as D&A and net working capital changes. However, one must remember that EBITDA ignores capital expenditures. As Warren Buffett apparently once asked: “Does management think the tooth fairy pays for capital expenditures?”\r\nLet’s come to this point later, but let’s first make some simplifying assumptions:\r\nDepreciation & amortization, \\(DA\\), equals capital expenditures, \\(CAPEX\\)\r\nThe change in net working capital, \\(\\Delta NWC\\), is 0\r\nAs the difference between D&A and capital expenditures as well as changes in net working capital are transitory, these assumptions should hold in the long run. We can then rewrite the above equation:\r\n\\[ \r\n\\begin{aligned}\r\nFCFF_i &= (EBITDA_i - DA_i) \\times (1 - \\tau) \\\\\r\n       &= EBITDA_i \\times (1-\\frac{DA_i}{EBITDA_i}) \\times (1-\\tau) \\\\\r\n       &= EBITDA_i \\times OCC_i \\times (1-\\tau),\r\n\\end{aligned}\r\n\\]\r\nwhere \\(OCC_i\\) is the operational cash conversion. If it is close to 100%, it means that most EBITDA ends up in the firm’s debt- and equity holders’ pockets. If it is close to 0% due to high capital requirements, a lot of cash must be invested to generate the EBITDA.\r\nLet’s add two more assumptions:\r\nOperational cash conversion stays constant over time, i.e. \\(OCC_i=OCC_{i+1}=...=OCC\\)\r\nEBITDA grows by a constant factor \\(g\\)\r\nWe can now rewrite the calculation of \\(EV_t\\):\r\n\\[\r\n\\begin{aligned}\r\nEV_t &= \\sum_{i=t+1}^{\\infty} \\frac{EBITDA_i \\times OCC_i \\times (1-\\tau)}{(1+r)^i} \\\\\r\n     &= \\sum_{i=t+1}^{\\infty} \\frac{EBITDA_t \\times OCC \\times (1-\\tau)\\times (1+g)^i}{(1+r)^i} \\\\\r\n     &= \\frac{EBITDA_t  \\times  OCC \\times (1-\\tau) \\times (1+g)}{r-g},\r\n\\end{aligned}\r\n\\]\r\nor\r\n\\[ \\frac{EV_t}{EBITDA_t} = \\frac{OCC \\times (1-\\tau) \\times (1+g)}{r-g} \\]\r\nIn words, the EV/EBITDA multiple is a function of the operational cash conversion \\(OCC\\), the growth of the business, \\(g\\), 1 minus the tax rate, and the discount rate, \\(r\\). All else being equal, the multiple is the higher,\r\nthe more cash generative the business;\r\nthe higher the growth of the business;\r\nthe lower the tax rate; and\r\nthe lower the discount rate, i.e., the risk of the business.\r\nIgnoring taxes by setting \\(\\tau=0\\), I plot the EV/EBITDA multiple in relation to the other three factors below.\r\n\r\nShow code\r\nlibrary(data.table)\r\nlibrary(ggplot2)\r\nDT <- as.data.table(expand.grid(OCC = seq(from=0.0,  to=1,   by=0.1),\r\n                                g   = seq(from=0.0, to=.05, by=0.001),\r\n                                r   = seq(from=0.1,  to=.15, by=0.01)))\r\nDT[, Multiple:=OCC*(1+g)/(r-g)]\r\n\r\nDT[, OCC:=as.factor(OCC)]\r\nDT[, r  :=as.factor(r)]\r\n\r\nggplot(DT, aes(x=g, y=Multiple, color=OCC)) + \r\n  geom_line() +\r\n  facet_wrap(~r) +\r\n  scale_x_continuous(labels = scales::percent) +\r\n  xlab(\"Steady-state growth rate\") + ylab(\"EV/EBITDA multiple\") + \r\n  labs(title=\"EV/EBITDA multiple as function of operational cash conversion (OCC),\\n steady-state growth and discount rate\",\r\n       caption = \"The values in the group boxes indicate the discount rate\")\r\n\r\n\r\n\r\n\r\nStarting with the operational cash conversion, note that the EV/EBITDA multiple is 0x in case of a 0%-cash conversion. This is not surprising: if a business does not generate any cash on top of what it must re-invest in capital, it should be worth zero. More generally, businesses in sectors with high cash generation (e.g., software) should have much higher valuation multiples as businesses in CAPEX-intensive sectors such as manufacturing. The growth rate is also an important factor. Going from a 0% p.a. growth rate to a 5% p.a. roughly doubles the EV/EBITDA multiple in case of lower discount rates. Finally, the higher the discount rate, which can for example be calculated with the Weighted-Average Cost of Capital (WACC) method, the lower the valuations. A change from 10% to 15% might not sound like much, but it lowers an EV/EBITDA of a business with 100% OCC and 5% p.a. growth from over 20x to just around 10x (see right end of the most upper curve in the top left vs. the bottom right chart).\r\nThese relationships are very important to keep in mind when valuing a business with the multiple approach. For example, if you are looking at a company that has a much lower cash-generation than its peers, it should have a much lower valuation. Applying the same EV/EBITDA multiple as the peers can lead to large valuation mistakes.\r\nFurthermore, remember that we needed quite a bit of simplifying assumptions to establish the relationship between the FCFF-DCF method and the EV/EBITDA multiple. These assumptions most certainly will not hold for an actual business. For example, it is not easy to translate future growth over many periods in one reasonable assumption for \\(g\\).\r\n\r\nThe attentive reader will notice that the tax is calculated based on the EBIT, the earnings before interest, which is incorrect, as it is in reality based on the EBT, the earnings after interest. In the DCF model, the tax benefits of debt and the accompanying interest are considered in the discount rate, which makes the calculation much simpler. Hence, \\(\\tau\\) has to be considered as a hypothetical tax rate, not the actual one, yet another simplification.↩︎\r\n",
    "preview": "posts/2021-01-17-evebitda-valuation-multiple-the-theory/evebitda-valuation-multiple-the-theory_files/figure-html5/Relationship_of_OCC_g_r_EV_EBITDA-1.png",
    "last_modified": "2021-02-06T10:56:38+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-17-automatically-obtaining-data-from-damodarans-website-with-r/",
    "title": "Automatically obtaining data from Damodaran's website with R",
    "description": "A quick tutorial on how to obtain data automatically from Damodaran's website with R.",
    "author": [
      {
        "name": "Christoph Jäckel",
        "url": {}
      }
    ],
    "date": "2021-01-17",
    "categories": [
      "R",
      "Data"
    ],
    "contents": "\r\nNYU professor Aswath Damodaran has very helpful data about many important financial market indicators such as discount rates, valuation multiples, etc. on his website. He also provides an archive with the history of these indicators. However, the data is saved in separate Excel files that do not necessarily have a standardized format to easily obtain the information automatically. As I am not a big friend of manually downloading dozens of files and copying the information together, this post shows you how to do it automatically in R.\r\nLet’s start with the problem: The screenshot below gives an example of one of Damodaran’s files. As you can see, there are a couple of rows at the beginning that contain metadata, while the actual data set starts in row 10. Unfortunately, this format can change from file to file, so you cannot be sure that every file is having the exact same format.\r\nScreenshot of Damodaran’s Excel file “Value EBIT & Value/EBITDA Multiples by Industry Sector” (http://pages.stern.nyu.edu/~adamodar/pc/archives/vebitda18.xls\", obtained on January 17, 2021)By looking at a few of these files though, I noticed that the general format is always the same and can be split in two parts:\r\nAt the beginning, there might be some rows that provide additional information about the file\r\nAfter that, the actual data set begins\r\nSo all we have to do is to write a function that reads in the Excel file and tries to determine which rows belong to part 1 and which belong to part 2. The below function does exactly that. It requires the readxl package to read in the Excel file, the httr package to get the correct URL to the Excel file, and the data.table package because I forgot by now how to work with R’s standard data.frames.\r\n\r\n\r\nShow code\r\n\r\n#' Wrapper to import data from Excel\r\n#'\r\n#' This functions serves as a wrapper to easily import data from particular Excel files, for example\r\n#' as provided by Damodaran on his website. These Excel files have a format in which information is\r\n#' provided in the first couple of lines and then the actual data set starts.\r\n#'\r\n#' Functions such as \\code{\\link{read_excel}} allow the user to explicitly tell the function which\r\n#' rows to skip. While this is certainly a helpful feature, it becomes cumbersome for many similar\r\n#' files but with different number of rows that can be skipped as the user would have to open each\r\n#' file manually to come up with the parameters. This wrapper tries to do it automatically.\r\n#'\r\n#'\r\n#' @param .str_file string; file location.\r\n#' @param .file_ext string; extension of the Excel-file. Default: \".xls\", for which this function\r\n#'                  is tested. So might not work with other Excel file extensions.\r\n#' @param .perc_numeric numeric; the percentage above which you assume a column is of type numeric.\r\n#'                      Background: in the function, you convert a column to numeric with \\code{\r\n#'                      as.numeric}; this will return NA if this function does not know how to\r\n#'                      convert the value to numeric; hence, you can guess now that if there aren't\r\n#'                      too many NAs from this function, the column is actually a numeric one.\r\n#'                      The default is 0.15, which means that a column is considered numeric if\r\n#'                      at least 15 per cent of the values can be converted.\r\n#' @param .bol_option_1 boolean; if TRUE, the header position is determined by looking at the starting\r\n#'                      point of non-NA values after conversion to non-numeric that occurs most often.\r\n#'                      if FALSE, you use the first row that has all observations (which is most likely\r\n#'                      the header).\r\n#' @return data.table from reading the Excel file with the start row determined by this wrapper.\r\n#'\r\n#' @importFrom readxl read_excel\r\n#' @importFrom httr GET write_disk\r\n#'\r\n#' @export\r\n#'\r\n#' @references\r\n#' \\url{https://stackoverflow.com/questions/41368628/read-excel-file-from-a-url-using-the-readxl-package}\r\n#'\r\n#' @examples\r\n#' \\dontrun{\r\n#' str_file <- \"http://pages.stern.nyu.edu/~adamodar/pc/archives/vebitda18.xls\"\r\n#' import_from_excel(str_file)\r\n#' }\r\nimport_from_excel <- function(.str_file, .file_ext = \".xls\", .perc_numeric=0.15,\r\n                              .bol_option_1=TRUE, ...) {\r\n\r\n  #### Save URL file locally with GET\r\n  httr::GET(.str_file, write_disk(tf <- tempfile(fileext = \".xls\")))\r\n\r\n  ####Save the file into a data.table\r\n  intDT <- as.data.table(readxl::read_excel(tf))\r\n\r\n  ####Determine first row by two ways\r\n\r\n  ## 1. It's likely that most columns in the data set are numeric; hence, you will convert\r\n  #     each column to a numeric and save the first non-NA number; the row where this happens\r\n  #     most often (essentially the median) is your best guess for the first data row (i.e.\r\n  #     one below the header)\r\n  i <- 1\r\n  vec_first_number <- numeric(ncol(intDT))\r\n  col_types <- rep(\"guess\", times=ncol(intDT))\r\n  for (col in names(intDT)) {\r\n\r\n    vec <- suppressWarnings(as.numeric(unlist(intDT[,col, with=FALSE])))\r\n    vec_first_number[i] <- suppressWarnings(min(which(!is.na(vec))))\r\n    #In case there is no numeric data in there, your best  guess is that the data set starts\r\n    #at the second line (after headers)\r\n    if (!is.finite(vec_first_number[i])) {\r\n      vec_first_number[i] <- 2\r\n    }\r\n    # Also save if you believe that the column in a numeric one, as you can read the column as\r\n    # numeric later\r\n    if (sum(!is.na(vec))/length(vec)>.perc_numeric) {\r\n      col_types[i] <- \"numeric\"\r\n    }\r\n    i <- i + 1\r\n  }\r\n  pos_header1 <- median(vec_first_number, na.rm=TRUE) - 1\r\n\r\n  ## 2. If the first few rows do not belong yet to the actual data set, you would expect more NAs\r\n  #     there than in the data set thereafter. In particular in case of a data set with headers,\r\n  #     with you kinda require here, you must expect that the row with the headers doesn't have\r\n  #     any NAs; hence, simply use the first occurrence of a row with no NAs as the decision\r\n  #     criterium\r\n  #     Of course, this can fail if one of the information rows is filled up or if there is\r\n  #     no header row or even the header includes NAs.\r\n\r\n  # Get NAs per row\r\n  # https://stackoverflow.com/questions/35306500/r-data-table-count-na-per-row\r\n  intDT[, num_obs := Reduce(`+`, lapply(.SD,function(x) !is.na(x)))]\r\n  pos_header2 <- suppressWarnings(min(which(intDT$num_obs==max(intDT$num_obs))))\r\n  if (!is.finite(pos_header2)) {\r\n    pos_header2 <- 1\r\n  }\r\n  intDT[, num_obs:=NULL]\r\n\r\n  if (pos_header1==pos_header2) {\r\n    cat(\"Success: Both approaches in import_from_excel guessed the same start row of the data set.\\n\")\r\n    pos_header <- pos_header1\r\n  } else {\r\n    ifelse(.bol_option_1, pos_header <- pos_header1, pos_header <- pos_header2)\r\n  }\r\n\r\n  #Download data again, now with skip filled in and check then number of columns; trim col_types\r\n  #This has to be done as otherwise it can happen that the last column doesn't exist anymore\r\n  #because it was only read before due to a line that is now excluded.\r\n  #See http://pages.stern.nyu.edu/~adamodar/pc/archives/fundgrEB14.xls as an example\r\n  intDT <- as.data.table(readxl::read_excel(tf, skip=pos_header))\r\n  if (ncol(intDT)<length(col_types)) {\r\n    col_types <- col_types[1:ncol(intDT)]\r\n  }\r\n\r\n  intDT <- as.data.table(readxl::read_excel(tf,\r\n                                    skip=pos_header,\r\n                                    col_names = TRUE,\r\n                                    col_types = col_types,\r\n                                    ...))\r\n  unlink(tf) #Clean up the temporary file\r\n  return(intDT)\r\n\r\n}\r\n\r\n\r\n\r\nYou can study the code in detail to figure out what’s going on, but in short it reads in the data and takes advantage of the fact that if a row is not yet a row of the data set, it most likely has a lot of NAs because import functions such as read_excel() convert missing cells to NA. You should then find a structural difference between rows from part 1 - they have many NAs - and rows from part 2 (the actual data set), which should not have many NAs. Of course, this will not work in all situations. For example, if many cells are filled in the first part or if the data set in part 2 does not have many columns, it is hard for the function to differentiate correctly.\r\nAnyways, let’s try the function out! I show how the function works on two examples, the EV/EBITDA and EPS Growth rate files. All we have to do is look on Damodaran’s website on how many years he is covering, entering the years in the for-loop, running import_from_excel() iteratively and putting the files together with rbind.\r\n\r\n\r\nShow code\r\n\r\n#Load packages that are required for the function import_from_excel\r\nlibrary(data.table)\r\nlibrary(readxl)\r\nlibrary(httr)\r\n### Download EV/EBITDA multiples\r\nstr_path <- \"http://pages.stern.nyu.edu/~adamodar/pc/archives/\" \r\nDT <- data.table()\r\nfor (year in 1998:2018) {\r\n  \r\n  intDT <- import_from_excel(.str_file=paste0(str_path, \"vebitda\", substr(year,star=3,stop=4), \".xls\"),\r\n                             na = c(\"NA\", \"N/A\"))\r\n  intDT[, Year:=year]\r\n  DT <- rbind(DT, intDT, fill=TRUE)\r\n                          \r\n} \r\n### Download Growth Rates\r\ngrowthDT <- data.table()\r\nfor (year in 1998:2018) {\r\n  \r\n  intDT <- import_from_excel(.str_file=paste0(str_path, \"fundgrEB\", substr(year,star=3,stop=4), \".xls\"),\r\n                             na = c(\"NA\", \"N/A\"))\r\n  intDT[, Year:=year]\r\n  growthDT <- rbind(growthDT, intDT, fill=TRUE)\r\n                          \r\n} \r\n\r\n\r\n\r\nWithin a minute or so, the data is downloaded, which saves quite a bit of time compared to downloading 20 or so files and combining them. Unfortunately, the job is not quite done yet, as a look at the table below shows. For example, the column names are now always the same so you have to check now which columns changed the name over time and adjust your data set accordingly.\r\n\r\n\r\nShow code\r\n\r\nlibrary(rmarkdown)\r\nlibrary(kableExtra)\r\nlibrary(dplyr)\r\nDT %>% \r\n   mutate_if(is.numeric, format, digits=2, nsmall = 0, big.mark=\",\", scientific=FALSE) %>%\r\n   paged_table()\r\n\r\n\r\n\r\n\r\n\r\nLet’s go through these steps for the EV/EBITDA data set:\r\nCombine columns such as “Industry Name” to “Industry”\r\nDamodaran does not use a consistent industry classification over time so I only focus on industries that are at least for 15 years in the data set; note that often Damodaran seems to use more specific industry classifications such as “Industry (Canada)” and you could just add them to the larger Industry; however, this is much more work intensive and might also not be so easy as you get aggregate numbers to start with.\r\nCheck that always the same column names were used: This is not the case. For example, Damodaran started using subsamples such as only EBITDA positive firms; this makes it quite hard to figure out automatically which column refers to which calculation; it seems though that at the most he separates between all and EBITDA-positive firms, which he didn’t do at the beginning.\r\nRename column names to work easier with them in R\r\n\r\n\r\nShow code\r\n\r\n#It seems that he once labeled the Industry column \"Industry Name\", so let's rename it \"Industry\"\r\n#DT[,list(NA1 = sum(is.na(`Industry Name`)), NA2 = sum(is.na(Industry))),by=Year]\r\nDT[is.na(Industry), Industry:=`Industry Name`]\r\nDT[, `Industry Name`:=NULL]\r\n#Same for Number of f/Firms\r\nDT[is.na(`Number of Firms`), `Number of Firms`:=`Number of firms`]\r\nDT[, `Number of firms`:=NULL]\r\n#Only use Industries that are persistently in the data set\r\nDT[,N:=.N,by=Industry]\r\nDT <- DT[N>=15]\r\nDT[, N:=NULL]\r\n#Merge Value/EBITDA and EV/EBITDA\r\nDT[, EV_EBITDA:=`Value/EBITDA`]\r\nDT[is.na(EV_EBITDA), EV_EBITDA:=`EV/EBITDA`]\r\n#Rename columns\r\nsetnames(DT, \r\n         old = c(\"MV of Equity\", \"ST Debt\", \"LT Debt\", \"Effective Tax Rate\"),\r\n         new = c(\"Equity\",       \"stDebt\",  \"ltDebt\",  \"Tax_rate\"))\r\nDT[, Debt:=stDebt + ltDebt]\r\nDT[, Debt_EBITDA := Debt/EBITDA]\r\nDT[!is.finite(Debt_EBITDA), Debt_EBITDA:=NA]\r\n\r\n\r\n\r\nLet’s again look at the data set.\r\n\r\n\r\nShow code\r\n\r\nDT %>% \r\n   mutate_if(is.numeric, format, digits=2, nsmall = 0, big.mark=\",\", scientific=FALSE) %>%\r\n   paged_table()\r\n\r\n\r\n\r\n\r\n\r\nNow we can already analyze the data, for example by looking at a box plot chart per industry.\r\n\r\n\r\nShow code\r\n\r\nlibrary(ggplot2)\r\nDT[EV_EBITDA < 30 & EV_EBITDA>0, MedianMult:= median(EV_EBITDA, na.rm=TRUE),by=Industry]\r\nggplot(data = DT[EV_EBITDA>0 & EV_EBITDA <30], \r\n       aes(x = reorder(Industry, MedianMult, na.rm=TRUE), y = EV_EBITDA)) + #fill = Continent_Name)) +\r\n geom_boxplot() + ylim(c(0,30))  + ylab(\"EV/EBITDA\") + xlab(\"Industry\") + coord_flip()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-17-automatically-obtaining-data-from-damodarans-website-with-r/Damodaran_vebitda_screenshot.PNG",
    "last_modified": "2021-06-13T16:46:16+02:00",
    "input_file": {},
    "preview_width": 2781,
    "preview_height": 1312
  }
]
