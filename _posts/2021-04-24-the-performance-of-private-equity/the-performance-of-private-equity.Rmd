---
title: "The ultimate guide to private equity performance"
description: |
  A thorough analysis of the performance of the asset class private equity.
author:
  - name: Christoph Jäckel
    url: {}
date: 01-09-2022
output:
  distill::distill_article:
    self_contained: false
    toc: true
categories:
  - Private Equity
  - Data
  - Research
draft: true
bibliography: C:/Users/Christoph Jaeckel/Desktop/CJaeckel Blog/Blog/docs/biblio.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(knitr.kable.NA = '')
library(data.table)
library(ggplot2)
library(ggsci) #For colors, see https://www.datanovia.com/en/blog/ggplot-colors-best-tricks-you-will-love/
library(kableExtra)
library(utilitiesCJ)
library(lubridate)
max_vintage <- 2005
DT <- readRDS("C:/Users/Christoph Jaeckel/Desktop/Preqin/preqin_data_22March2021.rds")
DT <- DT[type!="other"]
#Replace labels to match Harris data and for better readability
DT[, type:=gsub("buyout", "Buyout", type)]
DT[, type:=gsub("growth", "Growth", type)]
DT[, type:=gsub("vc",     "VC",     type)]
#Reclassify us to "north america"
DT[, geo:=gsub("us",     "North America", geo)]
DT[, geo:=gsub("europe", "Europe",        geo)]
DT[, geo:=gsub("row",    "Rest of World", geo)]

```

TODO:
1. Replace private equity with PE after the first occurrence
2. Replace type in code with Strategy ==> makes plots / tables nicer
3. Check what min_date is in the Preqin data set. I assume it is the first quarter you had cash flow activity for each fund. This is very important as if this is not so, your PME calculations might be off!

What's the performance of private equity? That's a simple question, yet there is no simple answer. There are too many parameters to adjust, such as the performance metric, the time period and the source of the data. In this blog post, I provide an overview of research papers that have looked into this topic and compare their results. I then do my own analysis with Preqin data. I focus in particular...

<aside>
I'm focusing on **fund** level performance. There is a separate literature that looks at the performance on the **deal** level. I discuss two such studies in more detail in [this blog post](https://www.christoph-jaeckel.com/posts/2021-02-28-why-does-private-equity-outperform-public-equity/).
</aside>

Should be a relatively long post, in which you summarize the performance based on sub-asset class (buyout, VC), geography (US/Europe), performance measure (Multiple, IRR, PME), different statistics (median, mean, pooled), data sources, etc. Also include J-curves.

In addition, explain some subtle issues, such as the right-skewed distribution. Also, compare with other studies.

# Introduction

# Literature review

In the following, I briefly introduce several studies about the performance of PE. This is not a comprehensive summary, but rather a selection of studies I consider important or helpful in shedding light on the question of what returns PE funds deliver to investors. For a more detailed literature review with a very helpful overview table, have a look at @korteweg:2019.

## @kaplan-schoar:2005

One of the earliest and most influential studies of PE performance, it was some sort of a kickstarter for research on PE. By finding persistence in fund returns - GPs that outperform the industry in one fund are more likely to do so with the next one as well - they also justified, or even established, investors key focus on a GP's track record. PE fund returns are predictable, in contrast to mutual funds, so betting on the same horses again and again pays off for investors. A GP with a top tercile fund has close to a 50% chance of having a top tercile fund with their next one (see Table IX), both when looking at the PME and IRR.

@kaplan-schoar:2005 do not find an outperformance of PE over the S&P 500 overall. For their analysis, they use Thomson Venture Economics (TVE) data, which was the prevalent data source back then and also used by the two studies that I introduce next. I don't want to spill the beans, but let's just say that data quality really matters...

## @phalippou-gottschalg:2009

The authors summarize their research question and answer concisely:

> The objective of this study is to estimate the performance of private equity funds both net-of-fees and gross-of-fees. We find that the average private equity fund underperforms the S&P 500 Index net-of-fees by about 3% per year and overperforms that index gross-of-fees by 3% per year.

While the language might be prosaic, the content is everything but. How to justify a 3% p.a. *underperformance* compared to the overall stock market for an asset class that should be deemed at least as risky (highly levered investments in much smaller companies than the S&P 500 constituents) and much more illiquid? And how to reconcile this underperformance with PE's unprecedented growth in committed capital from USD 5 to USD 300 billion from 1980 to 2004? Did investors keep pouring money into PE despite the poor performance or were they not able to calculate their returns correctly?

To derive at their results, @phalippou-gottschalg:2009 start with the observation that many older funds, 10 years or more, show no sign of activity: no capital calls or distributions and no changes to the NAV or residual value.^[I use the terms NAV and residual value interchangeably throughout this post.] They interpret this lack of activity as confirmation that the funds are in reality liquidated and write-off their final NAV to zero, in contrast to other studies that treat the residual value as a cash inflow of the same amount.

<aside>
@phalippou-gottschalg:2009 propose further corrections to the performance metrics, such as different weightings of the funds and risk adjustments. However, the write-off of the NAVs has the most profound impact on their results, which is why I focus on this part.
</aside>

Writing off the NAVs of the funds fully has a dramatic impact on the performance metrics. However, @stucke:2011 questioned that the inactivity of those funds was really a sign of living dead investments, rather than a result of poor data quality, which would make the results of @phalippou-gottschalg:2009 obsolete. Without further ado, let's discuss Stucke's findings next.

## @stucke:2011

This study illustrates nicely the complications that arise in PE research from uncertainty about the data quality. Concretely, Stucke looks closer at the Thomson Venture Economics ("TVE") data, which was until then the predominant data source for researchers. He finds that aggregate performance numbers of TVE are significantly smaller than those from other providers. More importantly, he is able to show that this is the result of about 40 percent of the funds in the TVE data not being updated.

This paper had a profound impact in academia as newer studies pretty much abandoned the TVE data set and focused on other sources instead (see, e.g., @harris-etal:2014 further below). It also brought into question many of the findings of studies that used the TVE data. Finally, it helped explain why far too many GPs could claim to be a top-quartile fund.^[In @harris-stucke:2012, he looks further into this issue and and also shows other reasons why more than 25% of funds can claim to be top-quartile.]

The starting point of Stucke's investigation are the findings of @phalippou-gottschalg:2009. In the previous section, I described how those authors took the evidence that the majority of funds had no cash flow activity and no changes in residual values as proof that these funds are "zombie" funds, i.e., funds that do not hold any meaningful residual value, despite reporting it. To deal with this issue, they set the residual value to 0 to calculate the fund's performance. Stucke finds this curious as

> constant NAVs rarely exist (particularly not over several years). Even in the later years of a fund’s lifetime the values of remaining investments get updated regularly. Furthermore, private equity funds without a single cash flow activity for more than three years should equally not exist (there will still be annual management fees or dividends from mature investments). In addition to this, the authors find that average NAVs of these funds equate to over 50% of the amount they invested. Keeping in mind that all of these funds are between 10 and 24 years old and most of them should be liquidated, remaining investments with a constant value as high as 50% of a
fund’s size – on average – are surprising.

Stucke subsequently shows that the constant NAVs and missing cash flows are indeed due to the fact that TVE stopped to receive updates from the GPs about the fund performance. To do so, he matches 140 funds from the TVE data with a data set obtained from LPs and finds that these funds showed substantial value creation *after* TVE stopped to receive updates. Not surprisingly then, results based on the TVE data lead to a downward bias of PE. When Stucke only focuses on fully liquidated funds in the data, which are unaffected by this issue, he finds a meaningful outperformance of U.S. buyout funds compared to the S&P 500.

## @harris-etal:2014

This study, published in one of the most prestigious finance journals, is the first to use data sourced by Burgiss. Why is that such a big thing? Because it is notoriously difficult to obtain high quality private equity. Note that there is typically no obligation by GPs and LPs to report their performance to any external parties and that's typically why they don't. So other data providers, such as Preqin, have to rely on data that is provided to them, either because the LPs are forced to do so - essentially U.S. public pension funds because of the [Freedom of Information Act](https://en.wikipedia.org/wiki/Freedom_of_Information_Act_(United_States)) - or because GPs want to. This, of course, creates a few issues: as the pension funds are forced to publish the data, but there is no benefits for them to do so, the data provided is not really well documented. They also do not provide the data in a standardized and granular way. This makes it very difficult to figure out exactly what happens and it is not uncommon to find different performance results from different pension funds for the very same fund. With data provided by GPs, there is a high risk of selection bias: obviously, GPs only have an interest to report those funds that perform well. In addition, GPs that had to close down because of their bad performance cannot report anymore (hindsight bias). Finally, you might have GPs with such a strong track record and such a high demand of LPs that they do not see any reason to publish their performance. And last but not least a GP might decide to stop reporting, as seemed to have happened in the case of TVE, with dire consequences as we have seen in the previous sections.

Burgiss does not have these issues as they source the data directly from LPs that use their platform for their internal bookkeeping. As a result, the quality checks when entering the data are rigorous and there is good reason to believe that there is no selection bias: LPs don't stop reporting on a fund just because it performs poorly. One could still argue that the group of LPs that use Burgiss did not assemble a portfolio that is representative of the whole private equity universe. For example, it is likely that the LPs that use Burgiss are larger, more institutionalized LPs that tend to invest in larger funds. However, one would have to argue that such biases in fund selection by the LPs are correlated with the performance of the funds selected. And there is no strong reason to do so.

@harris-etal:2014 continue to show that their sample of U.S. buyout funds outperformed the S&P 500 by roughly 3% p.a., which is a significant premium. Furthermore, this outperformance happened fairly consistently over their time period from 1984 to 2008 (see Table IV in their study). They also use other indices such as the Russell 3000, which is focused more on smaller listed companies that might be a better comparison for private equity. This slightly reduces the outperformance, but does not get rid of it.

For VC, the picture is more mixed. While VC outperformed markets considerably in the 90s, this trend reversed in the 2000s until 2008. Overall, they find an average outperformance, but a median underperformance over the whole sample. I would assume that the strong performance over the last few years, so after the end of their analysis, for growth companies might have changed the picture a bit. Overall though, it is fair to say that VC's performance is more cyclical than the buyout one.

In their Table VIII, @harris-etal:2014 compare the PMEs from the Burgiss data set with those from Cambridge Associates (CA), Preqin and Venture Economics. While the latter has a downward bias issue as uncovered by @stucke:2011, they show that the results with CA and Preqin data are rather similar. This result counters fears that because of the weaker data collection process applied by providers such as CA and Preqin systematic biases end up the results. Instead, the results are comparable. My co-author and I found the [same](https://www.bvca.co.uk/Portals/0/library/documents/Guide%20to%20Risk/Risk%20in%20Private%20Equity%20-%20Oct%202015.pdf?ver=2015-10-07-112204-040) when we compared the Preqin data set with two data sets that were sourced directly from the LPs or the GPs.

## @phalippou:2014

This study does pretty much the same as @harris-etal:2014, with two important deviations. 

First, it uses data from Preqin. However, to counter any criticism of data quality issues, it shows that the results are comparable to studies such as @harris-etal:2014. This is of course not surprising, as @harris-etal:2014 have made the same argument in their study.

Second, it argues that buyout funds invest in companies that are much smaller than most listed companies, that are value, instead of growth companies, and that use more leverage. As all these factors are known to have led to higher stock returns historically, Phalippou argues that one should use indices that adjust for these factors. Using a small value index essentially gets rid of any outperformance in the data, leveraging the index up based on some high-level assumptions, the mean and median PME falls below 1, indicating an underperformance of PE funds to these indices. 

[Similar results](https://www.christoph-jaeckel.com/posts/2021-02-28-why-does-private-equity-outperform-public-equity/) have been found by studies that looked at deal-level performance. Of course, it is important to bear in mind that the public indices Phalippou had to use to get the PME down had some of the strongest performances in U.S. public equity, which by itself is one of the strongest asset classes. It's a bit like saying that compared to Michael Jordan, LeBron James is a worse basketball player...that might be true, but it certainly doesn't mean he is bad at playing basketball.

## Honorable mentions

The literature on PE performance has become too vast to fully cover, so this overview cannot cover all studies published on the topic of private equity performance. Above, I introduced a few studies in more detail, let's mention briefly a few more:

- @hooke-yook:2016 compare 18 larger, well regarded buyout managers with the overall buyout market and find that their returns are slightly above average.
- @sorensen-etal:2014 ask the question if the net performance of PE investments is enough to compensate the investors for the leverage and illiquidity they take on and the management and incentive fees they pay to the GPs. To answer it, they develop a model that incorporate these risk/costs and find that funds must generate substantial alpha to compensate investors for them.
- @korteweg-sorensen:2017 build upon the work of @kaplan-schoar:2005 and decompose PE performance into three components: long-term persistence of the GP; spurious persistence that exists due to exposure to the same market conditions of two consecutive funds; persistence due to luck or noise. They find that long-term persistence still exists, although it has declined in the 2000s, relative to the 1990s. Interestingly, the decline in persistence is strongest in the case of VC, while buyout GPs show much stronger persistence.
- @cavagnaro-etal:2019 show that institutional investors' skill in selecting PE funds is important in determining the returns. As @korteweg-sorensen:2017 point out, this is a key piece to solve the puzzle as to why GP persistence continues to exist in PE: "Skilled PE firms are scarce, but LPs with the ability to identify these skilled firms can also be scarce."
- @brown-etal:2019 is one of my favorite studies as it helps me frequently in my job when I'm assessing a GP's track record. They examine if one can trust the GP's reported valuations. They should be mark-to-market, but we all know that there is enough wiggle room that one could be too optimistic or cautious, depending on the agenda of the GP. They find that GPs that struggle to raise a new fund inflate their valuations and in consequence their track record. Strong GPs, on the other hand, do not require such tricks and have no incentive  to overpromise to their investors, which is why they typically value their assets conservatively. Finally, their results indicate that investors look through the valuation manipulation of poor GPs as it does not help them in raising new funds. With regards to the question about PE performance, this is a very important study as it shows that residual values cannot be taken at face value and one has to be careful to draw conclusions from funds that are not fully realized. After all, this was the premise of @phalippou-gottschalg:2009, they just took an extreme approach (writing the residual value off).^[Of course, they had good reasons to do so when looking at their data as it was indeed very peculiar the very mature funds would report large residual values without any changes in activity. As @stucke:2011 subsequently showed, this was due to a failure of the data provider to receive updates on those funds and flag this accordingly.]

To summarize: Earlier studies found no outperformance, or even an underperformance, of PE compared to the broader stock market. These results, however, were due to a bias in the data used. Using better data, often reported directly from the investors, newer studies found an outperformance, or alpha, instead. A plethora of research linked this alpha with risks - for example due to illiquidity, leverage, and factor exposure - and costs (fees and carry), arguing that the true alpha, after adjusting for them, is closer to zero or even negative again.

From a practitioner's perspective, I find that there is often not much interest in explanations as to which factors drive the overall good returns of PE and arguments if they simply compensate investors for risk factors or can be considered true alpha. Alas, one can even [argue that Buffett's returns are simply due to exposure to some risk factors and leverage.](https://www.tandfonline.com/doi/full/10.2469/faj.v74.n4.3). How much do Buffett's investors care?

What is of great interest to practitioners is often a thorough understanding of private equity returns, with a focus on metrics that are used in the industry (IRR, multiple / MOIC). Yes, they have shortcomings, but despite these shortcomings they are still widely used. This is the focus of the empirical part of this blog post, where I present the results for different strategies (VC, growth, buyout) and geographies with a recent data set until 2020. 

# Data source and characteristics

Let's revisit some of those topics with an up-to-date data set from [Preqin](www.preqin.com). This is not the gold standard of data sets available - right now this can be considered to be Burgiss - but it has been shown repeatedly that results from Preqin lead to comparable conclusions as other, higher quality data sets (see, e.g., @harris-etal:2014, @phalippou:2014 or @diller-jaeckel:2015). I will only focus on funds for which Preqin provides cash flow data, which is only a subset of the funds that Preqin tracks and of the overall PE fund universe. I obtained the data in March 2020.

In this section, I will introduce the data set in more detail. As we have seen above, the quality of the data has a large impact on the results, and hence, it is important to get comfortable with the data set and understand its characteristics. However, readers interested in PE performance can skip this section.

## Number of funds over time and by type

Before we look into the performance, let's first have a look at the number and size of funds for which Preqin has cash flows. This gives us a better understanding of what share of the PE market the data set covers.

```{r PrepWork, echo=FALSE}
library("ggsci")
intDT <- unique(DT[vintage<=2019], by="id")[,.N,by=list(vintage, type)]
```

Looking at the numbers of funds shows that Preqin only has a few funds per vintage year until the mid-90s, while they increase substantially over time thereafter. While there is a persistent long-term growth for buyout and growth funds, there is quite a bit of volatility across vintage years, in line with overall market cycles. For example, there are `r intDT[vintage==2007 & type=="Buyout", N]` buyout funds with a vintage year of 2007 in the sample, while there are only `r intDT[vintage==2009 & type=="Buyout", N]` for 2009. Interestingly, for VC the absolute peak was in 2000, showing how keen investors were to invest in young companies around the dot-com bubble. 

```{r NumberOfFunds, fig.cap="Number of funds per vintage year with full cash flow data from Preqin, split by the type of fund."}
ggplot(data=intDT, aes(x=vintage, y=N, color=type)) + 
  geom_line() + xlab("Vintage year") + ylab("Number of funds") +
  scale_color_jco()
```

## Number of funds by region

Another interesting question is how many funds are there per geography. The table below gives the answer. Not surprisingly, North America is by far the largest contributor, which is also in line with [current fundraising data](https://www.statista.com/statistics/513780/private-equity-fundraising-by-region/) that shows North America's leading position. Europe is the second-largest market, followed by Asia. The other markets are very small in comparison. Going forward, I will therefore only report statistics for North America and Europe and pool the rest under "Rest of the World".

```{r NrOfFundsRegions}
intDT <- unique(DT[vintage<=2019], by="id")[,.N,by=list(geo_original, type)]
intDT <- dcast(intDT, geo_original ~ type, value.var = "N")

a <- colSums(intDT[,2:4],na.rm=TRUE)
globalDT <- data.table(geo_original="Global", a[1], a[2], a[3])
setnames(globalDT, names(globalDT)[2:4], names(a))

intDT <- rbind(intDT,
               globalDT,
               fill=TRUE,
               use.names=TRUE)
intDT[, Total:=rowSums(intDT[,2:4, with=FALSE], na.rm=TRUE)]

#Replace NAs with 0
for (i in names(intDT))
  intDT[is.na(get(i)), (i):=0]

setnames(intDT, "geo_original", "Geography")
kbl(intDT,
    caption=paste0("Number of funds in the Preqin data set with cash flow data for different geographies. Vintage years from ", 
                   min(DT$vintage), " to ", 2019, " considered.")) %>%
  kable_classic(full_width = FALSE)  %>%
  row_spec(nrow(intDT),    bold = TRUE)  
```

## Comparison with other data sets

At the risk of repeating myself: Preqin, as any PE data set, does not cover the whole universe of private equity funds. As such, one has to be careful from drawing conclusions about PE in general when looking at one particular data set. One important question to ask is therefore if the data set at hand is comparable to other data sets. I argued above that Preqin leads to comparable results when looking at the performance of PE. Let's look now if the Preqin data set also has a coverage that is similar to other sources. To do so, the plot below compares the number of funds in the Preqin data set with various other sources, as reported by @harris-etal:2014 in their Table I. As they look at vintages from 1984 to 2008 and only North American funds, I limit sample accordingly. Also, they only report numbers for buyout and VC funds. As I do not know how they classify growth funds, I ignore them.


```{r CompNumberOfFundsHarrisEtAl, fig.cap="Comparison of number of funds from Preqin compared with other data sets, as provided by Harris et al. (2014) (see Table I)."}
## Get Harris data and format so you can rbind
compDT <- fread("C:/Users/Christoph Jaeckel/Desktop/CJaeckel Blog/Data for Blog/NrFunds_Harris_et_al.csv")
compDT <- melt(compDT, id.vars=c("VY", "Type"))
setnames(compDT, c("value", "Type", "VY", "variable"), c("N", "type", "vintage", "Source"))
## Recalculate intDT to only consider North American funds
intDT <- unique(DT[vintage<=2019 & geo_original=="North America"], by="id")[,.N,by=list(vintage, type)]

## Update intDT so you can rbind
intDT[, Source:="Preqin"]
intDT <- intDT[type!="Growth"]
## Rbind
intDT <- rbind(intDT, compDT)

ggplot(data=intDT[vintage<=2008 & vintage>=1984], aes(x=vintage, y=N, color=Source)) +
  facet_grid(rows=vars(type)) +
  geom_line() + xlab("Vintage year") + ylab("Number of funds") +
  scale_color_jco()
```

With regards to buyout funds, Preqin (dark blue) is comparable with Burgiss (yellow) and other data providers, although being on the lower end of funds covered. For VC, the coverage of Preqin is substantially lower than others, in particular until around 2000. Most notably Cambridge Associates and Venture Economics cover many more funds. However, Preqin's coverage seemed to have increased in the 2000s. With regards to the coverage over time, the patterns are comparable across the main data providers: strong increase until 2000, sharp decrease during the dot-com bubble crash, steady increase thereafter. This gives me comfort that these data sets cover the overall PE market cycle, rather than some idiosyncratic data collection trends.^[For example, one could hypothesize that GPs' likelihood to report to data providers changes over time, so instead of observing trends in the overall PE market, we are looking at a trend of GPs' willingness to report. However, this is unlikely for two reasons. First, the trends resonate well with the overall market cycle. It just seems logical that the number of VC funds exploded in 2000, rather than GP's just willing to report those funds. Second, the data providers use different means for data collection - Burgiss, for example, relies on LPs, not GPs.]

## Coverage in relation to PE universe

While Preqin is comparable to other data providers in terms of coverage, especially after 2000, the question remains how good the coverage is. Do those data providers cover most of the PE universe or only a fraction of it? There will not be a perfect answer to this question as no one knows for sure how large the PE universe really is. However, we can look at data that estimates the size of the overall PE market and compare it with the size by the funds covered by Preqin.

In the table below, I compare fundraising numbers for the overall private markets, as reported by @mcKinsey:2021 (see Exhibit 2), with the aggregate size of the funds in Preqin that have cash flow data. As the McKinsey study only reports fundraising numbers over time for private markets, not private equity, I estimate the ratio by multiplying the historic numbers with the 2020 ratio shown in Exhibit 1 of their report (`r paste0(format(502.9/857.8*100, digits=1),"%")`, USD 857.8bn for private markets vs. USD 502.9bn for private equity). This is obviously a rather crude measure, but it should give use a ballpark idea of how good the coverage of Preqin is.

```{r SizePreqinVsMarket}
## Add fund sizes in Preqin data set for aggregate fundraising number
intDT <- unique(DT[vintage>=2010 & vintage<=2019], by="id")[,list(Preqin = round(sum(size/1000, na.rm=TRUE),digits=0)),keyby=list(vintage)]
## Get numbers in from Exhibit 2, McKinsey Global Private Markets Review 2021, April 2021
mcKinseyData <- data.table(vintage = 2010:2019,
                          Market  = c(311, 378, 434, 593, 670, 733,
                                      846, 979, 1040,1091))
## Merge the two
setkey(mcKinseyData, vintage)
intDT <- intDT[mcKinseyData]

## Calculate coverage; to do so, take the Global PE market of 2020 from Exhibit 1 (502.9)
#  and divide by total Private Markets (858.7) to estimate ratio of PE vs. Private Markets
#  Apply this ratio over time ==> obviously a simplification
intDT[, PE := Market*502.9/857.8]
intDT[, Coverage := paste0(format(Preqin/PE*100, digits=1), "%")]
avg_cov <- paste0(format(mean(intDT[,Preqin/PE*100]),digits=1),"%")
intDT[, PE := round(PE, digits=0)]

## Report
kbl(intDT[,list(vintage, Market, PE, Preqin, Coverage)],
    align = "c",
    col.names = c("Vintage year", "Private Markets Fundraising", "Private Equity Fundraising (estimated)", "Total size of Preqin funds with CF data", "Coverage"),
    caption="For vintage years from 2010 to 2019, the table compares the overall capital raised in private markets and private equity (estimated) with the total size of the funds for which Preqin has cash flow data. Data for the market comes from the McKinsey Global Private Markets Review 2021. Numbers in USD billion.") %>%
  kable_classic(full_width = FALSE)

```

The results are rather comforting as Preqin has on average a coverage of `r avg_cov` over the last decade. They aren't able to cover the full universe, but quite a big chunk of it! In the following, I will make the implicit assumptions that all results I find from the Preqin data set apply to the overall PE universe. Of course, there is always the risk that I report patterns that are due to the specific biases of the Preqin data set and do not apply to the PE universe. However, it's a risk I'm willing to take. It's a blog after all, not a PhD thesis...

## Size of funds

I apologize for the long table below, I know it's dense, but I think also insightful. The table shows, for each strategy and vintage year, three statistics: the number of funds, the average fund size, and the aggregate fund size, which is the product of the first two. 

```{r SizeFunds, layout="l-page", fig.cap="Dispersion of fund sizes by strategy over vintage years. Data from Preqin.", eval=FALSE}
ggplot(unique(DT, by="id")[vintage>=1990],
       aes(x=as.factor(vintage), y=size)) +
         geom_boxplot(outlier.shape = NA) +
         facet_grid(rows=vars(type), scales="free_y") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  xlab("Vintage year") + ylab("Fund size (in USD million)") +
  scale_y_continuous(limits = c(0, 12000))
```

```{r SizeTable}
intDT <- dcast(unique(DT, by="id")[vintage>=1990 & vintage<=2019, list(N        = .N, 
                                                       Avg_Size = mean(size,na.rm=TRUE), 
                                                       Agg      = sum(size/1000,na.rm=TRUE)),
                                   keyby=list(vintage, type)], vintage ~ type, value.var = c("N", "Avg_Size", "Agg"))
intDT[, vintage:=as.character(vintage)]
kbl(intDT[, c(1,2,5,8, 3,6,9, 4,7,10)],
    col.names = NULL,
    digits = 1,
    format.args = list(big.mark = ",")) %>%
  kable_classic() %>%
  add_header_above(c("Vintage",rep(c("Nr. funds", "Avg. size (in USDm)", "Agg. size (in USDbn)"),3))) %>%
  add_header_above(c(" ", "Buyout" = 3,  "Growth" = 3, "VC" = 3)) 

```

While buyout funds only make up `r intDT[, sum(N_Buyout)/(sum(N_Buyout)+sum(N_VC)+sum(N_Growth,na.rm=TRUE))]*100`% with regards to their numbers, they are responsible for `r format(intDT[, sum(Agg_Buyout)/(sum(Agg_Buyout)+sum(Agg_VC)+sum(Agg_Growth,na.rm=TRUE))]*100, digits=0)`% of the aggregate size due to their  larger size than VC and growth funds. Growth funds have actually raised a bit more capital overall in the sample than VC (USD `r round(intDT[, sum(Agg_Growth,na.rm=TRUE)],0)`bn vs. `r round(intDT[, sum(Agg_VC,na.rm=TRUE)],0)`bn), despite much lower number of funds (see chart above). These funds can be quite sizable, too, with brands like Warburg Pincus and Insight Partners that raise multi-billion funds. Given their lower number of funds per vintage year, the average size fluctuates quite a bit more than for the other two strategies. For example, in 2011 the average fund size was only around USD 500m, while it was three times as large the following year, even overtaking buyout funds. 

With regards to fund size growth over the years, VC had the most steady and slowest out of all of them: VCs were raising on average a few hundred millions in the 90s, and they continue to do so in the late 2010s. As VCs are backing founders and their visions and often not much more, the capital needs per investment simply don't change much over the decades. That also explains why the strong VCs can be so access restricted: getting more capital is not their concerns, it's about finding the right teams to back. For buyout funds, the story is different: they can always write bigger checks or do larger M&As, etc. That's why the fund sizes have grown much more over the years. However, fundraising is also more cyclical. For example, while a lot of buyout GPs collected money for large funds in 2006 and 2007, both the number and the size of funds plummeted  in 2009/10. For VC funds, only the number of funds declined, while the fund sizes remained pretty stable. My best guess is that some VCs with weaker track records were unable to raise follow-on funds, while the strong brands had a long line of LPs who were waiting to get it so that they could replace those LPs that suffered from the GFC.

Another interesting trend is that for buyout funds, the fund size quadrupled from 2010 to 2019, while the number of funds only increased two-fold. In contrast, the number of VC funds increased by a factor of `r round(intDT[vintage==2019, N_VC]/intDT[vintage==2010, N_VC], digit=1)`x in the same period, while the fund size only increased by `r round((intDT[vintage==2019, Avg_Size_VC]/intDT[vintage==2010, Avg_Size_VC]-1)*100, digit=0)`%. When things go well, LPs deploy more and more capital into the established buyout managers, while they give money to newer VCs as the capacity to the established brands is limited. 

So far, I looked at average fund sizes per strategy, but what about the dispersion? The table below gives the answer. 

```{r DispersionFundSizes}
fun <- function(x, probs = c(0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95)) {
  
  res <- quantile(x, probs = probs, na.rm=TRUE)
  # Add mean
  len <- length(res)
  if (len %% 2 != 1) {
    stop("probs should be of unequal length (symmetrical around the median).")
  }
  res <- c(res[1:((len+1)/2)], Mean=mean(x, na.rm=TRUE), res[((len+1)/2+1):len])
  return(as.list(res))
  
}

intDT <- DT[, fun(size), by=type]
kbl(intDT,
    digits=0,
    format.args = list(big.mark = ","),
    caption = "Mean and different percentiles of fund sizes for buyout, growth, and VC funds in the Preqin data set. Numbers in USD million.") %>%
  kable_classic(full_width = FALSE)
###Replace "%" with "Perc" to have no issues referring to column names in text
setnames(intDT, names(intDT)[2:ncol(intDT)], c("Perc5", "Perc10", "Perc25", "Median", "Mean", "Perc75", "Perc90", "Perc95"))
```

For all three strategies, there is a large spread between the 5^th^ and the 95^th^ percentile. Not surprisingly, the spread is much larger for buyout and growth funds (factor `r round(intDT[type=="Buyout", Perc95/Perc5], digits=0)`x and `r round(intDT[type=="Growth", Perc95/Perc5], digits=0)`x) than for VC (factor `r `round(intDT[type=="VC", Perc95/Perc5], digits=0)`x). Again, the most likely explanation is that successful VCs have to limit their fund sizes much more, as capital needs per investment are not that dependent on the market cycle and they don't want to dilute their returns by investing in more start-ups. Instead, they want to focus on those with the biggest chance of succeeding.

As a result, the distribution is more right-skewed for buyout and growth with a much larger mean than median. Yes, the average buyout fund size is USD `r format(intDT[type=="Buyout", "Mean"], digits=0, big.mark=",")` million, but the average buyout fund only has a size of USD `r format(intDT[type=="Buyout", "Median"], digits=0, big.mark=",")` million. The same goes for growth funds.

# Performance of PE

## TVPIs

```{r CacluatePerf}
setkey(DT, id, qtr) #DT has to be sorted by qtr as you need the last NAV as final valuation for TVPI
DT[, ncf:=dist-call]
DT[qtr==max_qtr, ncf:=ncf+nav]
DT[, date:=min_date %m+% months(3*(qtr-1))] #Get dates in for IRR
perfDT <- DT[, list(Invested = sum(call),
                    DPI      = sum(dist)/sum(call),
                    TVPI     = (sum(dist) + nav[.N])/sum(call),
                    IRR      = utilitiesCJ::IRR(ncf, date)*100),
             by=list(id, vintage, size, status, geo, type)]
cols <- c("Invested", "DPI", "TVPI", "IRR")
perfDT2 <- melt(perfDT, value.vars = c("id", "vintage", "size", "status", "geo", "type"),
               measure.vars= cols, variable.name = "stat")
# vyDT <- dcast(perfDT2, vintage ~ stat, fun=list(min, median, mean, max), value.var = "value", na.rm=TRUE)
# setnames(vyDT, names(vyDT), gsub("value_", "", names(vyDT)))
```

```{r TVPIperStrategy}
ggplot(perfDT2[stat=="TVPI" & vintage>=1990 & type!="Growth"], aes(factor(vintage),value)) + 
  scale_fill_jco() +
  geom_boxplot(aes(fill = factor(type)), outlier.size = -1, coef=0) +
  #scale_fill_manual(values = viridis::viridis(3)) +
  xlab("Vintage year") + ylab("TVPI") +
  theme(legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  coord_cartesian(ylim = c(0, 5))

  

```

Spread, pooled, by size: this is gonna be interesting as I'm pretty sure you have a high correlation with size and returns in VC (see logic above). That's why the pooled returns are so much better.

## DPIs

## IRRs

## PMEs
